%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% I, the copyright holder of this work, release this work into the
%% public domain. This applies worldwide. In some countries this may
%% not be legally possible; if so: I grant anyone the right to use
%% this work for any purpose, without any conditions, unless such
%% conditions are required by law.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[
  digital,     %% The `digital` option enables the default options for the
               %% digital version of a document. Replace with `printed`
               %% to enable the default options for the printed version
               %% of a document.
%%  color,       %% Uncomment these lines (by removing the %% at the
%%               %% beginning) to use color in the printed version of your
%%               %% document
  oneside,     %% The `oneside` option enables one-sided typesetting,
               %% which is preferred if you are only going to submit a
               %% digital version of your thesis. Replace with `twoside`
               %% for double-sided typesetting if you are planning to
               %% also print your thesis. For double-sided typesetting,
               %% use at least 120 g/m² paper to prevent show-through.
  nosansbold,  %% The `nosansbold` option prevents the use of the
               %% sans-serif type face for bold text. Replace with
               %% `sansbold` to use sans-serif type face for bold text.
  nocolorbold, %% The `nocolorbold` option disables the usage of the
               %% blue color for bold text, instead using black. Replace
               %% with `colorbold` to use blue for bold text.
  lof,         %% The `lof` option prints the List of Figures. Replace
               %% with `nolof` to hide the List of Figures.
  lot,         %% The `lot` option prints the List of Tables. Replace
               %% with `nolot` to hide the List of Tables.
]{fithesis4}
%% The following section sets up the locales used in the thesis.
\usepackage[resetfonts]{cmap} %% We need to load the T2A font encoding
\usepackage[T1,T2A]{fontenc}  %% to use the Cyrillic fonts with Russian texts.
\usepackage[
  main=english, %% By using `czech` or `slovak` as the main locale
                %% instead of `english`, you can typeset the thesis
                %% in either Czech or Slovak, respectively.
  english, german, czech, slovak %% The additional keys allow
]{babel}        %% foreign texts to be typeset as follows:
%%
%%   \begin{otherlanguage}{german}  ... \end{otherlanguage}
%%   \begin{otherlanguage}{czech}   ... \end{otherlanguage}
%%   \begin{otherlanguage}{slovak}  ... \end{otherlanguage}
%%
%%
%% The following section sets up the metadata of the thesis.
\thesissetup{ 
    date        = \the\year/\the\month/\the\day,
    university  = mu,
    faculty     = fi,
    type        = mgr,
    department  = Department of design and development of software systems,
    author      = Bc. Marcel Lukčo,
    gender      = m,
    advisor     = {Mgr. Miroslav Ruda},
    title       = {PBSMon2, web service for displaying MetaCenter status},
    TeXtitle    = {PBSMon2, web service for displaying MetaCenter status},
    keywords    = {cloud, cloud computing, distributed computing, MetaCentrum},
    TeXkeywords = {cloud, cloud computing, distributed computing, MetaCentrum},
    abstract    = {%  
      TBD
    },
    thanks      = {%
    First and foremost, I would like to express my sincere gratitude to my supervisor, Mgr. Miroslav Ruda, for his valuable guidance, insightful comments, and continuous support throughout the development of this thesis.

    I would also like to thank RNDr. Martin Kuba, Ph.D., for his technical assistance and expertise, which significantly contributed to the practical part of this work.
    
    My thanks also go to Mgr. Václav Chlumský, Ing. František Řezníček, and Mgr. Ivana Křenková for their collaboration, helpful advice, and support during the integration with related systems.
    },
    bib         = example.bib,
    %% Remove the following line to use the JVS 2018 faculty logo.
    facultyLogo = fithesis-fi,
}
\usepackage{makeidx}      %% The `makeidx` package contains
\makeindex                %% helper commands for index typesetting.
%% These additional packages are used within the document:
\usepackage{paralist} %% Compact list environments
\usepackage{amsmath}  %% Mathematics
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{url}      %% Hyperlinks
\usepackage{markdown} %% Lightweight markup
\usepackage{listings} %% Source code highlighting
\lstset{
  basicstyle      = \ttfamily,
  identifierstyle = \color{black},
  keywordstyle    = \color{blue},
  keywordstyle    = {[2]\color{cyan}},
  keywordstyle    = {[3]\color{olive}},
  stringstyle     = \color{teal},
  commentstyle    = \itshape\color{magenta},
  breaklines      = true,
}
\usepackage{floatrow} %% Putting captions above tables
\floatsetup[table]{capposition=top}
\usepackage[babel]{csquotes} %% Context-sensitive quotation marks
\begin{document}
%% The \chapter* command can be used to produce unnumbered chapters:
\chapter*{Introduction}
%% Unlike \chapter, \chapter* does not update the headings and does not
%% enter the chapter to the table of contents. I we want correct
%% headings and a table of contents entry, we must add them manually:
\markright{\textsc{Introduction}}
\addcontentsline{toc}{chapter}{Introduction}
Currently, high-performance computing plays a key role in the implementation of research projects across a wide range of scientific disciplines. In the academic sector, it is therefore essential to ensure access to sufficient computing power to enable the implementation of demanding simulations, the processing of large data sets, and the effective analysis of results. National computing infrastructures, including CESNET, have been created for this purpose. This academic institution offers computing resources dedicated exclusively to research and academic purposes.

The PBSmon web service was developed to monitor these resources and provide an overview of their status. This application ensures the regular collection of metadata from computing nodes, queues, servers, and running jobs using native calls to PBS servers. It then transforms this data into a visual form accessible to users. Thanks to PBSmon, users can monitor the current system load, the status of individual jobs, and the availability of computing resources in real time.

However, the original PBSmon application is monolithic and uses outdated libraries, such as Java library Stripes, which is no longer maintained. Growing demands for security, sustainability, and support for containerized deployment require a fundamental refactoring of the system.

This thesis deals with the design and implementation of a new version of the PBSmon system, built on modern technologies: backend within the NestJS framework and frontend within the React framework. The goal is to create a modular, maintainable, and cloud-native architecture that preserves the functionality of the original solution while being ready for further development and adaptation to current technological standards.


\chapter{Analysis of the Existing Solution – PBS MON}
\label{chap:analysis}
The Pbsmon application serves as a central tool for collecting and visualizing data from various systems related to computing resource management. It automatically collects information from the PBS (Portable Batch System) \cite{pbs2022}, the Perun identity management system, and virtual machines (OpenStack). It then processes this data and provides users with a comprehensive view of the current status of the infrastructure, jobs, and user activities within the entire MetaCentrum infrastructure.

The web interface is used to visualize data from the PBS, Perun, and OpenStack systems. The information displayed includes a personal view of jobs, available computing resources, an overview of computing machines and their utilization, a list of users, and other statistics. Among other things, the following data is collected:

\begin{itemize}
  \item PBS nodes (status, load, available resources),
  \item information about virtual machines
  \item physical machines (availability, utilization, technical parameters),
  \item users (identity, activity, jobs),
  \item queues (status, priority, configuration),
  \item jobs (status, start time, resources used, user).
\end{itemize}


\section{System Architecture}
\label{sec:system-architecture}
The existing solution is built as a monolithic Java application that collects data from various systems (PBS, Perun, OpenStack) and displays it in a web interface. The architecture is tightly coupled and uses multiple technologies, including C libraries and shell scripts.


\newpage
\subsection{Data collection environments}
\label{subsec:data-collection-environments}
Pbsmon collects following data from the following environments:
\begin{itemize}
  \item PBS environment
  \begin{itemize}
    \item Compute clusters managed by PBS systems
    \item Collection of Fairshare metrics from pbscache, which determine the user's priority when launching a job
    \item Collection of the /etc/group file from PBS, essential for assigning users to groups used in access control lists (ACLs)
  \end{itemize}
  \item Perun environment - an identity and group management system. Returns information about users and the list of physical machines.
  \item OpenStack environment - Information about virtual machines running on physical hosts
\end{itemize}

Each of these environments has its own method of communication and data representation. Within the system architecture, these differences are abstracted through interfaces and adapters that ensure unified data processing. All of these enviroments and their data structure are described in the following sections.

\subsection{Data Processing and Unification}
\label{subsec:data-processing-and-unification}

The data obtained from various sources — such as compute clusters (PBS), cloud platforms (e.g., OpenStack), and the identity management system (Perun) — differ in their formats and representations. After collection, the data are first stored in an in-memory cache and subsequently mapped and unified to establish logical relationships between entities from different systems. Among other things, the following relationships are established:

\begin{itemize}
  \item Hierarchical structure: user(Perun) → jobs (PBS) → queues (PBS)
  \item Mapping: virtual machines (OpenStack) and physical machines (Perun)
\end{itemize}

\subsection{Web Presentation}
\label{subsec:web-presentation}

The presentation of data is performed through server-side rendering of HTML pages. Only logged in users, that have approved access to Pbsmon can access the web interface. The authorization is performed by third party system using OICD token, that will be described in detail in section \ref{sec:authentication}.

These pages display detailed views of jobs, nodes, system states, and other information relevant to both users and administrators. As mentioned earlier, the web interface is strictly read-only and does not provide any functionality for data input or modification.
The frontend is not implemented as a standalone application; it is an integral part of the monolithic Java system.

One of the key features is so called \textbf{QSUB assembler}, which allows user to add required parameters for the job submission and pbsmon will notify the user about the nodes that fullfils the requirements. Among other, it produces a shell script that can be executed to submit the job to the PBS server with the required parameters

All of this will be described in more detail in section \ref{sec:web-layer-and-user-interface}.

\newpage
\section{Data Collection from PBS Environment}
\label{sec:data-collection-from-pbs-environment}
The Portable Batch System (PBS) is a distributed workload management system designed to schedule and monitor computational jobs across multiple compute nodes in a cluster environment. \cite{pbs2022} 

In a typical configuration, the PBS environment consists of a central management server and a set of compute nodes, which are individual physical or virtual hosts providing computational resources such as CPUs, memory, GPUs, and local storage. \cite{pbs2022} 

A compute node (also referred to as a host) represents the fundamental execution unit in the cluster — it is the machine where user jobs are actually executed. Each node communicates with the central PBS server, which manages job submission, scheduling, and resource allocation. \cite{pbs2022} 

\subsection{Entities in PBS Environment}
\label{subsec:entities-in-pbs-environment}

PBS provides structured data that enables continuous monitoring of the computing environment operation, scheduler behavior, and resource utilization. From the PBS perspective, the following main entities are collected: \cite{pbs2022}


\subsubsection{Server}
\label{subsubsec:server-pbs-server}

The PBS server is the authority for the entire cluster: it receives and registers jobs, maintains queues and nodes, tracks their states, and publishes global statistics (e.g., counts of jobs by state). It provides the following data: \cite{pbs2022}

\begin{itemize}
  \item \textbf{Policies and limits} -- defines and enforces scheduling and resource utilization rules (limits on job and CPU counts, job array rules, rescheduling on node failure, scheduling enable/disable).
  
  \item \textbf{Resources and default settings} -- manages available/assigned resources and defaults (e.g., resources\_available, resources\_default, default\_chunk, default\_queue) and provides summary utilization (assigned memory/CPU/nodes).
  
  \item \textbf{Integration with scheduler and reservations} -- provides parameters for the scheduler (iteration, backfill, sorting/fairshare formula) and supports time-based resource reservations (advance/standing/maintenance).
  
  \item \textbf{Security and access} -- manages access policies through ACLs (hosts, users, managers/operators), supports Kerberos/realm policies, and handles credential management/renewal.
  
  \item \textbf{Operations and management} -- configures logging, email notifications, license quotas/counters, and system version; allows management of hooks and other server-level objects.
\end{itemize}\cite{pbs2022}

\subsubsection{Jobs}
\label{subsubsec:jobs}

A job is the basic computational unit submitted by a user to the PBS server---it contains command(s) to execute and resource requirements (CPU, memory, time, GPU). The server queues it, schedules it to nodes, monitors its progress, and upon completion evaluates its result (including outputs and return code).\cite{pbs2022} 

A job can be a standalone task or a member of a job array (with multiple subjobs) sharing the same resource template. PBS provides the following data about jobs:\cite{pbs2022}

\begin{itemize}
  \item \textbf{Identity and ownership} -- job ID and name, owner (Job\_Owner), project/VO (project), target queue (queue), server, and submit host.
  
  \item \textbf{Lifecycle and scheduling} -- state and substate (job\_state, substate), priority, run count, holds (Hold\_Types), rerunnability (Rerunable), credentials and validity (credential\_id, credential\_validity).
  
  \item \textbf{Requested resources and placement} -- Resource\_List.* (e.g., select, ncpus, mem, walltime, place, scratch\_*, mpiprocs, ompthreads, nodect) and runtime identifiers (session\_id).
  
  \item \textbf{Timing metrics} -- ctime, qtime, stime, mtime, obittime, etime + derived indicators (e.g., eligible\_time).
  
  \item \textbf{I/O and environment} -- working directory (jobdir), stdout/stderr paths (Output\_Path, Error\_Path), submission arguments (Submit\_arguments), and environment variables (Variable\_List).
  
  \item \textbf{Result and diagnostics} -- return code (Exit\_status) and auxiliary fields for auditing and progress tracking.
\end{itemize}\cite{pbs2022}

\subsubsection{Queues}
\label{subsubsec:queues}

A queue is a logical structure for accepting and processing jobs---it defines its type (Execution vs. Route), resource defaults and limits, access rules, and how jobs are either executed on nodes or redirected to target queues. PBS provides the following data about queues: \cite{pbs2022}

\begin{itemize}
  \item \textbf{Identity and type} -- queue name, queue\_type (Execution/Route), Priority (weight in scheduling), operational state (enabled/started), and optionally hasnodes.
  
  \item \textbf{State and utilization} -- 
  aggregates such as total\_jobs and state\_count, and currently
  assigned resources resources\_assigned.* (e.g., memory, CPUs, nodes, MPI processes).
  
  \item \textbf{Policies and limits} -- resource boundaries resources\_max.* and minimums resources\_min.* (including GPU and walltime), extra rules like kill\_delay, backfill\_depth, or from\_route\_only (accepts jobs only via routing).
  
  \item \textbf{Defaults and placement} --
  resources\_default.
  (e.g., CPUs, walltime, placement, GPUs) 
  and default\_chunk.* 
  (e.g., implicit chunk size, queue\_list for targeting).
  
  \item \textbf{Routing (Route queues)} -- route\_destinations defines target execution queues to which jobs are automatically redirected.
  
  \item \textbf{Access and security} -- 
  ACL toggles and lists: 
  acl\_user\_enable/acl\_users, 
  acl\_group\_enable/acl\_groups,
   or acl\_host\_enable/acl\_hosts.
  
  \item \textbf{Organizational tags} -- optional attributes such as fairshare\_tree (fairshare hierarchy) or partition (infrastructure label/partition) for logical segmentation and policy purposes.
\end{itemize}

\parencite{pbs2022}
\subsubsection{Nodes}
\label{subsubsec:nodes}

A node represents a physical machine in the PBS environment that provides computational resources for job execution. Each node is managed by the PBS server and can be in various operational states depending on its availability and current workload.  PBS provides the following data about nodes: \cite{pbs2022}

\begin{itemize}
  \item \textbf{Identity and location} -- node name (vnode), hostname (host), cluster identifier (resources\_available.cluster), and the name of the execution daemon (Mom) that manages the node. 
  
  \item \textbf{State and availability} -- primary state (state) indicating the operational status (e.g., free, job-busy, down, offline) and auxiliary state (state\_aux) providing additional context. 
  
  \item \textbf{Available resources} -- total capacity of the node defined by \texttt{resources\_available.*} attributes, including:
  \begin{itemize}
    \item CPU resources: ncpus (number of CPUs), pcpus (physical CPUs), cpu\_vendor, cpu\_flag (CPU feature flags)
    \item Memory: mem (total memory), vmem (virtual memory), hpmem (high-performance memory)
    \item Accelerators: ngpus (number of GPUs), gpu\_mem, gpu\_cap (compute capability), cuda\_version
    \item Scratch storage: scratch\_local, scratch\_shared, scratch\_ssd, scratch\_shm (shared memory)
    \item Network: ethernet\_speed, infiniband
    \item Software: singularity (container support), os, osfamily, arch
    \item Organizational: queue\_list (queues accessible from this node), cluster-specific tags
  \end{itemize}
  
  \item \textbf{Assigned resources} -- currently allocated resources tracked by \texttt{resources\_assigned.*} attributes, including:
  \begin{itemize}
    \item ncpus, ngpus, naccelerators (number of assigned CPUs, GPUs, accelerators)
    \item mem, vmem, hbmem, accelerator\_memory (assigned memory)
    \item scratch\_local, scratch\_ssd (assigned storage)
  \end{itemize}
  
  \item \textbf{Active jobs} -- list of job identifiers (jobs) currently running on the node, with each job identified by its ID and task index (e.g., 14964063.pbs-m1.metacentrum.cz/0). 
  
  \item \textbf{Sharing and placement} -- sharing mode (\texttt{sharing}) that determines resource allocation (e.g., \texttt{default\_shared}, \texttt{force\_exclusive}), and reservation support (\texttt{resv\_enable}). 
  
  \item \textbf{Timestamps} -- last\_state\_change\_time (timestamp of the last state transition) and last\_used\_time (timestamp when the node was last utilized). 
\end{itemize} \cite{pbs2022}

The distinction between available and assigned resources enables monitoring of node utilization, while the state information provides insight into node availability for job scheduling. The jobs list allows tracking which specific jobs are consuming resources on each node, which is essential for resource accounting and troubleshooting. 

\subsubsection{Reservations}
\label{subsubsec:reservations}

Reservations are a mechanism in PBS that allows nodes to be reserved for exclusive use during a specific time period. When a reservation is created, the specified nodes become unavailable for regular job scheduling. For each reservation, PBS automatically creates a dedicated queue that provides access to the reserved resources. \cite{pbs2022}

PBS provides the following data about reservations:

\begin{itemize}
  \item \textbf{Identity} -- reservation name (Reserve\_Name), unique identifier (name), and the associated queue name (queue) that is created for the reservation. \cite{pbs2022}
  
  \item \textbf{Ownership and access} -- reservation owner (Reserve\_Owner) and list of authorized users (Authorized\_Users) \cite{pbs2022}
  
  \item \textbf{State information} -- reservation state (reserve\_state) and substate (reserve\_substate) indicating the current status of the reservation (e.g., active, confirmed, or in transition).  \cite{pbs2022}
  
  \item \textbf{Time constraints} -- reservation start time (reserve\_start), end time (reserve\_end), and duration (reserve\_duration) specified as Unix timestamps. \cite{pbs2022}
  
  \item \textbf{Resource requirements} -- the resources reserved by the reservation, including:
  \begin{itemize}
    \item Memory: Resource\_List.mem (total memory reserved)
    \item CPUs: Resource\_List.ncpus (number of CPUs)
    \item GPUs: Resource\_List.ngpus (number of GPUs, if applicable)
    \item Nodes: Resource\_List.nodect (number of nodes) and Resource\_List.select (detailed node selection specification)
    \item Placement: Resource\_List.place (placement policy, e.g., \texttt{free}, \texttt{exclhost})
    \item Walltime: Resource\_List.walltime (maximum execution time for jobs in the reservation)
  \end{itemize} \cite{pbs2022}
  
  \item \textbf{Reserved nodes} -- list of actual nodes allocated to the reservation (resv\_nodes) with their specific resource allocations. \cite{pbs2022}
  
  \item \textbf{Metadata} -- creation time (ctime), modification time (mtime), submission host (Submit\_Host), and partition information (partition, if applicable). \cite{pbs2022}
  
  \item \textbf{Retry information} -- reservation count (reserve\_count) and retry attempts (reserve\_retry) for tracking reservation lifecycle. \cite{pbs2022}
\end{itemize}


\subsubsection{Resources}
\label{subsubsec:resources}

Resources in PBS represent units of computational capacity that can be requested by jobs, allocated to nodes, and managed by queues and the server. Each resource is defined with a unique name and attributes that specify its data type and where it can be used within the PBS system. \cite{pbs2022} 

PBS provides a comprehensive list of all available resources in the system, where each resource definition includes: \cite{pbs2022}

\begin{itemize}
  \item \textbf{Resource name} -- the identifier used to reference the resource in job submissions, node configurations, and queue settings (e.g., \texttt{cput}, \texttt{mem}, \texttt{ncpus}, \texttt{ngpus}, \texttt{walltime}).
  
  \item \textbf{Type} -- the data type of the resource value, which determines how the resource is interpreted and validated. Common types include:
  \begin{itemize}
    \item \texttt{long} -- integer values (e.g., CPU count, GPU count)
    \item \texttt{size} -- memory or storage values with units (e.g., bytes, KB, MB, GB)
    \item \texttt{string} -- text values
    \item \texttt{float} -- floating-point numeric values
    \item \texttt{time} -- time duration values
  \end{itemize}
  
  \item \textbf{Flag} -- a string of characters indicating where the resource can be used or referenced:
  \begin{itemize}
    \item \texttt{h} -- can be used at the host/node level
    \item \texttt{q} -- can be used at the queue level
    \item \texttt{n} -- can be used at the node level
    \item \texttt{m} -- can be used at the server level 
  \end{itemize}\cite{pbs2022} 
\end{itemize}

\subsubsection{Scheduler status}
\label{subsubsec:scheduler-status}

The PBS scheduler is responsible for making decisions about which jobs to run, when to run them, and on which nodes to execute them.  A PBS system can have multiple schedulers, each managing a specific partition or set of resources. The scheduler continuously evaluates queued jobs against available resources and applies scheduling policies to optimize resource utilization and meet job requirements. \cite{pbs2022}

PBS provides status information about each scheduler instance, including: \cite{pbs2022}

\begin{itemize}
  \item \textbf{Identity and location} -- scheduler name, the host where the scheduler daemon runs (sched\_host), and the partition it manages (partition).
  
  \item \textbf{Operational state} -- current state of the scheduler (e.g., \texttt{scheduling}, \texttt{idle}) and whether scheduling is enabled (scheduling).
  
  \item \textbf{Scheduling cycle} -- scheduler cycle length (sched\_cycle\_length) defining how frequently the scheduler evaluates and schedules jobs, and the current iteration count (scheduler\_iteration).
  
  \item \textbf{Processor set configuration} -- settings for processor set (pset) handling: do\_not\_span\_psets (prevents jobs from spanning multiple processor sets) and only\_explicit\_psets (restricts scheduling to explicitly defined processor sets).
  
  \item \textbf{Scheduling mode} -- throughput\_mode indicates whether the scheduler prioritizes throughput optimization, and opt\_backfill\_fuzzy (if present) specifies the backfill optimization level.
  
  \item \textbf{Preemption settings} -- configuration for job preemption: preempt\_queue\_prio (priority threshold for preemption), preempt\_prio (queues or job types that can be preempted), preempt\_order (preemption order strategy), and preempt\_sort (sorting method for preemption selection, e.g., \texttt{min\_time\_since\_start}).
  
  \item \textbf{Integration and hooks} -- job\_run\_wait specifies the hook that controls when jobs can start execution (e.g., \texttt{runjob\_hook}).
  
  \item \textbf{File system paths} -- sched\_priv (path to scheduler private directory) and sched\_log (path to scheduler log files).
  
  \item \textbf{Logging and monitoring} -- log\_events (bitmask specifying which events to log) and server\_dyn\_res\_alarm (alarm threshold for dynamic resource changes).
\end{itemize} \cite{pbs2022}

\subsubsection{Fairshare metrics}
\label{subsubsec:fairshare-metrics}

A list of users, their fairshare values, and the timestamp when the record was last modified. 
fairshare values are used per scheduler.\cite{pbs2022}
This data is retrieved for both the QSUB assembler and user monitoring purposes.

\newpage
\subsection{Data Collection Mechanism}
\label{subsec:data-collection-mechanism}

\subsubsection{Batch Interface Library (IFL)}
\label{subsubsec:batch-interface-library}
PBS provides a C library, which represents the programming interface (API) of the PBS system, also known as the Batch Interface Library (IFL). \cite{pbs2022} 

This library allows external applications and tools to communicate with the PBS server. It provides functions for remote management of batch jobs, querying the system state, and managing computational resources through TCP/IP communication. Using the library, it is possible to implement a client application that: \cite{pbs2022} 

\begin{itemize}
  \item establishes a connection to the server (pbs\_connect),

  \item authenticates the user,
  
  \item creates and submits jobs (pbs\_submit),

  \item queries their status (pbs\_statjob, pbs\_selstat),
   
  \item modifies or deletes jobs (pbs\_alterjob, pbs\_deljob),
  
  \item works with information about the server, queues, nodes, or scheduler.\cite{pbs2022}
\end{itemize}   

Thus, library represents a key component for implementing a tool that enables data collection and monitoring of jobs managed by the PBS server.


Subsequently, current Pbsmon implementation contains C code that, when invoked, retrieves information from the PBS server using the library functions and then stores this data into a file, which the Java application later processes and saves into the in-memory store (see Listing \ref{lst:pbs_connect_example}).

This collection process is triggered whenever the user wants to display any Pbsmon page and when the data in the memory cache is older than 60 seconds.

Another important note is that collected data represents the current state of the PBS server. It does not include any historical data. \label{note:historical-data}

\newpage
\begin{lstlisting}[caption={Partial code snippet for data collection from PBS server},
  language=c, label={lst:pbs_connect_example}]
  #include <pbs_error.h>
  #include <pbs_ifl.h>

  int main(int argc, char **argv) {
      // ...
      con = pbs_connect(server);    
      if(con<0) {
          return 1;
      }
      /* get server info */
      bs = pbs_statserver(con, NULL, NULL);
      process_data(bs,"servers");
      /* get queues info */
      bs = pbs_statque(con, "", NULL, NULL);
      process_data(bs,"queues");
      /* get nodes info */
      bs = pbs_statnode(con, "", NULL, NULL);
      process_data(bs,"nodes");
      /* get jobs info: t - job arrays, x - finished jobs*/
      bs = pbs_statjob(con, "", NULL, "tx");
      process_data(bs,"jobs");
      /* get reservations info */
      bs = pbs_statresv(con, NULL, NULL, NULL);
      process_data(bs,"reservations");
      /* get resources info */
      bs = pbs_statrsc(con, NULL, NULL, NULL);
      process_data(bs,"resources");
      /* get scheduler info */
      bs = pbs_statsched(con, NULL, NULL);
      process_data(bs,"schedulers");
      /* end connection */
      pbs_disconnect(con);
      return 0;
  }  
\end{lstlisting}

\subsubsection{Fairshare metrics collection}
\label{subsubsec:fairshare-metrics-collection}

Fairshare metrics are collected using the bash command.

\begin{lstlisting}[caption={Metrics collection script},
  language=bash, label={lst:fairshare-metrics-collection}]
  list_cache <pbsServer> fairshare{.elixir}
\end{lstlisting}


This shell command returns a CSV file with columns:

user, last\_modified, fairshare.

\subsubsection{Acl list collection}
\label{subsubsec:acl-list-collection}

In addition to entities and caches, the shell script located on PBS server automatically pushes its /etc/group file every time it has changed and writes it to the filesystem of the Pbsmon server.
It is located in the following path:

\textbf{/etc/pbsmon/acl/<pbsServer>} -
for each PBS server, there is a separate file with the list of ACLs and their members.

\newpage
\section{Data Collection from Perun}
Perun is an open-source system developed in Java that serves for comprehensive management of identities, groups, attributes, and access to various resources and services. It is a modular solution designed for efficient management of users, organizations, and projects. The system is built with an emphasis on operation in distributed environments and on integration with existing identity systems in the fields of research and education.\cite{perun2025}

\subsection{Entities in Perun}
\label{subsec:entities-in-perun}
Entities retrieved from Perun are two independent data domains — users and machines. Each domain is exported by Perun as a separate JSON file.
In addition to that, perun collects and aftewards send etc/group file from each PBS server. The detailed description of these entities:

\subsubsection{Users}
\label{subsubsec:users-perun}
The user dataset contains information about all registered users in the MetaCentrum infrastructure, including their identifiers, names, organizational affiliations, and assigned virtual organizations. These data are used to enrich job statistics and to provide a link between computational activity and user identity.

\subsubsection{Machines}
\label{subsubsec:machines-perun}
In addition to management of users, Perun also collect information about physical computing resources of the MetaCentrum infrastructure.
These data are then passed to Pbsmon from Perun. Data are hierachically structured and grouped by Organization -> Cluster -> Computing Node.

Each record is an institution that has at least one cluster. Cluster is a group of computing nodes that are owned by the same institution. Computing node is a physical machine that is part of the cluster.

Each computing node is described by the following metadata:

\begin{itemize}
  \item CPU configuration
  \item Memory
  \item Storage
  \item Owner institution
  \item The list of individual node hostnames
\end{itemize}

These data are neccessary to get additional information about the complete information about the computing node for the running PBS jobs.
Additionally, these data independetly from PBS are important for knowing the complete information about the computing nodes for whole MetaCentrum infrastructure.

\subsubsection{etc/group Files}
\label{subsubsec:groups-perun}
In addition to user and machine metadata, Perun is also responsible for collecting \textbf{/etc/group} files from individual PBS servers. These files are included in the export and provide information about local UNIX groups and permissions. These files are then propagated to Pbsmon.


\subsection{Data Collection Mechanism}
\label{subsec:data-collection-mechanism-perun}

The integration of the Pbsmon system with Perun is designed using a PUSH model. Instead of direct access to the Perun database or invoking its API, Perun periodically generates JSON files containing all relevant information about users and computing resources. These files are then transferred via SSH directly to the Pbsmon server, where they are stored in the filesystem, and subsequently loaded and processed by Pbsmon. These files are stored in the filesystem as following:

\begin{itemize}
  \item \textbf{/etc/pbsmon/pbsmon\_users.json} 
  \item \textbf{/etc/pbsmon/pbsmon\_machines.json}
\end{itemize}

\subsubsection{Change Detection and Synchronization}
Currently, whenever user opens any page in Pbsmon, the system checks if the files were modified since last load. If they were, the files are loaded and processed by Pbsmon.


\newpage 
\section{Data Collection from Virtual Machines (OpenStack)}
OpenStack is a cloud operating system that controls large pools of compute, storage, and networking resources throughout a datacenter, all managed and provisioned through APIs with common authentication mechanisms. \cite{openstack2025}

Beyond standard infrastructure-as-a-service functionality, additional components provide orchestration, fault management and service management amongst other services to provide operators flexibility to customize their infrastructure and ensure high availability of user applications. \cite{openstack2025}

Within Metacentrum infrastructure, OpenStack is used to provide virtual machines to the users with specific needs that are not covered by the PBS environment. There are some computing nodes that are part of MetaCentrum infrastructure, but are reserved ad hoc for OpenStack. Within nodes returned by Perun mentioned in \ref{subsubsec:machines-perun} are also nodes that are used for OpenStack.

Current Pbsmon implementation gets very limited information about OpenStack virtual machines for each cluster. For each cluster, there is a list of virtual machines with their reserved CPU, name and user ID.

 
\subsection{Data Collection Mechanism}
\label{subsec:data-collection-mechanism-openstack}

The integration of the Pbsmon system with OpenStack is designed using a PUSH model. Instead of direct access to the API, JSON files are transferred via SSH directly to the Pbsmon server, where they are stored in the filesystem, and subsequently loaded and processed by Pbsmon. For each cluster, there is a separate file with the list of virtual machines with their reserved CPU, name and user ID. 

An example of the response structure for the OpenStack cluster named "glados" is provided in Appendix \ref{app:openstack-response-example}.


\newpage
\section{Retrieval of historical data}
\label{sec:collection-of-historical-data}

As mentioned in section \ref{note:historical-data}, the data collected by PBSmon represents only the current state of the PBS infrastructure. To access historical information about completed jobs, their resource usage, and long-term statistics, a separate application is responsible for collecting and storing historical PBS data in a PostgreSQL database.

This historical data collection system continuously records information about finished jobs, including their execution times, resource consumption, and user associations.

\section{Authentication}
\label{sec:authentication}
The current Pbsmon solution uses third-party authentication provided by Perun's e-INFRA system. Authentication is implemented using the OpenID Connect (OIDC) protocol, which allows users to authenticate through the centralized identity provider managed by Perun.  \cite{perun2025}

This approach ensures that only authorized users with valid credentials from the Perun identity management system can access the Pbsmon web interface.  \cite{perun2025}

The authentication flow follows the standard OIDC protocol, where users are redirected to the Proxy IdP for login, and upon successful authentication, an OIDC token is issued and used to grant access to the Pbsmon application. \cite{perun2025}

\section{Web Layer and User Interface}
\label{sec:web-layer-and-user-interface}
The current Pbsmon application is integrated as part of the MetaVO portal (\url{https://metavo.metacentrum.cz}), which serves as the main web interface for MetaCentrum services. Within MetaVO, Pbsmon is encapsulated as a single subsection called "Current state" (see \ref{fig:pbsmon}). This section is available only to logged-in users who have been granted access to Pbsmon through the authentication system described in section \ref{sec:authentication}.

The "Current state" subsection consists of the following pages:

\begin{itemize}
  \item \textbf{Personal view} - A dashboard with information about the logged-in person, including their job statistics, jobs queues and quick access to other relevant pages
  \item \textbf{QSUB assembler} - A tool for assembling commands for job submission, described in detail in subsection \ref{subsec:qsub-assembler}
  \item \textbf{Physical machines} - A list of Metacentrum grid infrastructure retrieven from Perun nodes with PBS mapping
  \item \textbf{PBS node state} - A list of all PBS nodes and their current status. Without mapping to the Perun physical machines.
  \item \textbf{Virtual machines} - A list of all virtual machines (no longer relevant in the current infrastructure)
  \item \textbf{Jobs queues} - A list of queues and reservations
  \item \textbf{Jobs} - A page with total statistics and navigation hub to "My jobs", "All jobs", and "Suspicious jobs"
  \item \textbf{User} - A list of users from Perun and their current CPU usage
  \item \textbf{Machine properties} - A list of all PBS nodes and their properties (e.g., architecture, OS family, cluster, cgroups, etc.). Page allows to click on the property to see all the nodes that have this property.
  \item \textbf{List of hardware} - A list of organizations and their clusters
  \item \textbf{Cloud} - Cloud-related information (no longer relevant)
\end{itemize}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{figures/pbsmon.png}
  \caption{Pbsmon layout}
  \label{fig:pbsmon}
\end{figure}

\newpage
\subsection{Personal view}
\label{subsec:personal-view}
The Personal view serves as the main dashboard for users, representing one of the most important and frequently used features of Pbsmon. This view provides users with an overview of their job statistics and quick access to other relevant pages.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{figures/old-pbsmon-personal-view.png}
  \caption{Personal view dashboard in the old Pbsmon}
  \label{fig:old-pbsmon-personal-view}
\end{figure}

The dashboard displays total counts of jobs associated with the user. However, since PBS always returns the current state of the system, the Personal view focuses on showing counts of \textit{relevant} jobs---those that are considered active. Specifically, this includes jobs that are currently in the queue, currently running, or have finished within the previous few days. This filtering ensures that users see meaningful information about their recent and ongoing computational work, rather than being overwhelmed by historical data from all past jobs.

The Personal view also contains links to other pages within the system. One such link leads to the "My jobs" page, which displays detailed information about the user's jobs. For user the list of his jobs are highly relevant, therefore it can be directly incorporated into the new dashboard within the new solution.

The page also displays all jobs queues that are accessible by the user. While this information is useful, it is considered less relevant and it is not necessary to be included in the new dashboard.

One limitation of the current implementation is that the dashboard only displays total CPU usage and does not include total GPU resources. This limitation stems from the fact that GPU usage was not a significant concern at the start of the old Pbsmon development, but has since become increasingly important as GPU computing has gained prominence in the MetaCentrum infrastructure.


\subsection{QSUB assembler}
\label{subsec:qsub-assembler}
The QSUB assembler is one of the most important features of the Pbsmon web interface, designed to assist users in submitting jobs to the MetaCentrum infrastructure. The \texttt{qsub} command is a CLI (Command Line Interface) command that allows users to submit jobs to the PBS system. This page provides a form-based interface for configuring job submission parameters.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/old-pbsmon-qsub-assembler.png}
  \caption{QSUB assembler interface}
  \label{fig:qsub-assembler}
\end{figure}

The QSUB assembler page presents users with an extensive form containing numerous parameters that can be set for job submission, such as resource requirements (CPU flags, memory, scratch space, architecture), queue selection, walltime, and other MetaCentrum-specific options. 

The current user interface presents parameters in a form equivalent to their syntax in the \texttt{qsub} command, which has the advantage that users gradually learn the \texttt{qsub} command syntax. However, the interface lacks supplementary explanations for individual fields, and therefore it is not always clear what the purpose of some less known or specific parameters is (e.g., \texttt{luna}, \texttt{umg}). The UI contains names directly according to PBS attributes, but without context or brief help, it can be difficult for users to understand what exactly these choices represent or how they will affect job execution.

After configuring the desired parameters, the page automatically generates a complete shell command that can be used in two ways:

\begin{itemize}
  \item Directly inserted into a terminal and executed to submit the job immediately
  \item Inserted as a comment directive in a PBS job script (e.g., \texttt{\#PBS -l ...}) for later execution
\end{itemize}

Upon confirmation (submit), the page performs a query against the available MetaCentrum infrastructure to determine job feasibility. The results are presented in two key overviews:

\begin{itemize}
  \item \textbf{Number of nodes meeting the requirements}: This shows the total count of nodes that are compatible with the specified parameters (e.g., nodes matching the CPU flags, memory requirements, scratch space, architecture, and other constraints)
  \item \textbf{Number of available nodes}: This displays the count of free nodes from the compatible set at the current moment, representing the real-time availability for job execution
\end{itemize}

Following these summary statistics, the page displays a detailed list of all nodes that match the specified requirements, providing users with list of nodes that can be used to submit the job.

\section{Summary of the Analysis}
\label{sec:summary-of-analysis}

The analysis of the existing Pbsmon solution reveals a system that has successfully served its purpose of monitoring and visualizing MetaCentrum infrastructure data. The application effectively integrates data from multiple sources (PBS, Perun, and OpenStack) and provides users with  views of computational resources, jobs, and system status. Key strengths include the QSUB assembler feature, which assists users in constructing job submission commands, and the successful integration with Perun's authentication system.

However, the analysis also identifies several significant limitations and areas for improvement. The system suffers from a \textbf{monolithic architecture} that tightly couples all components, making it difficult to maintain, extend, or deploy in modern containerized environments. The absence of dockerization further complicates deployment and scaling. The application relies on \textbf{outdated libraries}, most notably the Java library Stripes, which is no longer maintained, creating security and sustainability concerns.

From a user experience perspective, the interface presents parameters in a form that mirrors the \texttt{qsub} command syntax, which helps users learn PBS commands but lacks supplementary explanations for individual fields. This makes it difficult for users to understand the purpose of less common parameters (e.g., \texttt{luna}, \texttt{umg}) or how their choices will affect job execution. The web interface uses \textbf{server-side rendering} and is not implemented as a standalone frontend application, limiting flexibility and modern user experience capabilities.

The system also exhibits limitations in meeting current infrastructure demands. As GPU computing has gained prominence in MetaCentrum, the dashboard only displays total CPU usage statistics and does not include GPU resources or GPU time usage metrics. This reflects a common occurence with long-serving projects.

Another concern identified is a \textbf{potential GDPR violation}: the system displays information about all users that currently have active jobs and their jobs within MetaCentrum, which may not comply with data protection regulations regarding the visibility of personal computational activity to other users.

In summary, while Pbsmon has been functional and valuable, its monolithic architecture, outdated technology stack, lack of containerization support, suboptimal user experience, incomplete feature coverage for modern computing needs, and potential privacy compliance issues necessitate a fundamental redesign and modernization of the system.


\chapter{Design of the New Solution}
\label{chap:design}


\section{Functional requirements}
\label{sec:functional-requirements}
TBD

\section{GUI design}
\label{sec:gui-design}
TBD - using figma see appendix

\section{Architecture design}
\label{sec:architecture-design}
TBD

\section{Selected technologies}
\label{sec:selected-technologies}
TBD



\chapter{Implementation}
\label{chap:implementation}

\section{System Architecture}
\label{sec:system-architecture-new}

TBD

\section{API}
\label{sec:data-collection-mechanism}

TBD

\subsection{Documentation}
TBD - realized using OpenAPI, automatically generated from code, see appendix

\section{Page views}
\label{sec:page-views}

TBD


\chapter{Testing and Evaluation}

\section{Unit tests}
\label{sec:unit-tests}
TBD

\section{Integration tests}
\label{sec:integration-tests}
TBD

\section{Performance tests}
\label{sec:performance-tests}
TBD


\chapter{Deployment}
TBD

\chapter{Future improvements}


\chapter*{Conclusion}
\addcontentsline{toc}{chapter}{Conclusion}


\appendix
\chapter{OpenStack Response Example}
\label{app:openstack-response-example}

This appendix contains an example of the JSON response structure for the OpenStack cluster named "glados" that is collected by the PBSmon system.

\begin{lstlisting}[caption={Example response for OpenStack cluster "glados"},
  label={lst:openstack-glados-response}]
[
    {
        "CPUs": "40",
        "Hypervisor": "ics-gladosag-007-ostack.priv.cloud.muni.cz",
        "VMs": [
            {
                "CPUs": "38",
                "created": "2020-10-09T09:32:42Z",
                "instance_state": "ACTIVE",
                "name": "RationAI-node-2",
                "user_id": "1633180b677608f61e96784ee5cbc608c0f4b62d@einfra.cesnet.cz"
            }
        ]
    },
    {
        "CPUs": "40",
        "Hypervisor": "ics-gladosag-006-ostack.priv.cloud.muni.cz",
        "VMs": []
    },
    {
        "CPUs": "40",
        "Hypervisor": "ics-gladosag-003-ostack.priv.cloud.muni.cz",
        "VMs": [
            {
                "CPUs": "38",
                "created": "2020-01-17T10:29:22Z",
                "instance_state": "ACTIVE",
                "name": "RationAI-node-1",
                "user_id": "1633180b677608f61e96784ee5cbc608c0f4b62d@einfra.cesnet.cz"
            }
        ]
    },
    {
        "CPUs": "40",
        "Hypervisor": "ics-gladosag-004-ostack.priv.cloud.muni.cz",
        "VMs": []
    },
    {
        "CPUs": "40",
        "Hypervisor": "ics-gladosag-001-ostack.priv.cloud.muni.cz",
        "VMs": [
            {
                "CPUs": "16",
                "created": "2022-08-17T13:07:15Z",
                "instance_state": "ACTIVE",
                "name": "front-82e95edc-1e2d-11ed-9687-0ee20d64cb6e",
                "user_id": "9cd61d48508661633e261f711634b749fdc5d9fcc20769e55baac46ef01c5a63@egi.eu"
            }
        ]
    }
]
\end{lstlisting}

\end{document}
