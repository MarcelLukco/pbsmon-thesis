%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% I, the copyright holder of this work, release this work into the
%% public domain. This applies worldwide. In some countries this may
%% not be legally possible; if so: I grant anyone the right to use
%% this work for any purpose, without any conditions, unless such
%% conditions are required by law.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[
  digital,     %% The `digital` option enables the default options for the
               %% digital version of a document. Replace with `printed`
               %% to enable the default options for the printed version
               %% of a document.
%%  color,       %% Uncomment these lines (by removing the %% at the
%%               %% beginning) to use color in the printed version of your
%%               %% document
  oneside,     %% The `oneside` option enables one-sided typesetting,
               %% which is preferred if you are only going to submit a
               %% digital version of your thesis. Replace with `twoside`
               %% for double-sided typesetting if you are planning to
               %% also print your thesis. For double-sided typesetting,
               %% use at least 120 g/m² paper to prevent show-through.
  nosansbold,  %% The `nosansbold` option prevents the use of the
               %% sans-serif type face for bold text. Replace with
               %% `sansbold` to use sans-serif type face for bold text.
  nocolorbold, %% The `nocolorbold` option disables the usage of the
               %% blue color for bold text, instead using black. Replace
               %% with `colorbold` to use blue for bold text.
  lof,         %% The `lof` option prints the List of Figures. Replace
               %% with `nolof` to hide the List of Figures.
  lot,         %% The `lot` option prints the List of Tables. Replace
               %% with `nolot` to hide the List of Tables.
]{fithesis4}
%% The following section sets up the locales used in the thesis.
\usepackage[resetfonts]{cmap} %% We need to load the T2A font encoding
\usepackage[T1,T2A]{fontenc}  %% to use the Cyrillic fonts with Russian texts.
\usepackage[
  main=english, %% By using `czech` or `slovak` as the main locale
                %% instead of `english`, you can typeset the thesis
                %% in either Czech or Slovak, respectively.
  english, german, czech, slovak %% The additional keys allow
]{babel}        %% foreign texts to be typeset as follows:
%%
%%   \begin{otherlanguage}{german}  ... \end{otherlanguage}
%%   \begin{otherlanguage}{czech}   ... \end{otherlanguage}
%%   \begin{otherlanguage}{slovak}  ... \end{otherlanguage}
%%
%%
%% The following section sets up the metadata of the thesis.
\thesissetup{ 
    date        = \the\year/\the\month/\the\day,
    university  = mu,
    faculty     = fi,
    type        = mgr,
    department  = Department of design and development of software systems,
    author      = Bc. Marcel Lukčo,
    gender      = m,
    advisor     = {Mgr. Miroslav Ruda},
    title       = {PBSMon2, web service for displaying MetaCenter status},
    TeXtitle    = {PBSMon2, web service for displaying MetaCenter status},
    keywords    = {cloud, cloud computing, distributed computing, MetaCentrum},
    TeXkeywords = {cloud, cloud computing, distributed computing, MetaCentrum},
    abstract    = {%  
      TBD
    },
    thanks      = {%
    First and foremost, I would like to express my sincere gratitude to my supervisor, Mgr. Miroslav Ruda, for his valuable guidance, insightful comments, and continuous support throughout the development of this thesis.

    I would also like to thank Mgr. Václav Chlumský, for his technical assistance and expertise, which significantly contributed to the practical part of this work.
    
    My thanks also go to Ing. František Řezníček, and Mgr. Ivana Křenková for their collaboration, helpful advice, and support during the integration with related systems.
    },
    bib         = example.bib,
    %% Remove the following line to use the JVS 2018 faculty logo.
    facultyLogo = fithesis-fi,
}
\usepackage{makeidx}      %% The `makeidx` package contains
\makeindex                %% helper commands for index typesetting.
%% These additional packages are used within the document:
\usepackage{paralist} %% Compact list environments
\usepackage{amsmath}  %% Mathematics
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{url}      %% Hyperlinks
\usepackage{markdown} %% Lightweight markup
\usepackage{listings} %% Source code highlighting
\lstset{
  basicstyle      = \ttfamily,
  identifierstyle = \color{black},
  keywordstyle    = \color{blue},
  keywordstyle    = {[2]\color{cyan}},
  keywordstyle    = {[3]\color{olive}},
  stringstyle     = \color{teal},
  commentstyle    = \itshape\color{magenta},
  breaklines      = true,
}
\usepackage{floatrow} %% Putting captions above tables
\floatsetup[table]{capposition=top}
\usepackage[babel]{csquotes} %% Context-sensitive quotation marks
\begin{document}
%% The \chapter* command can be used to produce unnumbered chapters:
\chapter*{Introduction}
%% Unlike \chapter, \chapter* does not update the headings and does not
%% enter the chapter to the table of contents. I we want correct
%% headings and a table of contents entry, we must add them manually:
\markright{\textsc{Introduction}}
\addcontentsline{toc}{chapter}{Introduction}
Currently, high-performance computing plays a key role in the implementation of research projects across a wide range of scientific disciplines. In the academic sector, it is therefore essential to ensure access to sufficient computing power to enable the implementation of demanding simulations, the processing of large data sets, and the effective analysis of results. National computing infrastructures, including CESNET, have been created for this purpose. This academic institution offers computing resources dedicated exclusively to research and academic purposes.

The PBSmon web service was developed to monitor these resources and provide an overview of their status. This application ensures the regular collection of metadata from computing nodes, queues, servers, and running jobs using native calls to PBS servers. It then transforms this data into a visual form accessible to users. Thanks to PBSmon, users can monitor the current system load, the status of individual jobs, and the availability of computing resources in real time.

However, the original PBSmon application is monolithic and uses outdated libraries, such as Java library Stripes, which is no longer maintained. Growing demands for security, sustainability, and support for containerized deployment, and the need to cover both PBS and OpenStack require a fundamental refactoring and extension of the system.

This thesis deals with the design and implementation of a new version of the PBSmon system, built on modern technologies: backend within the NestJS framework and frontend within the React framework. 

The goal is to create a modular, maintainable, and cloud-native architecture that not only preserves the functionality of the original solution, but also extends it to support the OpenStack cloud and to be ready for further development and adaptation to current technological standards, including new features required by the assignment.


\chapter{Analysis of the Existing Solution – PBSmon}
\label{chap:analysis}
The PBSmon application serves as a central tool for collecting and visualizing data from various systems related to computing resource management. It automatically collects information from the PBS (Portable Batch System), the Perun identity management system, and virtual machines (OpenStack). It then processes this data and provides users with a comprehensive view of the current status of the infrastructure, jobs, and user activities within the entire MetaCentrum infrastructure.

The web interface is used to visualize data from the PBS, Perun, and OpenStack systems. The information displayed includes a personal view of jobs, available computing resources, an overview of computing machines and their utilization, a list of users, and other statistics. Among other things, the following data is collected:

\begin{itemize}
  \item PBS nodes (status, load, available resources),
  \item information about virtual machines
  \item physical machines (availability, utilization, technical parameters),
  \item users (identity, activity, jobs),
  \item queues (status, priority, configuration),
  \item jobs (status, start time, resources used, user).
\end{itemize}


\section{System Architecture}
\label{sec:system-architecture}
The existing solution is built as a monolithic Java application that collects data from various systems (PBS, Perun, OpenStack) and displays it in a web interface. The architecture is tightly coupled and uses multiple technologies, including C libraries and shell scripts.


\newpage
\subsection{Data collection environments}
\label{subsec:data-collection-environments}
PBSmon collects following data from the following environments:
\begin{itemize}
  \item PBS environment
  \begin{itemize}
    \item Compute clusters managed by PBS systems
    \item Collection of Fairshare metrics from pbscache, which determine the user's priority when launching a job
    \item Collection of the /etc/group file from PBS, essential for assigning users to groups used in access control lists (ACLs)
  \end{itemize}
  \item Perun environment - an identity and group management system. Returns information about users and the list of physical machines.
  \item OpenStack environment - Information about virtual machines running on physical hosts
\end{itemize}

Each of these environments has its own method of communication and data representation. Within the system architecture, these differences are abstracted through interfaces and adapters that ensure unified data processing. All of these enviroments and their data structure are described in the following sections.

\subsection{Data Processing and Unification}
\label{subsec:data-processing-and-unification}

The data obtained from various sources — such as compute clusters (PBS), cloud platforms (e.g., OpenStack), and the identity management system (Perun) — differ in their formats and representations. After collection, the data are first stored in an in-memory cache and subsequently mapped and unified to establish logical relationships between entities from different systems. Among other things, the following relationships are established:

\begin{itemize}
  \item Hierarchical structure: user(Perun) → jobs (PBS) → queues (PBS)
  \item Mapping: virtual machines (OpenStack) and physical machines (Perun)
\end{itemize}

\subsection{Web Presentation}
\label{subsec:web-presentation}

The presentation of data is performed through server-side rendering of HTML pages. Only logged in users, that have approved access to PBSmon can access the web interface. The authorization is performed by third party system using OICD token, that will be described in detail in section \ref{sec:authentication}.

These pages display detailed views of jobs, nodes, system states, and other information relevant to both users and administrators. As mentioned earlier, the web interface is strictly read-only and does not provide any functionality for data input or modification.
The frontend is not implemented as a standalone application; it is an integral part of the monolithic Java system.

One of the key features is so called \textbf{QSUB assembler}, which allows user to add required parameters for the job submission and PBSmon will notify the user about the nodes that fullfils the requirements. Among other, it produces a shell script that can be executed to submit the job to the PBS server with the required parameters

All of this will be described in more detail in section \ref{sec:web-layer-and-user-interface}.

\newpage
\section{Data Collection from PBS Environment}
\label{sec:data-collection-from-pbs-environment}
The Portable Batch System (PBS) is a distributed workload management system designed to schedule and monitor computational jobs across multiple compute nodes in a cluster environment. \cite{pbs2022} 

In a typical configuration, the PBS environment consists of a central management server and a set of compute nodes, which are individual physical or virtual hosts providing computational resources such as CPUs, memory, GPUs, and local storage. \cite{pbs2022} 

A compute node (also referred to as a host) represents the fundamental execution unit in the cluster — it is the machine where user jobs are actually executed. Each node communicates with the central PBS server, which manages job submission, scheduling, and resource allocation. \cite{pbs2022} 

\subsection{Entities in PBS Environment}
\label{subsec:entities-in-pbs-environment}

PBS provides structured data that enables continuous monitoring of the computing environment operation, scheduler behavior, and resource utilization. From the PBS perspective, the following main entities are collected: \cite{pbs2022}


\subsubsection{Server}
\label{subsubsec:server-pbs-server}

The PBS server is the authority for the entire cluster: it receives and registers jobs, maintains queues and nodes, tracks their states, and publishes global statistics (e.g., counts of jobs by state). It provides the following data: \cite{pbs2022}

\begin{itemize}
  \item \textbf{Policies and limits} -- defines and enforces scheduling and resource utilization rules (limits on job and CPU counts, job array rules, rescheduling on node failure, scheduling enable/disable).
  
  \item \textbf{Resources and default settings} -- manages available/assigned resources and defaults (e.g., resources\_available, resources\_default, default\_chunk, default\_queue) and provides summary utilization (assigned memory/CPU/nodes).
  
  \item \textbf{Integration with scheduler and reservations} -- provides parameters for the scheduler (iteration, backfill, sorting/fairshare formula) and supports time-based resource reservations (advance/standing/maintenance).
  
  \item \textbf{Security and access} -- manages access policies through ACLs (hosts, users, managers/operators), supports Kerberos/realm policies, and handles credential management/renewal.
  
  \item \textbf{Operations and management} -- configures logging, email notifications, license quotas/counters, and system version; allows management of hooks and other server-level objects.\cite{pbs2022}
\end{itemize}

\subsubsection{Jobs}
\label{subsubsec:jobs}

A job is the basic computational unit submitted by a user to the PBS server---it contains command(s) to execute and resource requirements (CPU, memory, time, GPU). The server queues it, schedules it to nodes, monitors its progress, and upon completion evaluates its result (including outputs and return code).\cite{pbs2022} 

A job can be a standalone task or a member of a job array (with multiple subjobs) sharing the same resource template. PBS provides the following data about jobs:\cite{pbs2022}

\begin{itemize}
  \item \textbf{Identity and ownership} -- job ID and name, owner (Job\_Owner), project/VO (project), target queue (queue), server, and submit host.
  
  \item \textbf{Lifecycle and scheduling} -- state and substate (job\_state, substate), priority, run count, holds (Hold\_Types), rerunnability (Rerunable), credentials and validity (credential\_id, credential\_validity).
  
  \item \textbf{Requested resources and placement} -- Resource\_List.* (e.g., select, ncpus, mem, walltime, place, scratch\_*, mpiprocs, ompthreads, nodect) and runtime identifiers (session\_id).
  
  \item \textbf{Timing metrics} -- ctime, qtime, stime, mtime, obittime, etime + derived indicators (e.g., eligible\_time).
  
  \item \textbf{I/O and environment} -- working directory (jobdir), stdout/stderr paths (Output\_Path, Error\_Path), submission arguments (Submit\_arguments), and environment variables (Variable\_List).
  
  \item \textbf{Result and diagnostics} -- return code (Exit\_status) and auxiliary fields for auditing and progress tracking.\cite{pbs2022}
\end{itemize}

\subsubsection{Queues}
\label{subsubsec:queues}

A queue is a logical structure for accepting and processing jobs---it defines its type (Execution vs. Route), resource defaults and limits, access rules, and how jobs are either executed on nodes or redirected to target queues. PBS provides the following data about queues: \cite{pbs2022}

\begin{itemize}
  \item \textbf{Identity and type} -- queue name, queue\_type (Execution/Route), Priority (weight in scheduling), operational state (enabled/started), and optionally hasnodes.
  
  \item \textbf{State and utilization} -- 
  aggregates such as total\_jobs and state\_count, and currently
  assigned resources resources\_assigned.* (e.g., memory, CPUs, nodes, MPI processes).
  
  \item \textbf{Policies and limits} -- resource boundaries resources\_max.* and minimums resources\_min.* (including GPU and walltime), extra rules like kill\_delay, backfill\_depth, or from\_route\_only (accepts jobs only via routing).
  
  \item \textbf{Defaults and placement} --
  resources\_default.
  (e.g., CPUs, walltime, placement, GPUs) 
  and default\_chunk.* 
  (e.g., implicit chunk size, queue\_list for targeting).
  
  \item \textbf{Routing (Route queues)} -- route\_destinations defines target execution queues to which jobs are automatically redirected.
  
  \item \textbf{Access and security} -- 
  ACL toggles and lists: 
  acl\_user\_enable/acl\_users, 
  acl\_group\_enable/acl\_groups,
   or acl\_host\_enable/acl\_hosts.
  
  \item \textbf{Organizational tags} -- optional attributes such as fairshare\_tree (fairshare hierarchy) or partition (infrastructure label/partition) for logical segmentation and policy purposes.
  \cite{pbs2022}
\end{itemize}

\subsubsection{Nodes}
\label{subsubsec:nodes}

A node represents a physical machine in the PBS environment that provides computational resources for job execution. Each node is managed by the PBS server and can be in various operational states depending on its availability and current workload.  PBS provides the following data about nodes: \cite{pbs2022}

\begin{itemize}
  \item \textbf{Identity and location} -- node name (vnode), hostname (host), cluster identifier (resources\_available.cluster), and the name of the execution daemon (Mom) that manages the node. 
  
  \item \textbf{State and availability} -- primary state (state) indicating the operational status (e.g., free, job-busy, down, offline) and auxiliary state (state\_aux) providing additional context. 
  
  \item \textbf{Available resources} -- total capacity of the node defined by \texttt{resources\_available.*} attributes, including:
  \begin{itemize}
    \item CPU resources: ncpus (number of CPUs), pcpus (physical CPUs), cpu\_vendor, cpu\_flag (CPU feature flags)
    \item Memory: mem (total memory), vmem (virtual memory), hpmem (high-performance memory)
    \item Accelerators: ngpus (number of GPUs), gpu\_mem, gpu\_cap (compute capability), cuda\_version
    \item Scratch storage: scratch\_local, scratch\_shared, scratch\_ssd, scratch\_shm (shared memory)
    \item Network: ethernet\_speed, infiniband
    \item Software: singularity (container support), os, osfamily, arch
    \item Organizational: queue\_list (queues accessible from this node), cluster-specific tags
  \end{itemize}
  
  \item \textbf{Assigned resources} -- currently allocated resources tracked by \texttt{resources\_assigned.*} attributes, including:
  \begin{itemize}
    \item ncpus, ngpus, naccelerators (number of assigned CPUs, GPUs, accelerators)
    \item mem, vmem, hbmem, accelerator\_memory (assigned memory)
    \item scratch\_local, scratch\_ssd (assigned storage)
  \end{itemize}
  
  \item \textbf{Active jobs} -- list of job identifiers (jobs) currently running on the node, with each job identified by its ID and task index (e.g., 14964063.pbs-m1.metacentrum.cz/0). 
  
  \item \textbf{Sharing and placement} -- sharing mode (\texttt{sharing}) that determines resource allocation (e.g., \texttt{default\_shared}, \texttt{force\_exclusive}), and reservation support (\texttt{resv\_enable}). 
  
  \item \textbf{Timestamps} -- last\_state\_change\_time (timestamp of the last state transition) and last\_used\_time (timestamp when the node was last utilized). \cite{pbs2022}
\end{itemize}

The distinction between available and assigned resources enables monitoring of node utilization, while the state information provides insight into node availability for job scheduling. The jobs list allows tracking which specific jobs are consuming resources on each node, which is essential for resource accounting and troubleshooting. 

\subsubsection{Reservations}
\label{subsubsec:reservations}

Reservations are a mechanism in PBS that allows nodes to be reserved for exclusive use during a specific time period. When a reservation is created, the specified nodes become unavailable for regular job scheduling. For each reservation, PBS automatically creates a dedicated queue that provides access to the reserved resources. \cite{pbs2022}

PBS provides the following data about reservations:

\begin{itemize}
  \item \textbf{Identity} -- reservation name (Reserve\_Name), unique identifier (name), and the associated queue name (queue) that is created for the reservation.
  
  \item \textbf{Ownership and access} -- reservation owner (Reserve\_Owner) and list of authorized users (Authorized\_Users).
  
  \item \textbf{State information} -- reservation state (reserve\_state) and substate (reserve\_substate) indicating the current status of the reservation (e.g., active, confirmed, or in transition).
  
  \item \textbf{Time constraints} -- reservation start time (reserve\_start), end time (reserve\_end), and duration (reserve\_duration) specified as Unix timestamps.
  
  \item \textbf{Resource requirements} -- the resources reserved by the reservation, including:
  \begin{itemize}
    \item Memory: Resource\_List.mem (total memory reserved)
    \item CPUs: Resource\_List.ncpus (number of CPUs)
    \item GPUs: Resource\_List.ngpus (number of GPUs, if applicable)
    \item Nodes: Resource\_List.nodect (number of nodes) and Resource\_List.select (detailed node selection specification)
    \item Placement: Resource\_List.place (placement policy, e.g., \texttt{free}, \texttt{exclhost})
    \item Walltime: Resource\_List.walltime (maximum execution time for jobs in the reservation)
  \end{itemize}
  
  \item \textbf{Reserved nodes} -- list of actual nodes allocated to the reservation (resv\_nodes) with their specific resource allocations.
  
  \item \textbf{Metadata} -- creation time (ctime), modification time (mtime), submission host (Submit\_Host), and partition information (partition, if applicable).
  
  \item \textbf{Retry information} -- reservation count (reserve\_count) and retry attempts (reserve\_retry) for tracking reservation lifecycle. \cite{pbs2022}
\end{itemize}


\subsubsection{Resources}
\label{subsubsec:resources}

Resources in PBS represent units of computational capacity that can be requested by jobs, allocated to nodes, and managed by queues and the server. Each resource is defined with a unique name and attributes that specify its data type and where it can be used within the PBS system. \cite{pbs2022} 

PBS provides a comprehensive list of all available resources in the system, where each resource definition includes: \cite{pbs2022}

\begin{itemize}
  \item \textbf{Resource name} -- the identifier used to reference the resource in job submissions, node configurations, and queue settings (e.g., \texttt{cput}, \texttt{mem}, \texttt{ncpus}, \texttt{ngpus}, \texttt{walltime}).
  
  \item \textbf{Type} -- the data type of the resource value, which determines how the resource is interpreted and validated. Common types include:
  \begin{itemize}
    \item \texttt{long} -- integer values (e.g., CPU count, GPU count)
    \item \texttt{size} -- memory or storage values with units (e.g., bytes, KB, MB, GB)
    \item \texttt{string} -- text values
    \item \texttt{float} -- floating-point numeric values
    \item \texttt{time} -- time duration values
  \end{itemize}
  
  \item \textbf{Flag} -- a string of characters indicating where the resource can be used or referenced:
  \begin{itemize}
    \item \texttt{h} -- can be used at the host/node level
    \item \texttt{q} -- can be used at the queue level
    \item \texttt{n} -- can be used at the node level
    \item \texttt{m} -- can be used at the server level 
  \end{itemize}\cite{pbs2022} 
\end{itemize}

\subsubsection{Scheduler status}
\label{subsubsec:scheduler-status}

The PBS scheduler is responsible for making decisions about which jobs to run, when to run them, and on which nodes to execute them.  A PBS system can have multiple schedulers, each managing a specific partition or set of resources. The scheduler continuously evaluates queued jobs against available resources and applies scheduling policies to optimize resource utilization and meet job requirements. \cite{pbs2022}

PBS provides status information about each scheduler instance, including: \cite{pbs2022}

\begin{itemize}
  \item \textbf{Identity and location} -- scheduler name, the host where the scheduler daemon runs (sched\_host), and the partition it manages (partition).
  
  \item \textbf{Operational state} -- current state of the scheduler (e.g., \texttt{scheduling}, \texttt{idle}) and whether scheduling is enabled (scheduling).
  
  \item \textbf{Scheduling cycle} -- scheduler cycle length (sched\_cycle\_length) defining how frequently the scheduler evaluates and schedules jobs, and the current iteration count (scheduler\_iteration).
  
  \item \textbf{Processor set configuration} -- settings for processor set (pset) handling: do\_not\_span\_psets (prevents jobs from spanning multiple processor sets) and only\_explicit\_psets (restricts scheduling to explicitly defined processor sets).
  
  \item \textbf{Scheduling mode} -- throughput\_mode indicates whether the scheduler prioritizes throughput optimization, and opt\_backfill\_fuzzy (if present) specifies the backfill optimization level.
  
  \item \textbf{Preemption settings} -- configuration for job preemption: preempt\_queue\_prio (priority threshold for preemption), preempt\_prio (queues or job types that can be preempted), preempt\_order (preemption order strategy), and preempt\_sort (sorting method for preemption selection, e.g., \texttt{min\_time\_since\_start}).
  
  \item \textbf{Integration and hooks} -- job\_run\_wait specifies the hook that controls when jobs can start execution (e.g., \texttt{runjob\_hook}).
  
  \item \textbf{File system paths} -- sched\_priv (path to scheduler private directory) and sched\_log (path to scheduler log files).
  
  \item \textbf{Logging and monitoring} -- log\_events (bitmask specifying which events to log) and server\_dyn\_res\_alarm (alarm threshold for dynamic resource changes). \cite{pbs2022}
\end{itemize}

\subsubsection{Fairshare metrics}
\label{subsubsec:fairshare-metrics}

A list of users, their fairshare values, and the timestamp when the record was last modified. 
fairshare values are used per scheduler.\cite{pbs2022}
This data is retrieved for both the QSUB assembler and user monitoring purposes.

\newpage
\subsection{Data Collection Mechanism}
\label{subsec:data-collection-mechanism}

\subsubsection{Batch Interface Library (IFL)}
\label{subsubsec:batch-interface-library}
PBS provides a C library, which represents the programming interface (API) of the PBS system, also known as the Batch Interface Library (IFL). \cite{pbs2022} 

This library allows external applications and tools to communicate with the PBS server. It provides functions for remote management of batch jobs, querying the system state, and managing computational resources through TCP/IP communication. Using the library, it is possible to implement a client application that: \cite{pbs2022} 

\begin{itemize}
  \item establishes a connection to the server (pbs\_connect),

  \item authenticates the user,
  
  \item creates and submits jobs (pbs\_submit),

  \item queries their status (pbs\_statjob, pbs\_selstat),
   
  \item modifies or deletes jobs (pbs\_alterjob, pbs\_deljob),
  
  \item works with information about the server, queues, nodes, or scheduler.\cite{pbs2022}
\end{itemize}   

Thus, library represents a key component for implementing a tool that enables data collection and monitoring of jobs managed by the PBS server.


Subsequently, current PBSmon implementation contains C code that, when invoked, retrieves information from the PBS server using the library functions and then stores this data into a file, which the Java application later processes and saves into the in-memory store (see Listing \ref{lst:pbs_connect_example}).

This collection process is triggered whenever the user wants to display any PBSmon page and when the data in the memory cache is older than 60 seconds.

Another important note is that collected data represents the current state of the PBS server. It does not include any historical data. \label{note:historical-data}

\newpage
\begin{lstlisting}[caption={Partial code snippet for data collection from PBS server},
  language=c, label={lst:pbs_connect_example}]
  #include <pbs_error.h>
  #include <pbs_ifl.h>

  int main(int argc, char **argv) {
      // ...
      con = pbs_connect(server);    
      if(con<0) {
          return 1;
      }
      /* get server info */
      bs = pbs_statserver(con, NULL, NULL);
      process_data(bs,"servers");
      /* get queues info */
      bs = pbs_statque(con, "", NULL, NULL);
      process_data(bs,"queues");
      /* get nodes info */
      bs = pbs_statnode(con, "", NULL, NULL);
      process_data(bs,"nodes");
      /* get jobs info: t - job arrays, x - finished jobs*/
      bs = pbs_statjob(con, "", NULL, "tx");
      process_data(bs,"jobs");
      /* get reservations info */
      bs = pbs_statresv(con, NULL, NULL, NULL);
      process_data(bs,"reservations");
      /* get resources info */
      bs = pbs_statrsc(con, NULL, NULL, NULL);
      process_data(bs,"resources");
      /* get scheduler info */
      bs = pbs_statsched(con, NULL, NULL);
      process_data(bs,"schedulers");
      /* end connection */
      pbs_disconnect(con);
      return 0;
  }  
\end{lstlisting}

\subsubsection{Fairshare metrics collection}
\label{subsubsec:fairshare-metrics-collection}

Fairshare metrics are collected using the bash command.

\begin{lstlisting}[caption={Metrics collection script},
  language=bash, label={lst:fairshare-metrics-collection}]
  list_cache <pbsServer> fairshare{.elixir}
\end{lstlisting}


This shell command returns a CSV file with columns:

user, last\_modified, fairshare.

\subsubsection{Group file collection}
\label{subsubsec:group-file-collection}

In addition to entities and caches, the shell script located on PBS server automatically pushes its /etc/group file. These files provide information about local UNIX groups and their members. Whenever time it has changed, the PBS server writes the updated version to the PBSmon server's filesystem under the following path: \textbf{/etc/pbsmon/group/<pbsServer>}.


\newpage
\section{Data Collection from Perun}
Perun is an open-source system developed in Java that serves for comprehensive management of identities, groups, attributes, and access to various resources and services. It is a modular solution designed for efficient management of users, organizations, and projects. The system is built with an emphasis on operation in distributed environments and on integration with existing identity systems in the fields of research and education.\cite{perun2025}

\subsection{Entities in Perun}
\label{subsec:entities-in-perun}
Entities retrieved from Perun are two independent data domains — users and machines. Each domain is exported by Perun as a separate JSON file. The detailed description of these entities:

\subsubsection{Users}
\label{subsubsec:users-perun}
The user dataset contains information about all registered users in the MetaCentrum infrastructure, including their identifiers, names, organizational affiliations, and assigned virtual organizations. These data are used to enrich job statistics and to provide a link between computational activity and user identity.

\subsubsection{Machines}
\label{subsubsec:machines-perun}
In addition to management of users, Perun also collect information about physical computing resources of the MetaCentrum infrastructure.
These data are then passed to PBSmon from Perun. Data are hierachically structured and grouped by Organization -> Cluster -> Computing Node.

Each record is an institution that has at least one cluster. Cluster is a group of computing nodes that are owned by the same institution. Computing node is a physical machine that is part of the cluster.

Each computing node is described by the following metadata:

\begin{itemize}
  \item CPU configuration
  \item Memory
  \item Storage
  \item Owner institution
  \item The list of individual node hostnames
\end{itemize}

These data are neccessary to get additional information about the complete information about the computing node for the running PBS jobs.
Additionally, these data independetly from PBS are important for knowing the complete information about the computing nodes for whole MetaCentrum infrastructure.


\subsection{Data Collection Mechanism}
\label{subsec:data-collection-mechanism-perun}

The integration of the PBSmon system with Perun is designed using a PUSH model. Instead of direct access to the Perun database or invoking its API, Perun periodically generates JSON files containing all relevant information about users and computing resources. These files are then transferred via SSH directly to the PBSmon server, where they are stored in the filesystem, and subsequently loaded and processed by PBSmon. These files are stored in the filesystem as following:

\begin{itemize}
  \item \textbf{/etc/pbsmon/pbsmon\_users.json} 
  \item \textbf{/etc/pbsmon/pbsmon\_machines.json}
\end{itemize}

\subsubsection{Change Detection and Synchronization}
Currently, whenever user opens any page in PBSmon, the system checks if the files were modified since last load. If they were, the files are loaded and processed by PBSmon.


\newpage 
\section{Data Collection from Virtual Machines (OpenStack)}
OpenStack is a cloud operating system that controls large pools of compute, storage, and networking resources throughout a datacenter, all managed and provisioned through APIs with common authentication mechanisms. \cite{openstack2025}

Beyond standard infrastructure-as-a-service functionality, additional components provide orchestration, fault management and service management amongst other services to provide operators flexibility to customize their infrastructure and ensure high availability of user applications. \cite{openstack2025}

Within Metacentrum infrastructure, OpenStack is used to provide virtual machines to the users with specific needs that are not covered by the PBS environment. There are some computing nodes that are part of MetaCentrum infrastructure, but are reserved ad hoc for OpenStack. Within nodes returned by Perun mentioned in \ref{subsubsec:machines-perun} are also nodes that are used for OpenStack.

Current PBSmon implementation gets very limited information about OpenStack virtual machines for each cluster. For each cluster, there is a list of virtual machines with their reserved CPU, name and user ID.

 
\subsection{Data Collection Mechanism}
\label{subsec:data-collection-mechanism-openstack}

The integration of the PBSmon system with OpenStack is designed using a PUSH model. Instead of direct access to the API, JSON files are transferred via SSH directly to the PBSmon server, where they are stored in the filesystem, and subsequently loaded and processed by PBSmon. For each cluster, there is a separate file with the list of virtual machines with their reserved CPU, name and user ID. 

An example of the response structure for the OpenStack cluster named "glados" is provided in Appendix \ref{app:openstack-response-example}.


\newpage
\section{Retrieval of historical data}
\label{sec:collection-of-historical-data}

As mentioned in section \ref{note:historical-data}, the data collected by PBSmon represents only the current state of the PBS infrastructure. To access historical information about completed jobs, their resource usage, and long-term statistics, a separate application is responsible for collecting and storing historical PBS data in a PostgreSQL database.

This historical data collection system continuously records information about finished jobs, including their execution times, resource consumption, and user associations.

\section{Authentication}
\label{sec:authentication}
The current PBSmon solution uses third-party authentication provided by Perun's e-INFRA system. Authentication is implemented using the OpenID Connect (OIDC) protocol, which allows users to authenticate through the centralized identity provider managed by Perun.  \cite{perun2025}

This approach ensures that only authorized users with valid credentials from the Perun identity management system can access the PBSmon web interface.  \cite{perun2025}

The authentication flow follows the standard OIDC protocol, where users are redirected to the Proxy IdP for login, and upon successful authentication, an OIDC token is issued and used to grant access to the PBSmon application. \cite{perun2025}

\section{Web Layer and User Interface}
\label{sec:web-layer-and-user-interface}
The current PBSmon application is integrated as part of the MetaVO portal (\url{https://metavo.metacentrum.cz}), which serves as the main web interface for MetaCentrum services. Within MetaVO, PBSmon is encapsulated as a single subsection called "Current state" (see \ref{fig:pbsmon}). This section is available only to logged-in users who have been granted access to PBSmon through the authentication system described in section \ref{sec:authentication}.

The "Current state" subsection consists of the following pages:

\begin{itemize}
  \item \textbf{Personal view} - A dashboard with information about the logged-in person, including their job statistics, jobs queues and quick access to other relevant pages
  \item \textbf{QSUB assembler} - A tool for assembling commands for job submission, described in detail in subsection \ref{subsec:qsub-assembler}
  \item \textbf{Physical machines} - A list of Metacentrum grid infrastructure retrieven from Perun nodes with PBS mapping
  \item \textbf{PBS node state} - A list of all PBS nodes and their current status. Without mapping to the Perun physical machines.
  \item \textbf{Virtual machines} - A list of all virtual machines (no longer relevant in the current infrastructure)
  \item \textbf{Jobs queues} - A list of queues and reservations
  \item \textbf{Jobs} - A page with total statistics and navigation hub to "My jobs", "All jobs", and "Suspicious jobs"
  \item \textbf{User} - A list of users from Perun and their current CPU usage
  \item \textbf{Machine properties} - A list of all PBS nodes and their properties (e.g., architecture, OS family, cluster, cgroups, etc.). Page allows to click on the property to see all the nodes that have this property.
  \item \textbf{List of hardware} - A list of organizations and their clusters
  \item \textbf{Cloud} - Cloud-related information (no longer relevant)
\end{itemize}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{figures/pbsmon.png}
  \caption{PBSmon layout}
  \label{fig:pbsmon}
\end{figure}

\newpage
\subsection{Personal view}
\label{subsec:personal-view}
The Personal view serves as the main dashboard for users, representing one of the most important and frequently used features of PBSmon. This view provides users with an overview of their job statistics and quick access to other relevant pages.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{figures/old-pbsmon-personal-view.png}
  \caption{Personal view dashboard in the old PBSmon}
  \label{fig:old-pbsmon-personal-view}
\end{figure}

The dashboard displays total counts of jobs associated with the user. However, since PBS always returns the current state of the system, the Personal view focuses on showing counts of \textit{relevant} jobs---those that are considered active. Specifically, this includes jobs that are currently in the queue, currently running, or have finished within the previous few days. This filtering ensures that users see meaningful information about their recent and ongoing computational work, rather than being overwhelmed by historical data from all past jobs.

The Personal view also contains links to other pages within the system. One such link leads to the "My jobs" page, which displays detailed information about the user's jobs. For user the list of his jobs are highly relevant, therefore it can be directly incorporated into the new dashboard within the new solution.

The page also displays all jobs queues that are accessible by the user. While this information is useful, it is considered less relevant and it is not necessary to be included in the new dashboard.

One limitation of the current implementation is that the dashboard only displays total CPU usage and does not include total GPU resources. This limitation stems from the fact that GPU usage was not a significant concern at the start of the old PBSmon development, but has since become increasingly important as GPU computing has gained prominence in the MetaCentrum infrastructure.


\subsection{QSUB assembler}
\label{subsec:qsub-assembler}
The QSUB assembler is one of the most important features of the PBSmon web interface, designed to assist users in submitting jobs to the MetaCentrum infrastructure. The \texttt{qsub} command is a CLI (Command Line Interface) command that allows users to submit jobs to the PBS system. This page provides a form-based interface for configuring job submission parameters.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/old-pbsmon-qsub-assembler.png}
  \caption{QSUB assembler interface}
  \label{fig:qsub-assembler}
\end{figure}

The QSUB assembler page presents users with an extensive form containing numerous parameters that can be set for job submission, such as resource requirements (CPU flags, memory, scratch space, architecture), queue selection, walltime, and other MetaCentrum-specific options. 

The current user interface presents parameters in a form equivalent to their syntax in the \texttt{qsub} command, which has the advantage that users gradually learn the \texttt{qsub} command syntax. However, the interface lacks supplementary explanations for individual fields, and therefore it is not always clear what the purpose of some less known or specific parameters is (e.g., \texttt{luna}, \texttt{umg}). The UI contains names directly according to PBS attributes, but without context or brief help, it can be difficult for users to understand what exactly these choices represent or how they will affect job execution.

After configuring the desired parameters, the page automatically generates a complete shell command that can be used in two ways:

\begin{itemize}
  \item Directly inserted into a terminal and executed to submit the job immediately
  \item Inserted as a comment directive in a PBS job script (e.g., \texttt{\#PBS -l ...}) for later execution
\end{itemize}

Upon confirmation (submit), the page performs a query against the available MetaCentrum infrastructure to determine job feasibility. The results are presented in two key overviews:

\begin{itemize}
  \item \textbf{Number of nodes meeting the requirements}: This shows the total count of nodes that are compatible with the specified parameters (e.g., nodes matching the CPU flags, memory requirements, scratch space, architecture, and other constraints)
  \item \textbf{Number of available nodes}: This displays the count of free nodes from the compatible set at the current moment, representing the real-time availability for job execution
\end{itemize}

Following these summary statistics, the page displays a detailed list of all nodes that match the specified requirements, providing users with list of nodes that can be used to submit the job.

\section{Summary of the Analysis}
\label{sec:summary-of-analysis}

The analysis of the existing PBSmon solution reveals a system that has successfully served its purpose of monitoring and visualizing MetaCentrum infrastructure data. The application effectively integrates data from multiple sources (PBS, Perun, and OpenStack) and provides users with  views of computational resources, jobs, and system status. Key strengths include the QSUB assembler feature, which assists users in constructing job submission commands, and the successful integration with Perun's authentication system.

However, the analysis also identifies several significant limitations and areas for improvement. The system suffers from a \textbf{monolithic architecture} that tightly couples all components, making it difficult to maintain, extend, or deploy in modern containerized environments. The absence of dockerization further complicates deployment and scaling. The application relies on \textbf{outdated libraries}, most notably the Java library Stripes, which is no longer maintained, creating security and sustainability concerns.

From a user experience perspective, the interface presents parameters in a form that mirrors the \texttt{qsub} command syntax, which helps users learn PBS commands but lacks supplementary explanations for individual fields. This makes it difficult for users to understand the purpose of less common parameters (e.g., \texttt{luna}, \texttt{umg}) or how their choices will affect job execution. The web interface uses \textbf{server-side rendering} and is not implemented as a standalone frontend application, limiting flexibility and modern user experience capabilities.

The system also exhibits limitations in meeting current infrastructure demands. As GPU computing has gained prominence in MetaCentrum, the dashboard only displays total CPU usage statistics and does not include GPU resources or GPU time usage metrics. This reflects a common occurence with long-serving projects.

Another concern identified is a \textbf{potential GDPR violation}: the system displays information about all users that currently have active jobs and their jobs within MetaCentrum, which may not comply with data protection regulations regarding the visibility of personal computational activity to other users.

In summary, while PBSmon has been functional and valuable, its monolithic architecture, outdated technology stack, lack of containerization support, suboptimal user experience, incomplete feature coverage for modern computing needs, and potential privacy compliance issues necessitate a fundamental redesign and modernization of the system.


\chapter{Relevant applications and technologies}
\label{chap:relevant-applications-and-technologies}

This chapter summarizes selected applications and technologies related to the operation and future evolution of the MetaCentrum computing infrastructure, as well as monitoring and user-facing access patterns relevant to this thesis. Besides the PBS-based grid environment, MetaCentrum also provides web-based user portals, project and allocation tooling, metrics-based monitoring stacks, and container platforms. These components form an ecosystem in which the modernized PBSmon solution is expected to operate and where future integrations may be considered.

\section{Open OnDemand}
Open OnDemand is a web-based portal designed to provide end users with interactive access to computing resources directly from a web browser \cite{Hudak2018OpenOnDemand}. In MetaCentrum, the OnDemand instance enables users to work with files through a graphical file manager, run interactive applications, and create or modify batch jobs using a GUI-oriented workflow \cite{MetacentrumOnDemandDocs}.

A key benefit of OnDemand is the ability to start interactive workloads (e.g., desktop sessions or specialized applications) without requiring local software installation, which reduces the entry barrier for less experienced users and accelerates experimentation \cite{MetacentrumOnDemandDocs}. In addition, OnDemand provides browser-based shell access to selected frontends, effectively allowing users to open a terminal session in the browser and submit jobs using standard cluster tooling when needed \cite{MetacentrumOnDemandDocs}.

From the perspective of monitoring, OnDemand primarily targets user interaction and job submission. It intentionally presents a simplified view tailored to the logged-in user workflow (files, sessions, job composer) and does not aim to replace an operator-oriented monitoring tool that exposes detailed, cluster-wide state and relationships between PBS entities (nodes, queues, scheduling policies, historical trends). Furthermore, authentication is performed using the user's academic identity, which limits administrative impersonation-style workflows and cross-user views compared to dedicated monitoring or administration tooling \cite{MetacentrumOnDemandDocs,Hudak2018OpenOnDemand}.

\section{Zeus}
Zeus is a CESNET web application developed for MetaCentrum to support project and resource allocation workflows for both PBS-managed compute resources and cloud resources. The system focuses on capturing and managing project-related processes, enabling users to request quotas, and allowing administrators to allocate requested resources. An important part of the solution is integration with the Perun identity and access management system, including propagation of project information towards computing resources \cite{Valalsky2025thesis}.

Compared to PBSmon, Zeus is not a monitoring tool; it addresses a different layer of the infrastructure lifecycle (project management and allocation rather than operational state visualization). Nevertheless, Zeus is relevant when considering the long-term evolution of the monitoring ecosystem: allocation and project metadata can become an important enrichment signal for monitoring dashboards (e.g., mapping jobs and users to projects, quotas, or allocation decisions). Therefore, Zeus is considered a potential integration point for future PBSmon extensions \cite{Valalsky2025thesis}.

\section{Prometheus}
Prometheus is an open-source monitoring and alerting toolkit that collects and stores metrics as time series enriched by labels. It is widely used for infrastructure and application monitoring and provides a flexible query language (PromQL) for aggregations and alerting \cite{PrometheusOverview,Brazil2018Prometheus}. Prometheus is typically deployed in a pull-based model, where the Prometheus server scrapes metrics endpoints exposed by monitored targets at regular intervals \cite{PrometheusOverview}.

In this thesis, Prometheus is therefore considered primarily as a complementary monitoring technology for the OpenStack part of the infrastructure, not as a replacement for PBS entity monitoring. While PBSmon focuses on presenting structured state of PBS objects (jobs, nodes, queues) and their relationships, Prometheus provides metrics for OpenStack projects and infrastructure. Both approaches can coexist: Current PBSmon provides primarily domain-specific views of PBS entities, whereas Prometheus-based monitoring can cover cloud services and selected operational indicators in parallel. This thesis aims to integrate OpenStack metrics into the new PBSmon solution.


\subsection{Thanos}
Thanos is a set of components that extends Prometheus deployments with features such as long-term storage in object storage, high availability, and a global query view across multiple Prometheus instances \cite{ThanosWebsite,ThanosGitHub}. In practice, Thanos addresses typical Prometheus limitations related to retention, high-availability operation, and querying across multiple Prometheus servers \cite{ThanosWebsite,ThanosGitHub}.

This thesis does not adopt Thanos, because the primary goal is to provide a reliable view of the current operational state of the MetaCentrum PBS-based grid environment, rather than to design a long-term metrics retention and global query layer. Nevertheless, Thanos remains relevant as a future consideration if PBSmon (or surrounding infrastructure) evolves towards stronger historical analytics.

\section{Kubernetes}
Kubernetes is an open-source container orchestration platform for automating deployment, scaling, and management of containerized applications \cite{KubernetesDocs,Burns2022Kubernetes}. It provides a declarative model for running workloads, service discovery, and operational automation that is now standard in cloud-native environments \cite{KubernetesDocs,Burns2022Kubernetes}.

MetaCentrum (within e-INFRA CZ) also operates a container platform based on Kubernetes, including a Rancher-based distribution and related user-facing tooling \cite{EinfraHPCKubernetes}. This is relevant for PBSmon because Kubernetes-based services are part of the broader infrastructure portfolio and strengthen the requirement for modularity and extensibility of monitoring solutions.

\chapter{Design of the New Solution}
\label{chap:design}
Building upon the analysis in Chapter~\ref{chap:analysis}, this chapter focuses on the design of a modern replacement for the original PBSmon system. The goal is to propose a solution that not only preserves the existing functionality, but also improves maintainability, usability and alignment with current infrastructure and security requirements.

In addition to the functional rewriting of the existing features, the new design must introduce first-class support for working with OpenStack. In particular, it has to enable users to view OpenStack projects, the virtual machines assigned to them, and other relevant metadata in a clear and structured manner consistent with how resources are organised in the underlying cloud environment.

Furthermore, the design addresses the potential GDPR issue identified in Chapter~\ref{chap:analysis}, where personal computational activity may become visible to other users. To mitigate this risk, the new solution introduces a simple role model with two distinct roles: \emph{user} and \emph{admin}. This separation of privileges restricts access to sensitive information and ensures that operations affecting other users or system-wide configuration are only available to administrators.

The remainder of this chapter is structured as follows. Section~\ref{sec:functional-requirements} defines the functional requirements derived from the analysis and target use cases. Section~\ref{sec:gui-design} outlines the user interface design of the new application. Section~\ref{sec:architecture-design} describes the overall system architecture, including the division into backend and frontend parts. Finally, Section~\ref{sec:selected-technologies} justifies the selection of the technologies used to implement the proposed design.

\section{Functional Requirements}
\label{sec:functional-requirements}

The goal of the new solution is to provide a read-only monitoring application that aggregates information from multiple data sources used in MetaCentrum. The system does not modify the state of the underlying infrastructure; instead, it offers a unified user interface for viewing job information, infrastructure topology and cloud resources. Access to this information is controlled by a simple role model with two roles: \emph{user} and \emph{admin}. An administrator can perform all actions available to a regular user, plus additional operations related to support and diagnostics.

\subsection{Domain Entities and Data Sources}
\label{sec:domain-entities}

The new solution reuses most of the domain model of the original PBSmon application and extends it with entities from the OpenStack environment. PBS and Perun entities were already introduced in the description of the existing solution (see Chapter~\ref{chap:analysis}); this section briefly summarises them and then focuses on the new OpenStack concepts that motivate several of the following requirements.

\subsubsection*{PBS and Perun (recap)}

From PBS, the application uses primarily jobs, nodes, queues, reservations and fairshare entries, together with UNIX groups obtained from the \texttt{/etc/group} file to determine relationships between users. From Perun, it reuses the list of users with richer metadata (full name, organisation, \dots) and the description of the MetaCentrum infrastructure, including organisations, clusters and their nodes. For a detailed description of these entities, see Chapter~\ref{chap:analysis}.

\subsubsection*{OpenStack entities}

The new solution also obtains more detailed information from the OpenStack cloud environment. Two main entities are relevant for the design:

\begin{itemize}
  \item \textbf{Project} -- a logical container that owns cloud resources. Each project has an owner (either an individual user or a group), a date of creation and assigned resources (quotas). Projects can therefore represent both individual and group activities.
  \item \textbf{Virtual machine (VM)} -- a virtual machine instance assigned to a specific project. For each VM, the system needs to track at least its name, associated project and creation time, so that it can be attributed to the correct user or group in the monitoring interface.
\end{itemize}

These OpenStack entities are used in the functional requirements below, in particular in the personal view for users and in the infrastructure overview.

\subsection{Roles and Access Control}
The system defines two roles:

\begin{itemize}
  \item \textbf{User} -- a regular MetaCentrum user. A user has access primarily to information related to their own activity and to entities connected to their UNIX groups. Global information is either anonymised or aggregated where necessary.
  \item \textbf{Admin} -- an administrative user. An admin can see complete, non-anonymised information across all users and resources and can impersonate other users for troubleshooting and support. An admin has all capabilities of a regular user.
\end{itemize}

\subsection{Functional Requirements for Users}

A regular user must be able to use the application as a personal monitoring dashboard and as a tool for exploring the infrastructure relevant to their work. The following functional requirements apply:

\begin{description}
  \item[FR-01 Personal overview]  
  The system shall provide a personal view (dashboard) where a user can see a summary of their total usage statistics, their recent and active PBS jobs, and their OpenStack projects.

  \item[FR-02 View own jobs]  
  The system shall allow a user to list and inspect the details of their own PBS jobs, including state, queue, submission time, requested resources and basic runtime information.

  \item[FR-03 View global jobs with anonymisation]  
  The system shall allow a user to browse all PBS jobs in the system. Jobs owned by other users shall be anonymised, except when the owner is:
  \begin{itemize}
    \item the current user, or
    \item a user belonging to one of the same UNIX groups as the current user.
  \end{itemize}
  In these cases, the real user identity shall be shown.

  \item[FR-04 View OpenStack projects and VMs]  
  The system shall allow a user to view all OpenStack projects associated with them and to list the virtual machines assigned to each of these projects, including basic metadata (such as name and creation date).

  \item[FR-05 View queues]  
  The system shall provide an overview of all PBS queues, including their basic configuration and purpose, so that users can understand where their jobs are being scheduled.

  \item[FR-06 View infrastructure and reservations]  
  The system shall provide a view of the MetaCentrum infrastructure based on Perun data, showing organisations, clusters and nodes, together with information about existing reservations on these resources. Only reservation information shall be displayed; actual usage of the resources is out of scope due to missing data.

  \item[FR-07 View users within groups]  
  The system shall allow a user to see basic information about other users who belong to the same UNIX groups, using data from \texttt{/etc/group}.

  \item[FR-08 View own group membership]  
  The system shall display the current user's group membership as derived from \texttt{/etc/group}, so that the user understands which groups they are associated with.

  \item[FR-09 View fairshare information]  
  The system shall display fairshare information relevant to the current user, derived from the PBS fairshare entries, and optionally allow comparison with other users in the same groups.

  \item[FR-10 QSUB assembler]  
  The system shall provide a ``QSUB assembler'' -- an interactive interface that helps the user construct a valid \texttt{qsub} command by selecting queues, resources and other parameters. The result shall be presented in a form that can be copied and used in a terminal. Alltogether with the list of nodes that fullfils the requirements.

  \item[FR-11 Detail pages]  
  For all major entities (job, node, queue, project, VM, reservation, user), the system shall offer detail pages accessible from overview tables, with information limited according to the user's role and the anonymisation rules described above.
\end{description}

\subsection{Functional Requirements for Admins}

Administrators require a complete, non-anonymised view of the system in order to support users and diagnose problems. In addition to FR-01~--~FR-11, the following requirements apply:

\begin{description}
  \item[FR-12 Full visibility]  
  The system shall allow an admin to see all available data for all users, jobs, queues, infrastructure elements, projects and VMs without anonymisation.

  \item[FR-13 User impersonation]  
  The system shall allow an admin to impersonate a selected user, i.e., to temporarily view the application exactly as that user would see it (including all access restrictions and anonymisation). This functionality is intended solely for support and debugging.

\end{description}

These functional requirements form the basis for the graphical user interface design described in Section~\ref{sec:gui-design} and for the architecture of the new solution discussed in Section~\ref{sec:architecture-design}.

\newpage
\section{Graphical User Interface Design}
\label{sec:gui-design}

A key part of the new solution is the redesign of the graphical user interface (GUI). The existing application has grown over time and its interface is not intuitive for new users, which makes common tasks unnecessarily difficult and increases the learning curve. To improve usability, it was necessary to propose a clearer and more consistent user interface that better reflects typical workflows and makes important information easier to find.

At the same time, the current users are already familiar with the existing interface and rely on established interaction patterns. A radical change to the layout or navigation could therefore have a negative impact on productivity and would likely be met with resistance. The GUI design of the new solution therefore aims for an evolutionary rather than revolutionary change: it introduces a more intuitive structure and visual hierarchy, but preserves similar navigation concepts and overall information density so that experienced users can adapt quickly.

Another important requirement was to keep the visual identity of the MetaCentrum computing grid\footnote{\url{https://www.metacentrum.cz/en/}}. The brand colour is a distinctive shade of orange, which is already used in existing tools and documentation and therefore forms an important part of the user experience. However, large orange areas on the screen can be visually tiring and, in extreme cases, reduce readability. For this reason, the new interface is based on a dark grey theme that serves as a neutral background, while orange is used as an accent colour for interactive elements such as primary buttons, active navigation items and important status indicators. This approach preserves the recognisable MetaCentrum brand while improving visual comfort during long-term use.

For the design process, the Figma tool was used to create the visual specification of the interface. In Figma, a set of basic building blocks was first defined as reusable components, ensuring consistency across the entire application and simplifying future modifications. On top of these components, the main layout was designed, including a sidebar menu for navigation between the key parts of the application and a content area for displaying overviews and detail views. The resulting design of the new personal view is illustrated in Figure~\ref{fig:new-pbsmon-gui-design} and serves both as a communication tool with stakeholders and as a reference for the implementation of the frontend.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{figures/figma-personal-view.png}
  \caption{Figma design of the new personal view}
  \label{fig:new-pbsmon-gui-design}
\end{figure}


\section{Architecture Design}
\label{sec:architecture-design}

The new solution is designed as a two-layer web application consisting of a backend API and a separate frontend. The backend exposes the domain data and operations through a JSON-based HTTP API, while the frontend is responsible for rendering the user interface and handling user interaction in the browser. This separation follows a common pattern used in modern web applications and reflects the requirements for flexibility, maintainability and potential reuse of the data layer.

Both layers are developed in a single monorepository (monorepo). The monorepo contains the API and frontend as separate packages (or subprojects) that may share common code where appropriate (for example domain models or API client definitions), while still preserving a clear logical separation between the two layers.

\subsection{Overview of the Two-Layer Architecture}

The backend API layer integrates data from PBS, Perun and OpenStack, transforms them into a unified domain model and enforces the access control rules based on the \emph{user} and \emph{admin} roles. It is implemented as a stateless web service that returns all responses in JSON format. Its only responsibility is to serve data and perform server-side computations such as aggregation, filtering and anonymisation.

The frontend layer is implemented as a separate web application that runs in the user's browser. It communicates with the API using HTTP requests, consumes the JSON responses and renders them into the graphical user interface described in Section~\ref{sec:gui-design}. The frontend handles navigation between views, local state management (e.g. filters and sorting) and presents the data in tables, charts and detail pages.

\subsection{API Layer}

The API layer acts as an integration and abstraction layer over the underlying systems:

\begin{itemize}
  \item It collects data from PBS, Perun and OpenStack and periodically fetches the necessary entities (jobs, nodes, queues, reservations, users, infrastructure descriptions, projects, virtual machines, \dots).
  \item It normalises and combines the collected data into a consistent model tailored to the needs of the monitoring application.
  \item It implements role-based access control and anonymisation rules as described in the functional requirements, including the distinction between regular users and administrators.
  \item It exposes the data through a set of endpoints that return JSON, covering overviews (e.g. lists of jobs or projects) as well as detail views for individual entities.
\end{itemize}

By concentrating all domain logic and access control in the API, the system ensures that the same rules are applied consistently regardless of the client and that sensitive operations cannot be bypassed by directly accessing the underlying systems.

\subsection{Frontend Layer}

The frontend layer consumes the API and transforms the JSON data into an interactive web interface:

\begin{itemize}
  \item It provides the personal view for users, including their own statistics, jobs and OpenStack projects.
  \item It renders overviews of jobs, queues, infrastructure and reservations, with client-side filtering and sorting where appropriate.
  \item It presents anonymised or full data depending on the current role, based on information obtained from the API.
  \item It implements the QSUB assembler, where the user selects parameters and the frontend assembles a command that can be copied to a terminal.
\end{itemize}

Because the frontend is decoupled from the backend at the API boundary, it can be developed and iterated independently at the application level, even though both parts live in the same monorepo. The visual design can evolve without changes to the data layer as long as the API contract remains stable.

\subsection{Rationale, Advantages and Disadvantages}

The choice of a two-layer architecture with a JSON-based API and a separate frontend, implemented together in a monorepo, was motivated by several considerations.

Among the main advantages are:

\begin{itemize}
  \item \textbf{Separation of concerns} -- the API focuses on data integration, business logic and security, while the frontend focuses on user experience and presentation. This separation simplifies reasoning about each part and reduces coupling.
  \item \textbf{Reuse and extensibility} -- the API can later be reused by other clients (for example command-line tools, scripts or additional internal applications) without changes to the core logic. New frontend features can be added as long as they build on the existing endpoints.
  \item \textbf{Monorepo code sharing} -- keeping the API and frontend in a single monorepo enables straightforward sharing of common code, such as type definitions, domain models or API client libraries, which reduces duplication and the risk of inconsistencies between layers.
  \item \textbf{Coordinated development} -- the monorepo makes it easy to evolve the API and frontend together in a single change set, while the architectural separation still allows developers to work on backend and frontend parts with clear boundaries.
  \item \textbf{Potential for independent deployment} -- even though the code is stored in a single repository, the API and frontend can be built and deployed as separate artefacts if needed (for example scaling the API independently), which is important for handling peaks in monitoring queries.
  \item \textbf{Security} -- all access control and anonymisation logic is centralised in the API layer, so there is a single place where role-based rules are enforced and audited.
\end{itemize}

The chosen architecture also has disadvantages that need to be acknowledged:

\begin{itemize}
  \item \textbf{Increased operational complexity compared to a monolithic server-rendered application} -- there are two logical components (API and frontend) that must be configured to communicate correctly, including handling of authentication and possible cross-origin restrictions, even though they are stored in one repository.
  \item \textbf{Network overhead} -- every interaction between the user and the data layer requires HTTP requests from the browser to the API, which may introduce additional latency compared to server-side rendering directly coupled to the data sources.
  \item \textbf{Duplication of validation and error handling} -- some validation and error reporting needs to be implemented both in the API (for correctness and security) and in the frontend (for user-friendly messages), which slightly increases implementation effort.
  \item \textbf{Monorepo maintenance} -- while a monorepo simplifies code sharing, it also requires consistent tooling and conventions to keep the structure manageable as the project grows.
\end{itemize}

Despite these drawbacks, the benefits of a clear separation between the data layer and the user interface, combined with the practical advantages of a monorepo for this relatively small project, were considered more important. In particular, the need to integrate multiple heterogeneous data sources, enforce non-trivial access rules and support future extensions favours an API-centric architecture. The resulting design provides a flexible and maintainable foundation for the new PBSmon replacement.


\section{Selected technologies}
\label{sec:selected-technologies}

As mentioned earlier, the original solution was implemented as a web application. The new solution will also take the form of a web application, but it will be built on a different technology stack. When selecting the technologies, it was important to take into account current trends in web development, as well as future maintainability and extensibility.

As described in Section \ref{sec:architecture-design}, the new solution should be conceptually divided into two parts: an API and a frontend. To simplify development and maintenance, JavaScript was chosen as the primary programming language, enabling both parts of the system to be implemented in the same language. Specifically, NestJS is used for the API layer and React for the frontend. These technologies are described in more detail in the following subsections.

\subsection{NestJS}

NestJS is a progressive Node.js framework designed for building efficient, reliable and scalable server-side applications. It leverages TypeScript as its primary language, while still supporting plain JavaScript. The framework is heavily inspired by architectural patterns known from enterprise environments, such as the Model–View–Controller (MVC) pattern and layered architectures, and it emphasises concepts like modules, controllers and providers. \cite{nestjs2025}

One of the main advantages of NestJS is its modular architecture. The application is structured into self-contained modules, which encapsulate related functionality. This organisation improves the readability of the codebase and simplifies long-term maintenance and extensibility. NestJS also provides built-in support for dependency injection, which encourages loose coupling between components and facilitates testing. \cite{nestjs2025}

According to articles \cite{nestjs2025-devto, nestjsbackend2025}, NestJS is well suited for backend development and has accumulated more than 60,000 stars on GitHub, placing it among the world’s most popular backend frameworks. It is a modern, flexible and powerful framework that is gaining popularity thanks to its strong TypeScript support, modular architecture and mature ecosystem.

For these reasons, NestJS was chosen as the framework for implementing the API layer of the new solution.

\subsection{React}

React is a JavaScript library for building user interfaces using a declarative, component-based model. It allows composing complex views from smaller reusable components and integrates well with modern JavaScript and TypeScript tooling. \cite{react-docs}

In this project, React is used as a purely client-side rendered frontend. Rendering of the user interface is thus offloaded from the server to the client, which reduces the load on the backend API and simplifies the server-side implementation. The main trade-off of client-side rendering is weaker support for search engine optimisation (SEO) compared to server-side rendering or static site generation. However, SEO is not required in this system, as it targets authenticated users and is not intended for public indexing. For this reason, a purely client-side React application is sufficient and avoids additional complexity. \cite{react-docs}

React was chosen mainly due to its wide adoption, mature ecosystem and good TypeScript support, which make it a suitable choice for the frontend part of the solution \cite{react-docs}.


\subsubsection{Tailwind CSS}

Tailwind CSS is a utility-first CSS framework that provides a set of pre-defined classes for styling HTML elements. It is designed to be used in conjunction with React, allowing for a more efficient and consistent styling approach. Tailwind CSS is a popular choice for modern web development due to its ease of use, flexibility and performance. \cite{tailwindcss2025}

I have chosen Tailwind CSS due to its popularity and ease of use. It is a popular choice for modern web development and is a good fit for the new solution.

\subsection{Nginx}

Nginx ("engine x") is a high-performance HTTP web server and reverse proxy that can also act as a load balancer and content cache. It is designed to handle a large number of concurrent connections with low resource usage, which makes it suitable as an entry point for web applications.  \cite{nginx-docs}

In this project, Nginx is used primarily as a reverse proxy in front of the backend API and as a static file server for the React frontend. Requests to the API are forwarded from Nginx to the NestJS application, while requests for frontend assets are served directly from Nginx. This setup enables centralised configuration of TLS termination, request routing and basic security headers, while the application servers focus on business logic. \cite{nginx-docs}

As mentioned in the section \ref{sec:architecture-design}, the new solution consists of separate modules — the API and the frontend — Nginx is used to provide a single unified interface to the outside world. Both parts of the system are exposed under one domain and port, and the internal structure is hidden from clients, which simplifies deployment and future maintenance. 

\subsection{Docker}
\label{subsec:docker}
%% \todo - Some explanation about docker and history of dockerization, why it is used

Docker is a platform for building, distributing and running applications in lightweight containers. A container packages the application together with its runtime dependencies and configuration, which makes the resulting environment reproducible across different machines. \cite{docker-docs}

In this project, Docker is used to containerise the backend API, the frontend and supporting services (such as Nginx). Each service runs in its own container, while a shared configuration (e.g.\ using \texttt{docker-compose}) defines how the containers are connected and started. This approach simplifies local development, testing and deployment, as the entire system can be started or stopped with a single command and behaves consistently across environments. \cite{docker-docs}


\chapter{Implementation}
\label{chap:implementation}

This chapter describes the implementation of the new PBSmon solution. The main objective was not only to migrate the legacy monolithic system to a modern technology stack, but also to deliver a maintainable and extensible codebase that supports the current scope of MetaCentrum infrastructure. In particular, the implementation covers monitoring data obtained from the PBS scheduling environment and extends the original functionality by integrating OpenStack as an additional data source.

The resulting system is implemented as a separated backend and frontend that communicate via a JSON API. This separation enables clearer responsibility boundaries, simplifies testing, and allows both parts to evolve independently. At the same time, the implementation keeps the core purpose of PBSmon intact: regularly collecting infrastructure metadata, normalizing it into a consistent representation, and presenting it to users in a readable form.


\section{Authorization and Authentication}
\label{sec:authorization-and-authentication}
TBD - using Perun's e-infra AAI, using OIDC token

\section{System Architecture}
\label{sec:system-architecture-new}

This chapter describes the implementation of PBSmon 2.0 and explains how the individual parts of the system were realized in code. The project is implemented as a monorepo and consists of multiple runtime components that together provide data collection, a JSON API, and a web-based user interface. The repository is structured into the following main components:


\begin{itemize}
  \item \texttt{api/} -- backend service exposing a JSON API consumed by the frontend. It aggregates and normalizes monitoring data from the supported infrastructure sources (PBS, Perun and OpenStack).
  \item \texttt{web/} -- frontend web application responsible for rendering page views and visualizing the collected monitoring data.
  \item \texttt{pbs-collector/} -- a dedicated micro-service for periodic data acquisition from PBS environment. Described in more detail in section \ref{sec:pbs-data-collection}.
  \item \texttt{nginx/} -- reverse proxy configuration used as the entry point to route requests to the web application and the API.
  \item \texttt{docker-compose.prod.yml} -- production deployment definition describing services, networking, and environment configuration.
  \item \texttt{deploy.sh} -- deployment helper script that automates the deployment process.
\end{itemize}

At runtime, user requests are handled through the reverse proxy, which serves the frontend and forwards API calls to the backend. The frontend then communicates with the backend exclusively through the JSON API, while the collector runs independently to keep the monitoring data up to date.


\section{API}
\label{sec:impl-api}
The backend API forms the integration layer between the monitored infrastructure and the web user interface. Its primary purpose is to provide a JSON API for the frontend while hiding the complexity of individual data sources. In PBSmon 2.0, the API aggregates monitoring data from the PBS environment, Perun and OpenStack, normalizes them into a consistent representation, and exposes them through JSON endpoints.


\newpage
\subsubsection{API code structure}
The API source code is organized into the following top-level directories:
\begin{itemize}
  \item \texttt{src/} -- source code of the API.
  \begin{itemize}
    \item \texttt{common/} -- shared utilities and cross-cutting concerns.
    \item \texttt{config/} -- application and environment configuration. Eq. QSUB assembler configuration, pbs server list, etc.
    \item \texttt{modules/} -- feature-oriented modules. Each module represent a domain entity and its related data. Each module consists of controllers and services. And therefore each module have its own routes and endpoints. There are list of modules:
    \begin{itemize}
      \item \texttt{accounting/} -- module for collecting accounting data from the accounting database.
      \item \texttt{app/} 
      \item \texttt{data-collection/} -- module responsible for collecting data from the infrastructure sources.
      \item \texttt{groups/} -- module for retrieving user groups from the PBS server.
      \item \texttt{infrastructure/} -- module for the infrastructure overview.
      \item \texttt{jobs/} -- module for the jobs overview.
      \item \texttt{projects/} -- module for the OpenStack projects overview.
      \item \texttt{qsub/} -- module for the QSUB assembler.
      \item \texttt{queues/} -- module for the queues overview.
      \item \texttt{status/} -- module for the status overview.
      \item \texttt{storage-spaces/} -- module for the storage spaces overview.
      \item \texttt{users/} -- module for the users overview.
    \end{itemize}
  \end{itemize}
  \item \texttt{data/} -- runtime collected data from the infrastructure sources.
\end{itemize}


\subsection{PBS data collection}
\label{sec:pbs-data-collection}
During the implementation, an important architectural decision concerned the placement of the PBS data collection logic: whether it should be part of the main API service or separated into an independent microservice (\texttt{pbs-collector}). The final design uses a dedicated collector because the PBS integration introduces specific and operationally demanding requirements (installation of the \texttt{libopenpbs} IFL library, a Debian-based runtime, and a properly configured Kerberos environment). In addition, prior experience with the legacy solution showed stability issues caused by memory leaks in the OpenPBS library, which could lead to process instability and, in the worst case, affect the entire server. By isolating the collector in a Docker container with explicitly defined resource limits, such failures are contained: once the limits are exceeded or the process becomes unstable, the container can be automatically restarted without impacting the API and the user interface.

The \texttt{pbs-collector} microservice is therefore responsible for periodic retrieval of PBS data and exporting it into filesystem for later processing by the API. It collects core PBS entities (e.g., jobs, nodes, queues, and related objects) using the IFL interface of \texttt{libopenpbs} (as described in Section~\ref{subsubsec:batch-interface-library}), and it also gathers fairshare values for the users via the \texttt{list\_cache} command (as described in Section~\ref{subsubsec:fairshare-metrics}). This separation keeps the API focused on data delivery and presentation concerns, while the collector encapsulates the low-level PBS-specific integration and its operational constraints.


\subsection{Perun data collection}
\label{sec:perun-data-collection}
Perun is not collected by PBSmon in the same way as PBS or OpenStack, because the required data are provided to PBSmon via a push mechanism external to the application. For this reason, PBSmon does not implement an active Perun client; instead, it only consumes prepared Perun exports.

Within the API repository, Perun-related inputs are stored under \texttt{/api/data/perun/}. This directory contains static JSON files:

\begin{itemize}
  \item \texttt{pbsmon\_users.json} -- users data from Perun.
  \item \texttt{pbsmon\_machines.json} -- machines data from Perun.
\end{itemize}

These files were described in more detail in section~\ref{subsec:entities-in-perun}. During development, these files were mocked to enable frontend and API development without relying on the external Perun export pipeline. In production, the application follows the same approach as the legacy PBSmon and loads these JSON files directly from the filesystem at runtime.


\subsection{OpenStack data collection}
TBD - using prometheus program - that was describe in some non existing section - these data stores only into memory and are not collected to the filesystem. 


\subsection{Documentation}
\label{sec:api-documentation}
The API contract is described using an OpenAPI specification, which defines the available endpoints, their parameters, and response schemas. In addition to the static specification, the implementation exposes a Swagger UI that renders the OpenAPI document and provides an interactive interface for exploring and testing the API. This allows developers and administrators to call individual endpoints directly from the browser, inspect responses, and verify the expected behaviour without requiring external tools. The OpenAPI specification is included within the thesis archive.


\section{Frontend - page views}
\label{sec:page-views}
This section describes the implemented frontend page views and how they present monitoring information to end users. Each view corresponds to a specific domain area of the system (e.g., jobs, nodes, queues, outages, news, or OpenStack-related data) and is implemented as a composition of reusable UI components such as tables, filters, and detail panels. The pages obtain all data exclusively through the backend JSON API, which keeps the frontend independent of the underlying infrastructure protocols and allows the UI to evolve without changing data-collection logic.

% For every page view, the following aspects are documented: its purpose and displayed data, the API endpoints it consumes, and the main interaction patterns provided to the user (sorting, filtering, paging, and error/loading states). This makes the connection between the backend modules and the user-facing interface explicit and shows how the collected data are transformed into actionable monitoring views.

\paragraph{Layout}
TBD - describe layout and navigation, the lists of page views 


\subsection{Personal view}
\label{subsec:new-personal-view}
TBD
\begin{itemize}
  \item purpose 
  \item displayed data + user interactions 
  \item API endpoints
  \item covered use cases
  \item Image of the page
\end{itemize}


\subsection{QSUB assembler}
\label{subsec:new-qsub-assembler}
TBD
\begin{itemize}
  \item purpose 
  \item displayed data + user interactions 
  \item API endpoints
  \item covered use cases
  \item Image of the page
\end{itemize}

\subsection{Machines}
\label{subsec:machines}

TBD
\begin{itemize}
  \item purpose 
  \item displayed data + user interactions 
  \item API endpoints
  \item covered use cases
  \item Image of the page
\end{itemize}

\subsubsection{Node detail view}
TBD
\begin{itemize}
  \item purpose 
  \item displayed data + user interactions 
  \item API endpoints
  \item covered use cases
  \item Image??
\end{itemize}

\subsection{Storage spaces}
\label{subsec:storage-spaces}
TBD
\begin{itemize}
  \item purpose 
  \item displayed data + user interactions 
  \item API endpoints
  \item covered use cases
  \item Image of the page
\end{itemize}

\subsection{Cloud Projects}
\label{subsec:projects}
TBD
\begin{itemize}
  \item purpose 
  \item displayed data + user interactions 
  \item API endpoints
  \item covered use cases
  \item Image of the page
\end{itemize}

\subsubsection{Project detail view}
TBD
\begin{itemize}
  \item purpose 
  \item displayed data + user interactions 
  \item API endpoints
  \item covered use cases
  \item Image??
\end{itemize}


\subsection{Jobs Queues}
\label{subsec:queues}
TBD
\begin{itemize}
  \item purpose 
  \item displayed data + user interactions 
  \item API endpoints
  \item covered use cases
  \item Image of the page
\end{itemize}
\subsubsection{Queue detail view}
TBD
\begin{itemize}
  \item purpose 
  \item displayed data + user interactions 
  \item API endpoints
  \item covered use cases
  \item Image??
\end{itemize}

\subsection{Jobs}
\label{subsec:jobs}
TBD
\begin{itemize}
  \item purpose 
  \item displayed data + user interactions 
  \item API endpoints
  \item covered use cases
  \item Image of the page
\end{itemize}

\subsubsection{Job detail view}
TBD
\begin{itemize}
  \item purpose 
  \item displayed data + user interactions 
  \item API endpoints
  \item covered use cases
  \item Image??
\end{itemize}

\subsection{Users}
\label{subsec:users}

TBD
\begin{itemize}
  \item purpose 
  \item displayed data + user interactions 
  \item API endpoints
  \item covered use cases
  \item Image??
\end{itemize}

\subsubsection{User detail view}
TBD
\begin{itemize}
  \item purpose 
  \item displayed data + user interactions 
  \item API endpoints
  \item covered use cases
  \item Image??
\end{itemize}

\subsection{Users groups}
\label{subsec:groups}
TBD
\begin{itemize}
  \item purpose 
  \item displayed data + user interactions 
  \item API endpoints
  \item covered use cases
\end{itemize}

\subsubsection{Group detail view}
TBD
\begin{itemize}
  \item purpose 
  \item displayed data + user interactions 
  \item API endpoints
  \item covered use cases
\end{itemize}


\section{Comparison with the legacy solution}
\label{sec:comparison-with-the-legacy-solution}
TBD - compare the new solution with the legacy solution in terms of features, performance, scalability, maintainability, etc. 

IF is not too long - just move to conclusion


\chapter{Future improvements}
TBD -
\begin{itemize}
  \item Kubernetes extensions - showing state of kubernetes infrastructure 
  \item Export to CSV - for analytical purposes - for admin users 
  \item Showing personal disk quotas within user 
  \item Option to submit jobs using the API - for users
\end{itemize}




\chapter*{Conclusion}
\addcontentsline{toc}{chapter}{Conclusion}
TBD - simply state conclusion


\appendix
\chapter{OpenStack Response Example}
\label{app:openstack-response-example}

This appendix contains an example of the JSON response structure for the OpenStack cluster named "glados" that is collected by the PBSmon system.

\begin{lstlisting}[caption={Example response for OpenStack cluster "glados"},
  label={lst:openstack-glados-response}]
[
    {
        "CPUs": "40",
        "Hypervisor": "ics-gladosag-007-ostack.priv.cloud.muni.cz",
        "VMs": [
            {
                "CPUs": "38",
                "created": "2020-10-09T09:32:42Z",
                "instance_state": "ACTIVE",
                "name": "RationAI-node-2",
                "user_id": "1633180b677608f61e96784ee5cbc608c0f4b62d@einfra.cesnet.cz"
            }
        ]
    },
    {
        "CPUs": "40",
        "Hypervisor": "ics-gladosag-006-ostack.priv.cloud.muni.cz",
        "VMs": []
    },
    {
        "CPUs": "40",
        "Hypervisor": "ics-gladosag-003-ostack.priv.cloud.muni.cz",
        "VMs": [
            {
                "CPUs": "38",
                "created": "2020-01-17T10:29:22Z",
                "instance_state": "ACTIVE",
                "name": "RationAI-node-1",
                "user_id": "1633180b677608f61e96784ee5cbc608c0f4b62d@einfra.cesnet.cz"
            }
        ]
    },
    {
        "CPUs": "40",
        "Hypervisor": "ics-gladosag-004-ostack.priv.cloud.muni.cz",
        "VMs": []
    },
    {
        "CPUs": "40",
        "Hypervisor": "ics-gladosag-001-ostack.priv.cloud.muni.cz",
        "VMs": [
            {
                "CPUs": "16",
                "created": "2022-08-17T13:07:15Z",
                "instance_state": "ACTIVE",
                "name": "front-82e95edc-1e2d-11ed-9687-0ee20d64cb6e",
                "user_id": "9cd61d48508661633e261f711634b749fdc5d9fcc20769e55baac46ef01c5a63@egi.eu"
            }
        ]
    }
]
\end{lstlisting}

\end{document}
