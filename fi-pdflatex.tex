%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% I, the copyright holder of this work, release this work into the
%% public domain. This applies worldwide. In some countries this may
%% not be legally possible; if so: I grant anyone the right to use
%% this work for any purpose, without any conditions, unless such
%% conditions are required by law.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[
  digital,     %% The `digital` option enables the default options for the
               %% digital version of a document. Replace with `printed`
               %% to enable the default options for the printed version
               %% of a document.
%%  color,       %% Uncomment these lines (by removing the %% at the
%%               %% beginning) to use color in the printed version of your
%%               %% document
  oneside,     %% The `oneside` option enables one-sided typesetting,
               %% which is preferred if you are only going to submit a
               %% digital version of your thesis. Replace with `twoside`
               %% for double-sided typesetting if you are planning to
               %% also print your thesis. For double-sided typesetting,
               %% use at least 120 g/m² paper to prevent show-through.
  nosansbold,  %% The `nosansbold` option prevents the use of the
               %% sans-serif type face for bold text. Replace with
               %% `sansbold` to use sans-serif type face for bold text.
  nocolorbold, %% The `nocolorbold` option disables the usage of the
               %% blue color for bold text, instead using black. Replace
               %% with `colorbold` to use blue for bold text.
  lof,         %% The `lof` option prints the List of Figures. Replace
               %% with `nolof` to hide the List of Figures.
  lot,         %% The `lot` option prints the List of Tables. Replace
               %% with `nolot` to hide the List of Tables.
]{fithesis4}
%% The following section sets up the locales used in the thesis.
\usepackage[resetfonts]{cmap} %% We need to load the T2A font encoding
\usepackage[T1,T2A]{fontenc}  %% to use the Cyrillic fonts with Russian texts.
\usepackage[
  main=english, %% By using `czech` or `slovak` as the main locale
                %% instead of `english`, you can typeset the thesis
                %% in either Czech or Slovak, respectively.
  english, german, czech, slovak %% The additional keys allow
]{babel}        %% foreign texts to be typeset as follows:
%%
%%   \begin{otherlanguage}{german}  ... \end{otherlanguage}
%%   \begin{otherlanguage}{czech}   ... \end{otherlanguage}
%%   \begin{otherlanguage}{slovak}  ... \end{otherlanguage}
%%
%%
%% The following section sets up the metadata of the thesis.
\thesissetup{ 
    date        = \the\year/\the\month/\the\day,
    university  = mu,
    faculty     = fi,
    type        = mgr,
    department  = Department of design and development of software systems,
    author      = Bc. Marcel Lukčo,
    gender      = m,
    advisor     = {Mgr. Miroslav Ruda},
    title       = {PBSMon2, web service for displaying MetaCenter status},
    TeXtitle    = {PBSMon2, web service for displaying MetaCenter status},
    keywords    = {cloud, cloud computing, distributed computing, MetaCentrum},
    TeXkeywords = {cloud, cloud computing, distributed computing, MetaCentrum},
    abstract    = {%  
    This thesis describes the development of a new web application intended to replace the existing PBSMon monitoring tool used in the MetaCenter environment. The work begins with an analysis of the original solution, its limitations, and relevant application and technologies relevant to MetaCenter infrastructure. Based on the identified requirements, the thesis proposes a new architecture and justifies key design decisions, including the separation into a multiple cooperating components. All of this followed by the describing of the implementation results of the new solution. Concluding in outlining possible future improvements and extensions.
    },
    thanks      = {%
    First and foremost, I would like to express my sincere gratitude to my supervisor, Mgr. Miroslav Ruda, for his valuable guidance, insightful comments, and continuous support throughout the development of this thesis.

    I would also like to thank Mgr. Václav Chlumský, for his technical assistance and expertise, which significantly contributed to the practical part of this work.
    
    My thanks also go to Ing. František Řezníček, and Mgr. Ivana Křenková for their collaboration, helpful advice, and support during the integration with related systems.
    },
    bib         = example.bib,
    %% Remove the following line to use the JVS 2018 faculty logo.
    facultyLogo = fithesis-fi,
}
\usepackage{makeidx}      %% The `makeidx` package contains
\makeindex                %% helper commands for index typesetting.
%% These additional packages are used within the document:
\usepackage{paralist} %% Compact list environments
\usepackage{amsmath}  %% Mathematics
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{url}      %% Hyperlinks
\usepackage{markdown} %% Lightweight markup
\usepackage{listings} %% Source code highlighting
\lstset{
  basicstyle      = \ttfamily,
  identifierstyle = \color{black},
  keywordstyle    = \color{blue},
  keywordstyle    = {[2]\color{cyan}},
  keywordstyle    = {[3]\color{olive}},
  stringstyle     = \color{teal},
  commentstyle    = \itshape\color{magenta},
  breaklines      = true,
}
\usepackage{floatrow} %% Putting captions above tables
\floatsetup[table]{capposition=top}
\usepackage[babel]{csquotes} %% Context-sensitive quotation marks
\begin{document}
%% The \chapter* command can be used to produce unnumbered chapters:
\chapter*{Introduction}
%% Unlike \chapter, \chapter* does not update the headings and does not
%% enter the chapter to the table of contents. I we want correct
%% headings and a table of contents entry, we must add them manually:
\markright{\textsc{Introduction}}
\addcontentsline{toc}{chapter}{Introduction}
Currently, high-performance computing plays a key role in the implementation of research projects across a wide range of scientific disciplines. In the academic sector, it is therefore essential to ensure access to sufficient computing power to enable the implementation of demanding simulations, the processing of large data sets, and the effective analysis of results. National computing infrastructures, including CESNET, have been created for this purpose. This academic institution offers computing resources dedicated exclusively to research and academic purposes.

The PBSMon web service was developed to monitor these resources and provide an overview of their status. This application ensures the regular collection of metadata from computing nodes, queues, servers, and running jobs using native calls to PBS servers. It then transforms this data into a visual form accessible to users. Thanks to PBSMon, users can monitor the current system load, the status of individual jobs, and the availability of computing resources in real time.

However, the original PBSMon application is monolithic and uses outdated libraries, such as Java library Stripes, which is no longer maintained. Growing demands for security, sustainability, and support for containerized deployment, and the need to cover both PBS and OpenStack require a fundamental refactoring and extension of the system.

This thesis deals with the design and implementation of a new version of the PBSMon system, built on modern technologies: backend within the NestJS framework and frontend within the React framework. 

The goal is to create a modular, maintainable, and cloud-native architecture that not only preserves the functionality of the original solution, but also extends it to support the OpenStack cloud and to be ready for further development and adaptation to current technological standards, including new features required by the assignment.


\chapter{Analysis of the Existing Solution – PBSMon}
\label{chap:analysis}
The PBSMon application serves as a central tool for collecting and visualizing data from various systems related to computing resource management. It automatically collects information from the PBS (Portable Batch System), the Perun identity management system, and virtual machines (OpenStack). It then processes this data and provides users with a comprehensive view of the current status of the infrastructure, jobs, and user activities within the entire MetaCenter infrastructure.

The web interface is used to visualize data from the PBS, Perun, and OpenStack systems. The information displayed includes a personal view of jobs, available computing resources, an overview of computing machines and their utilization, a list of users, and other statistics. Among other things, the following data is collected:

\begin{itemize}
  \item PBS nodes (status, load, available resources),
  \item information about virtual machines
  \item physical machines (availability, utilization, technical parameters),
  \item users (identity, activity, jobs),
  \item queues (status, priority, configuration),
  \item jobs (status, start time, resources used, user).
\end{itemize}


\section{System Architecture}
\label{sec:system-architecture}
The existing solution is built as a monolithic Java application that collects data from various systems (PBS, Perun, OpenStack) and displays it in a web interface. The architecture is tightly coupled and uses multiple technologies, including C libraries and shell scripts.


\newpage
\subsection{Data collection environments}
\label{subsec:data-collection-environments}
PBSMon collects following data from the following environments:
\begin{itemize}
  \item PBS environment
  \begin{itemize}
    \item Compute clusters managed by PBS systems
    \item Collection of Fairshare metrics from pbscache, which determine the user's priority when launching a job
    \item Collection of the /etc/group file from PBS, essential for assigning users to groups used in access control lists (ACLs)
  \end{itemize}
  \item Perun environment - an identity and group management system. Returns information about users and the list of physical machines.
  \item OpenStack environment - Information about virtual machines running on physical hosts
\end{itemize}

Each of these environments has its own method of communication and data representation. Within the system architecture, these differences are abstracted through interfaces and adapters that ensure unified data processing. All of these enviroments and their data structure are described in the following sections.

\subsection{Data Processing and Unification}
\label{subsec:data-processing-and-unification}

The data obtained from various sources — such as compute clusters (PBS), cloud platforms (e.g., OpenStack), and the identity management system (Perun) — differ in their formats and representations. After collection, the data are first stored in an in-memory cache and subsequently mapped and unified to establish logical relationships between entities from different systems. Among other things, the following relationships are established:

\begin{itemize}
  \item Hierarchical structure: user(Perun) → jobs (PBS) → queues (PBS)
  \item Mapping: virtual machines (OpenStack) and physical machines (Perun)
\end{itemize}

\subsection{Web Presentation}
\label{subsec:web-presentation}

The presentation of data is performed through server-side rendering of HTML pages. Only logged in users, that have approved access to PBSMon can access the web interface. The authorization is performed by third party system using OICD token, that will be described in detail in section \ref{sec:authentication}.

These pages display detailed views of jobs, nodes, system states, and other information relevant to both users and administrators. As mentioned earlier, the web interface is strictly read-only and does not provide any functionality for data input or modification.
The frontend is not implemented as a standalone application; it is an integral part of the monolithic Java system.

One of the key features is so called \textbf{QSUB assembler}, which allows user to add required parameters for the job submission and PBSMon will notify the user about the nodes that fullfils the requirements. Among other, it produces a shell script that can be executed to submit the job to the PBS server with the required parameters

All of this will be described in more detail in section \ref{sec:web-layer-and-user-interface}.

\newpage
\section{Data Collection from PBS Environment}
\label{sec:data-collection-from-pbs-environment}
The Portable Batch System (PBS) is a distributed workload management system designed to schedule and monitor computational jobs across multiple compute nodes in a cluster environment. \cite{pbs2022} 

In a typical configuration, the PBS environment consists of a central management server and a set of compute nodes, which are individual physical or virtual hosts providing computational resources such as CPUs, memory, GPUs, and local storage. \cite{pbs2022} 

A compute node (also referred to as a host) represents the fundamental execution unit in the cluster — it is the machine where user jobs are actually executed. Each node communicates with the central PBS server, which manages job submission, scheduling, and resource allocation. \cite{pbs2022} 

\subsection{Entities in PBS Environment}
\label{subsec:entities-in-pbs-environment}

PBS provides structured data that enables continuous monitoring of the computing environment operation, scheduler behavior, and resource utilization. From the PBS perspective, the following main entities are collected: \cite{pbs2022}


\subsubsection{Server}
\label{subsubsec:server-pbs-server}

The PBS server is the authority for the entire cluster: it receives and registers jobs, maintains queues and nodes, tracks their states, and publishes global statistics (e.g., counts of jobs by state). It provides the following data: \cite{pbs2022}

\begin{itemize}
  \item \textbf{Policies and limits} -- defines and enforces scheduling and resource utilization rules (limits on job and CPU counts, job array rules, rescheduling on node failure, scheduling enable/disable).
  
  \item \textbf{Resources and default settings} -- manages available/assigned resources and defaults (e.g., resources\_available, resources\_default, default\_chunk, default\_queue) and provides summary utilization (assigned memory/CPU/nodes).
  
  \item \textbf{Integration with scheduler and reservations} -- provides parameters for the scheduler (iteration, backfill, sorting/fairshare formula) and supports time-based resource reservations (advance/standing/maintenance).
  
  \item \textbf{Security and access} -- manages access policies through ACLs (hosts, users, managers/operators), supports Kerberos/realm policies, and handles credential management/renewal.
  
  \item \textbf{Operations and management} -- configures logging, email notifications, license quotas/counters, and system version; allows management of hooks and other server-level objects.\cite{pbs2022}
\end{itemize}

\subsubsection{Jobs}
\label{subsubsec:jobs}

A job is the basic computational unit submitted by a user to the PBS server---it contains command(s) to execute and resource requirements (CPU, memory, time, GPU). The server queues it, schedules it to nodes, monitors its progress, and upon completion evaluates its result (including outputs and return code).\cite{pbs2022} 

A job can be a standalone task or a member of a job array (with multiple subjobs) sharing the same resource template. PBS provides the following data about jobs:\cite{pbs2022}

\begin{itemize}
  \item \textbf{Identity and ownership} -- job ID and name, owner (Job\_Owner), project/VO (project), target queue (queue), server, and submit host.
  
  \item \textbf{Lifecycle and scheduling} -- state and substate (job\_state, substate), priority, run count, holds (Hold\_Types), rerunnability (Rerunable), credentials and validity (credential\_id, credential\_validity).
  
  \item \textbf{Requested resources and placement} -- Resource\_List.* (e.g., select, ncpus, mem, walltime, place, scratch\_*, mpiprocs, ompthreads, nodect) and runtime identifiers (session\_id).
  
  \item \textbf{Timing metrics} -- ctime, qtime, stime, mtime, obittime, etime + derived indicators (e.g., eligible\_time).
  
  \item \textbf{I/O and environment} -- working directory (jobdir), stdout/stderr paths (Output\_Path, Error\_Path), submission arguments (Submit\_arguments), and environment variables (Variable\_List).
  
  \item \textbf{Result and diagnostics} -- return code (Exit\_status) and auxiliary fields for auditing and progress tracking.\cite{pbs2022}
\end{itemize}

\subsubsection{Queues}
\label{subsubsec:queues}

A queue is a logical structure for accepting and processing jobs---it defines its type (Execution vs. Route), resource defaults and limits, access rules, and how jobs are either executed on nodes or redirected to target queues. PBS provides the following data about queues: \cite{pbs2022}

\begin{itemize}
  \item \textbf{Identity and type} -- queue name, queue\_type (Execution/Route), Priority (weight in scheduling), operational state (enabled/started), and optionally hasnodes.
  
  \item \textbf{State and utilization} -- 
  aggregates such as total\_jobs and state\_count, and currently
  assigned resources resources\_assigned.* (e.g., memory, CPUs, nodes, MPI processes).
  
  \item \textbf{Policies and limits} -- resource boundaries resources\_max.* and minimums resources\_min.* (including GPU and walltime), extra rules like kill\_delay, backfill\_depth, or from\_route\_only (accepts jobs only via routing).
  
  \item \textbf{Defaults and placement} --
  resources\_default.
  (e.g., CPUs, walltime, placement, GPUs) 
  and default\_chunk.* 
  (e.g., implicit chunk size, queue\_list for targeting).
  
  \item \textbf{Routing (Route queues)} -- route\_destinations defines target execution queues to which jobs are automatically redirected.
  
  \item \textbf{Access and security} -- 
  ACL toggles and lists: 
  acl\_user\_enable/acl\_users, 
  acl\_group\_enable/acl\_groups,
   or acl\_host\_enable/acl\_hosts.
  
  \item \textbf{Organizational tags} -- optional attributes such as fairshare\_tree (fairshare hierarchy) or partition (infrastructure label/partition) for logical segmentation and policy purposes.
  \cite{pbs2022}
\end{itemize}

\subsubsection{Nodes}
\label{subsubsec:nodes}

A node represents a physical machine in the PBS environment that provides computational resources for job execution. Each node is managed by the PBS server and can be in various operational states depending on its availability and current workload.  PBS provides the following data about nodes: \cite{pbs2022}

\begin{itemize}
  \item \textbf{Identity and location} -- node name (vnode), hostname (host), cluster identifier (resources\_available.cluster), and the name of the execution daemon (Mom) that manages the node. 
  
  \item \textbf{State and availability} -- primary state (state) indicating the operational status (e.g., free, job-busy, down, offline) and auxiliary state (state\_aux) providing additional context. 
  
  \item \textbf{Available resources} -- total capacity of the node defined by \texttt{resources\_available.*} attributes, including:
  \begin{itemize}
    \item CPU resources: ncpus (number of CPUs), pcpus (physical CPUs), cpu\_vendor, cpu\_flag (CPU feature flags)
    \item Memory: mem (total memory), vmem (virtual memory), hpmem (high-performance memory)
    \item Accelerators: ngpus (number of GPUs), gpu\_mem, gpu\_cap (compute capability), cuda\_version
    \item Scratch storage: scratch\_local, scratch\_shared, scratch\_ssd, scratch\_shm (shared memory)
    \item Network: ethernet\_speed, infiniband
    \item Software: singularity (container support), os, osfamily, arch
    \item Organizational: queue\_list (queues accessible from this node), cluster-specific tags
  \end{itemize}
  
  \item \textbf{Assigned resources} -- currently allocated resources tracked by \texttt{resources\_assigned.*} attributes, including:
  \begin{itemize}
    \item ncpus, ngpus, naccelerators (number of assigned CPUs, GPUs, accelerators)
    \item mem, vmem, hbmem, accelerator\_memory (assigned memory)
    \item scratch\_local, scratch\_ssd (assigned storage)
  \end{itemize}
  
  \item \textbf{Active jobs} -- list of job identifiers (jobs) currently running on the node, with each job identified by its ID and task index (e.g., 14964063.pbs-m1.metacentrum.cz/0). 
  
  \item \textbf{Sharing and placement} -- sharing mode (\texttt{sharing}) that determines resource allocation (e.g., \texttt{default\_shared}, \texttt{force\_exclusive}), and reservation support (\texttt{resv\_enable}). 
  
  \item \textbf{Timestamps} -- last\_state\_change\_time (timestamp of the last state transition) and last\_used\_time (timestamp when the node was last utilized). \cite{pbs2022}
\end{itemize}

The distinction between available and assigned resources enables monitoring of node utilization, while the state information provides insight into node availability for job scheduling. The jobs list allows tracking which specific jobs are consuming resources on each node, which is essential for resource accounting and troubleshooting. 

\subsubsection{Reservations}
\label{subsubsec:reservations}

Reservations are a mechanism in PBS that allows nodes to be reserved for exclusive use during a specific time period. When a reservation is created, the specified nodes become unavailable for regular job scheduling. For each reservation, PBS automatically creates a dedicated queue that provides access to the reserved resources. \cite{pbs2022}

PBS provides the following data about reservations:

\begin{itemize}
  \item \textbf{Identity} -- reservation name (Reserve\_Name), unique identifier (name), and the associated queue name (queue) that is created for the reservation.
  
  \item \textbf{Ownership and access} -- reservation owner (Reserve\_Owner) and list of authorized users (Authorized\_Users).
  
  \item \textbf{State information} -- reservation state (reserve\_state) and substate (reserve\_substate) indicating the current status of the reservation (e.g., active, confirmed, or in transition).
  
  \item \textbf{Time constraints} -- reservation start time (reserve\_start), end time (reserve\_end), and duration (reserve\_duration) specified as Unix timestamps.
  
  \item \textbf{Resource requirements} -- the resources reserved by the reservation, including:
  \begin{itemize}
    \item Memory: Resource\_List.mem (total memory reserved)
    \item CPUs: Resource\_List.ncpus (number of CPUs)
    \item GPUs: Resource\_List.ngpus (number of GPUs, if applicable)
    \item Nodes: Resource\_List.nodect (number of nodes) and Resource\_List.select (detailed node selection specification)
    \item Placement: Resource\_List.place (placement policy, e.g., \texttt{free}, \texttt{exclhost})
    \item Walltime: Resource\_List.walltime (maximum execution time for jobs in the reservation)
  \end{itemize}
  
  \item \textbf{Reserved nodes} -- list of actual nodes allocated to the reservation (resv\_nodes) with their specific resource allocations.
  
  \item \textbf{Metadata} -- creation time (ctime), modification time (mtime), submission host (Submit\_Host), and partition information (partition, if applicable).
  
  \item \textbf{Retry information} -- reservation count (reserve\_count) and retry attempts (reserve\_retry) for tracking reservation lifecycle. \cite{pbs2022}
\end{itemize}


\subsubsection{Resources}
\label{subsubsec:resources}

Resources in PBS represent units of computational capacity that can be requested by jobs, allocated to nodes, and managed by queues and the server. Each resource is defined with a unique name and attributes that specify its data type and where it can be used within the PBS system. \cite{pbs2022} 

PBS provides a comprehensive list of all available resources in the system, where each resource definition includes: \cite{pbs2022}

\begin{itemize}
  \item \textbf{Resource name} -- the identifier used to reference the resource in job submissions, node configurations, and queue settings (e.g., \texttt{cput}, \texttt{mem}, \texttt{ncpus}, \texttt{ngpus}, \texttt{walltime}).
  
  \item \textbf{Type} -- the data type of the resource value, which determines how the resource is interpreted and validated. Common types include:
  \begin{itemize}
    \item \texttt{long} -- integer values (e.g., CPU count, GPU count)
    \item \texttt{size} -- memory or storage values with units (e.g., bytes, KB, MB, GB)
    \item \texttt{string} -- text values
    \item \texttt{float} -- floating-point numeric values
    \item \texttt{time} -- time duration values
  \end{itemize}
  
  \item \textbf{Flag} -- a string of characters indicating where the resource can be used or referenced:
  \begin{itemize}
    \item \texttt{h} -- can be used at the host/node level
    \item \texttt{q} -- can be used at the queue level
    \item \texttt{n} -- can be used at the node level
    \item \texttt{m} -- can be used at the server level 
  \end{itemize}\cite{pbs2022} 
\end{itemize}

\subsubsection{Scheduler status}
\label{subsubsec:scheduler-status}

The PBS scheduler is responsible for making decisions about which jobs to run, when to run them, and on which nodes to execute them.  A PBS system can have multiple schedulers, each managing a specific partition or set of resources. The scheduler continuously evaluates queued jobs against available resources and applies scheduling policies to optimize resource utilization and meet job requirements. \cite{pbs2022}

PBS provides status information about each scheduler instance, including: \cite{pbs2022}

\begin{itemize}
  \item \textbf{Identity and location} -- scheduler name, the host where the scheduler daemon runs (sched\_host), and the partition it manages (partition).
  
  \item \textbf{Operational state} -- current state of the scheduler (e.g., \texttt{scheduling}, \texttt{idle}) and whether scheduling is enabled (scheduling).
  
  \item \textbf{Scheduling cycle} -- scheduler cycle length (sched\_cycle\_length) defining how frequently the scheduler evaluates and schedules jobs, and the current iteration count (scheduler\_iteration).
  
  \item \textbf{Processor set configuration} -- settings for processor set (pset) handling: do\_not\_span\_psets (prevents jobs from spanning multiple processor sets) and only\_explicit\_psets (restricts scheduling to explicitly defined processor sets).
  
  \item \textbf{Scheduling mode} -- throughput\_mode indicates whether the scheduler prioritizes throughput optimization, and opt\_backfill\_fuzzy (if present) specifies the backfill optimization level.
  
  \item \textbf{Preemption settings} -- configuration for job preemption: preempt\_queue\_prio (priority threshold for preemption), preempt\_prio (queues or job types that can be preempted), preempt\_order (preemption order strategy), and preempt\_sort (sorting method for preemption selection, e.g., \texttt{min\_time\_since\_start}).
  
  \item \textbf{Integration and hooks} -- job\_run\_wait specifies the hook that controls when jobs can start execution (e.g., \texttt{runjob\_hook}).
  
  \item \textbf{File system paths} -- sched\_priv (path to scheduler private directory) and sched\_log (path to scheduler log files).
  
  \item \textbf{Logging and monitoring} -- log\_events (bitmask specifying which events to log) and server\_dyn\_res\_alarm (alarm threshold for dynamic resource changes). \cite{pbs2022}
\end{itemize}

\subsubsection{Fairshare metrics}
\label{subsubsec:fairshare-metrics}

A list of users, their fairshare values, and the timestamp when the record was last modified. 
fairshare values are used per scheduler.\cite{pbs2022}
This data is retrieved for both the QSUB assembler and user monitoring purposes.

\newpage
\subsection{Data Collection Mechanism}
\label{subsec:data-collection-mechanism}

\subsubsection{Batch Interface Library (IFL)}
\label{subsubsec:batch-interface-library}
PBS provides a C library, which represents the programming interface (API) of the PBS system, also known as the Batch Interface Library (IFL). \cite{pbs2022} 

This library allows external applications and tools to communicate with the PBS server. It provides functions for remote management of batch jobs, querying the system state, and managing computational resources through TCP/IP communication. Using the library, it is possible to implement a client application that: \cite{pbs2022} 

\begin{itemize}
  \item establishes a connection to the server (pbs\_connect),

  \item authenticates the user,
  
  \item creates and submits jobs (pbs\_submit),

  \item queries their status (pbs\_statjob, pbs\_selstat),
   
  \item modifies or deletes jobs (pbs\_alterjob, pbs\_deljob),
  
  \item works with information about the server, queues, nodes, or scheduler.\cite{pbs2022}
\end{itemize}   

Thus, library represents a key component for implementing a tool that enables data collection and monitoring of jobs managed by the PBS server.


Subsequently, current PBSMon implementation contains C code that, when invoked, retrieves information from the PBS server using the library functions and then stores this data into a file, which the Java application later processes and saves into the in-memory store (see Listing \ref{lst:pbs_connect_example}).

This collection process is triggered whenever the user wants to display any PBSMon page and when the data in the memory cache is older than 60 seconds.

Another important note is that collected data represents the current state of the PBS server. It does not include any historical data. \label{note:historical-data}

\newpage
\begin{lstlisting}[caption={Partial code snippet for data collection from PBS server},
  language=c, label={lst:pbs_connect_example}]
  #include <pbs_error.h>
  #include <pbs_ifl.h>

  int main(int argc, char **argv) {
      // ...
      con = pbs_connect(server);    
      if(con<0) {
          return 1;
      }
      /* get server info */
      bs = pbs_statserver(con, NULL, NULL);
      process_data(bs,"servers");
      /* get queues info */
      bs = pbs_statque(con, "", NULL, NULL);
      process_data(bs,"queues");
      /* get nodes info */
      bs = pbs_statnode(con, "", NULL, NULL);
      process_data(bs,"nodes");
      /* get jobs info: t - job arrays, x - finished jobs*/
      bs = pbs_statjob(con, "", NULL, "tx");
      process_data(bs,"jobs");
      /* get reservations info */
      bs = pbs_statresv(con, NULL, NULL, NULL);
      process_data(bs,"reservations");
      /* get resources info */
      bs = pbs_statrsc(con, NULL, NULL, NULL);
      process_data(bs,"resources");
      /* get scheduler info */
      bs = pbs_statsched(con, NULL, NULL);
      process_data(bs,"schedulers");
      /* end connection */
      pbs_disconnect(con);
      return 0;
  }  
\end{lstlisting}

\subsubsection{Fairshare metrics collection}
\label{subsubsec:fairshare-metrics-collection}

Fairshare metrics are collected using the bash command.

\begin{lstlisting}[caption={Metrics collection script},
  language=bash, label={lst:fairshare-metrics-collection}]
  list_cache <pbsServer> fairshare{.elixir}
\end{lstlisting}


This shell command returns a CSV file with columns:

user, last\_modified, fairshare.

\subsubsection{Group file collection}
\label{subsubsec:group-file-collection}

In addition to entities and caches, the shell script located on PBS server automatically pushes its /etc/group file. These files provide information about local UNIX groups and their members. Whenever time it has changed, the PBS server writes the updated version to the PBSMon server's filesystem under the following path: \textbf{/etc/pbsMon/group/<pbsServer>}.


\newpage
\section{Data Collection from Perun}
Perun is an open-source system developed in Java that serves for comprehensive management of identities, groups, attributes, and access to various resources and services. It is a modular solution designed for efficient management of users, organizations, and projects. The system is built with an emphasis on operation in distributed environments and on integration with existing identity systems in the fields of research and education.\cite{perun2025}

\subsection{Entities in Perun}
\label{subsec:entities-in-perun}
Entities retrieved from Perun are two independent data domains — users and machines. Each domain is exported by Perun as a separate JSON file. The detailed description of these entities:

\subsubsection{Users}
\label{subsubsec:users-perun}
The user dataset contains information about all registered users in the MetaCenter infrastructure, including their identifiers, names, organizational affiliations, and assigned virtual organizations. These data are used to enrich job statistics and to provide a link between computational activity and user identity.

\subsubsection{Machines}
\label{subsubsec:machines-perun}
In addition to management of users, Perun also collect information about physical computing resources of the MetaCenter infrastructure.
These data are then passed to PBSMon from Perun. Data are hierachically structured and grouped by Organization -> Cluster -> Computing Node.

Each record is an institution that has at least one cluster. Cluster is a group of computing nodes that are owned by the same institution. Computing node is a physical machine that is part of the cluster.

Each computing node is described by the following metadata:

\begin{itemize}
  \item CPU configuration
  \item Memory
  \item Storage
  \item Owner institution
  \item The list of individual node hostnames
\end{itemize}

These data are neccessary to get additional information about the complete information about the computing node for the running PBS jobs.
Additionally, these data independetly from PBS are important for knowing the complete information about the computing nodes for whole MetaCenter infrastructure.


\subsection{Data Collection Mechanism}
\label{subsec:data-collection-mechanism-perun}

The integration of the PBSMon system with Perun is designed using a PUSH model. Instead of direct access to the Perun database or invoking its API, Perun periodically generates JSON files containing all relevant information about users and computing resources. These files are then transferred via SSH directly to the PBSMon server, where they are stored in the filesystem, and subsequently loaded and processed by PBSMon. These files are stored in the filesystem as following:

\begin{itemize}
  \item \textbf{/etc/pbsmon/pbsmon\_users.json} 
  \item \textbf{/etc/pbsmon/pbsmon\_machines.json}
\end{itemize}

\subsubsection{Change Detection and Synchronization}
Currently, whenever user opens any page in PBSMon, the system checks if the files were modified since last load. If they were, the files are loaded and processed by PBSMon.


\newpage 
\section{Data Collection from Virtual Machines (OpenStack)}
OpenStack is a cloud operating system that controls large pools of compute, storage, and networking resources throughout a datacenter, all managed and provisioned through APIs with common authentication mechanisms. \cite{openstack2025}

Beyond standard infrastructure-as-a-service functionality, additional components provide orchestration, fault management and service management amongst other services to provide operators flexibility to customize their infrastructure and ensure high availability of user applications. \cite{openstack2025}

Within Metacenter infrastructure, OpenStack is used to provide virtual machines to the users with specific needs that are not covered by the PBS environment. There are some computing nodes that are part of MetaCenter infrastructure, but are reserved ad hoc for OpenStack. Within nodes returned by Perun mentioned in \ref{subsubsec:machines-perun} are also nodes that are used for OpenStack.

Current PBSMon implementation gets very limited information about OpenStack virtual machines for each cluster. For each cluster, there is a list of virtual machines with their reserved CPU, name and user ID.

 
\subsection{Data Collection Mechanism}
\label{subsec:data-collection-mechanism-openstack}

The integration of the PBSMon system with OpenStack is designed using a PUSH model. Instead of direct access to the API, JSON files are transferred via SSH directly to the PBSMon server, where they are stored in the filesystem, and subsequently loaded and processed by PBSMon. For each cluster, there is a separate file with the list of virtual machines with their reserved CPU, name and user ID. 

An example of the response structure for the OpenStack cluster named "glados" is provided in Appendix \ref{app:openstack-response-example}.


\newpage
\section{Retrieval of historical data}
\label{sec:collection-of-historical-data}

As mentioned in section \ref{note:historical-data}, the data collected by PBSMon represents only the current state of the PBS infrastructure. To access historical information about completed jobs, their resource usage, and long-term statistics, a separate application is responsible for collecting and storing historical PBS data in a PostgreSQL database.

This historical data collection system continuously records information about finished jobs, including their execution times, resource consumption, and user associations.

\section{Authentication}
\label{sec:authentication}
The current PBSMon solution uses third-party authentication provided by Perun's e-INFRA system. Authentication is implemented using the OpenID Connect (OIDC) protocol, which allows users to authenticate through the centralized identity provider managed by Perun.  \cite{perun2025}

This approach ensures that only authorized users with valid credentials from the Perun identity management system can access the PBSMon web interface.  \cite{perun2025}

The authentication flow follows the standard OIDC protocol, where users are redirected to the Proxy IdP for login, and upon successful authentication, an OIDC token is issued and used to grant access to the PBSMon application. \cite{perun2025}

\section{Web Layer and User Interface}
\label{sec:web-layer-and-user-interface}
The current PBSMon application is integrated as part of the MetaVO portal (\url{https://metavo.metacentrum.cz}), which serves as the main web interface for MetaCenter services. Within MetaVO, PBSMon is encapsulated as a single subsection called "Current state" (see \ref{fig:pbsmon}). This section is available only to logged-in users who have been granted access to PBSMon through the authentication system described in section \ref{sec:authentication}.

The "Current state" subsection consists of the following pages:

\begin{itemize}
  \item \textbf{Personal view} - A dashboard with information about the logged-in person, including their job statistics, jobs queues and quick access to other relevant pages
  \item \textbf{QSUB assembler} - A tool for assembling commands for job submission, described in detail in subsection \ref{subsec:qsub-assembler}
  \item \textbf{Physical machines} - A list of Metacenter grid infrastructure retrieven from Perun with PBS mapping
  \item \textbf{PBS node state} - A list of all PBS nodes and their current status. Without mapping to the Perun physical machines.
  \item \textbf{Virtual machines} - A list of all virtual machines (no longer relevant in the current infrastructure)
  \item \textbf{Jobs queues} - A list of queues and reservations
  \item \textbf{Jobs} - A page with total statistics and navigation hub to "My jobs", "All jobs", and "Suspicious jobs"
  \item \textbf{User} - A list of users from Perun and their current CPU usage
  \item \textbf{Machine properties} - A list of all PBS nodes and their properties (e.g., architecture, OS family, cluster, cgroups, etc.). Page allows to click on the property to see all the nodes that have this property.
  \item \textbf{List of hardware} - A list of organizations and their clusters
  \item \textbf{Cloud} - Cloud-related information (no longer relevant)
\end{itemize}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{figures/pbsmon.png}
  \caption{PBSMon layout}
  \label{fig:pbsmon}
\end{figure}

\newpage
\subsection{Personal view}
\label{subsec:personal-view}
The Personal view serves as the main dashboard for users, representing one of the most important and frequently used features of PBSMon. This view provides users with an overview of their job statistics and quick access to other relevant pages.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{figures/old-pbsmon-personal-view.png}
  \caption{Personal view dashboard in the old PBSMon}
  \label{fig:old-pbsmon-personal-view}
\end{figure}

The dashboard displays total counts of jobs associated with the user. However, since PBS always returns the current state of the system, the Personal view focuses on showing counts of \textit{relevant} jobs---those that are considered active. Specifically, this includes jobs that are currently in the queue, currently running, or have finished within the previous few days. This filtering ensures that users see meaningful information about their recent and ongoing computational work, rather than being overwhelmed by historical data from all past jobs.

The Personal view also contains links to other pages within the system. One such link leads to the "My jobs" page, which displays detailed information about the user's jobs. For user the list of his jobs are highly relevant, therefore it can be directly incorporated into the new dashboard within the new solution.

The page also displays all jobs queues that are accessible by the user. While this information is useful, it is considered less relevant and it is not necessary to be included in the new dashboard.

One limitation of the current implementation is that the dashboard only displays total CPU usage and does not include total GPU resources. This limitation stems from the fact that GPU usage was not a significant concern at the start of the old PBSMon development, but has since become increasingly important as GPU computing has gained prominence in the MetaCenter infrastructure.


\subsection{QSUB assembler}
\label{subsec:qsub-assembler}
The QSUB assembler is one of the most important features of the PBSMon web interface, designed to assist users in submitting jobs to the MetaCenter infrastructure. The \texttt{qsub} command is a CLI (Command Line Interface) command that allows users to submit jobs to the PBS system. This page provides a form-based interface for configuring job submission parameters.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/old-pbsmon-qsub-assembler.png}
  \caption{QSUB assembler interface}
  \label{fig:qsub-assembler}
\end{figure}

The QSUB assembler page presents users with an extensive form containing numerous parameters that can be set for job submission, such as resource requirements (CPU flags, memory, scratch space, architecture), queue selection, walltime, and other MetaCenter-specific options. 

The current user interface presents parameters in a form equivalent to their syntax in the \texttt{qsub} command, which has the advantage that users gradually learn the \texttt{qsub} command syntax. However, the interface lacks supplementary explanations for individual fields, and therefore it is not always clear what the purpose of some less known or specific parameters is (e.g., \texttt{luna}, \texttt{umg}). The UI contains names directly according to PBS attributes, but without context or brief help, it can be difficult for users to understand what exactly these choices represent or how they will affect job execution.

After configuring the desired parameters, the page automatically generates a complete shell command that can be used in two ways:

\begin{itemize}
  \item Directly inserted into a terminal and executed to submit the job immediately
  \item Inserted as a comment directive in a PBS job script (e.g., \texttt{\#PBS -l ...}) for later execution
\end{itemize}

Upon confirmation (submit), the page performs a query against the available MetaCenter infrastructure to determine job feasibility. The results are presented in two key overviews:

\begin{itemize}
  \item \textbf{Number of nodes meeting the requirements}: This shows the total count of nodes that are compatible with the specified parameters (e.g., nodes matching the CPU flags, memory requirements, scratch space, architecture, and other constraints)
  \item \textbf{Number of available nodes}: This displays the count of free nodes from the compatible set at the current moment, representing the real-time availability for job execution
\end{itemize}

Following these summary statistics, the page displays a detailed list of all nodes that match the specified requirements, providing users with list of nodes that can be used to submit the job.

\section{Summary of the Analysis}
\label{sec:summary-of-analysis}

The analysis of the existing PBSMon solution reveals a system that has successfully served its purpose of monitoring and visualizing MetaCenter infrastructure data. The application effectively integrates data from multiple sources (PBS, Perun, and OpenStack) and provides users with  views of computational resources, jobs, and system status. Key strengths include the QSUB assembler feature, which assists users in constructing job submission commands, and the successful integration with Perun's authentication system.

However, the analysis also identifies several significant limitations and areas for improvement. The system suffers from a \textbf{monolithic architecture} that tightly couples all components, making it difficult to maintain, extend, or deploy in modern containerized environments. The absence of dockerization further complicates deployment and scaling. The application relies on \textbf{outdated libraries}, most notably the Java library Stripes, which is no longer maintained, creating security and sustainability concerns.

From a user experience perspective, the interface presents parameters in a form that mirrors the \texttt{qsub} command syntax, which helps users learn PBS commands but lacks supplementary explanations for individual fields. This makes it difficult for users to understand the purpose of less common parameters (e.g., \texttt{luna}, \texttt{umg}) or how their choices will affect job execution. The web interface uses \textbf{server-side rendering} and is not implemented as a standalone frontend application, limiting flexibility and modern user experience capabilities.

The system also exhibits limitations in meeting current infrastructure demands. As GPU computing has gained prominence in MetaCenter, the dashboard only displays total CPU usage statistics and does not include GPU resources or GPU time usage metrics.

Another concern identified is a \textbf{potential GDPR violation}: the system displays information about all users that currently have active jobs and their jobs within MetaCenter, which may not comply with data protection regulations regarding the visibility of personal computational activity to other users.

In summary, while PBSMon has been functional and valuable, its monolithic architecture, outdated technology stack, lack of containerization support, suboptimal user experience, incomplete feature coverage for modern computing needs, and potential privacy compliance issues necessitate a fundamental redesign and modernization of the system.


\chapter{Relevant applications and technologies}
\label{chap:relevant-applications-and-technologies}

This chapter summarizes selected applications and technologies related to the operation and future evolution of the MetaCenter computing infrastructure, as well as monitoring and user-facing access patterns relevant to this thesis. Besides the PBS-based grid environment, MetaCenter also provides web-based user portals, project and allocation tooling, metrics-based monitoring stacks, and container platforms. These components form an ecosystem in which the modernized PBSMon solution is expected to operate and where future integrations may be considered.

\section{Open OnDemand}
Open OnDemand is a web-based portal designed to provide end users with interactive access to computing resources directly from a web browser \cite{Hudak2018OpenOnDemand}. In MetaCenter, the OnDemand instance enables users to work with files through a graphical file manager, run interactive applications, and create or modify batch jobs using a GUI-oriented workflow \cite{MetacenterOnDemandDocs}.

A key benefit of OnDemand is the ability to start interactive workloads (e.g., desktop sessions or specialized applications) without requiring local software installation, which reduces the entry barrier for less experienced users and accelerates experimentation \cite{MetacenterOnDemandDocs}. In addition, OnDemand provides browser-based shell access to selected frontends, effectively allowing users to open a terminal session in the browser and submit jobs using standard cluster tooling when needed \cite{MetacenterOnDemandDocs}.

From the perspective of monitoring, OnDemand primarily targets user interaction and job submission. It intentionally presents a simplified view tailored to the logged-in user workflow (files, sessions, job composer) and does not aim to replace an operator-oriented monitoring tool that exposes detailed, cluster-wide state and relationships between PBS entities (nodes, queues, scheduling policies, historical trends). Furthermore, authentication is performed using the user's academic identity, which limits administrative impersonation-style workflows and cross-user views compared to dedicated monitoring or administration tooling \cite{MetacenterOnDemandDocs,Hudak2018OpenOnDemand}.

\section{Zeus}
\label{sec:project-zeus}
Zeus is a CESNET web application developed for MetaCenter to support project and resource allocation workflows for both PBS-managed compute resources and cloud resources. The system focuses on capturing and managing project-related processes, enabling users to request quotas, and allowing administrators to allocate requested resources. An important part of the solution is integration with the Perun identity and access management system, including propagation of project information towards computing resources \cite{Valalsky2025thesis}.

Compared to PBSMon, Zeus is not a monitoring tool; it addresses a different layer of the infrastructure lifecycle (project management and allocation rather than operational state visualization). Nevertheless, Zeus is relevant when considering the long-term evolution of the monitoring ecosystem: allocation and project metadata can become an important enrichment signal for monitoring dashboards (e.g., mapping jobs and users to projects, quotas, or allocation decisions). Therefore, Zeus is considered a potential integration point for future PBSMon extensions \cite{Valalsky2025thesis}.

\section{Prometheus}
Prometheus is an open-source monitoring and alerting toolkit that collects and stores metrics as time series enriched by labels. It is widely used for infrastructure and application monitoring and provides a flexible query language (PromQL) for aggregations and alerting \cite{PrometheusOverview,Brazil2018Prometheus}. Prometheus is typically deployed in a pull-based model, where the Prometheus server scrapes metrics endpoints exposed by monitored targets at regular intervals \cite{PrometheusOverview}.

In this thesis, Prometheus is therefore considered primarily as a complementary monitoring technology for the OpenStack part of the infrastructure, not as a replacement for PBS entity monitoring. While PBSMon focuses on presenting structured state of PBS objects (jobs, nodes, queues) and their relationships, Prometheus provides metrics for OpenStack projects and infrastructure. Both approaches can coexist: Current PBSMon provides primarily domain-specific views of PBS entities, whereas Prometheus-based monitoring can cover cloud services and selected operational indicators in parallel. This thesis aims to integrate OpenStack metrics into the new PBSMon solution.


\subsection{Thanos}
Thanos is a set of components that extends Prometheus deployments with features such as long-term storage in object storage, high availability, and a global query view across multiple Prometheus instances \cite{ThanosWebsite,ThanosGitHub}. In practice, Thanos addresses typical Prometheus limitations related to retention, high-availability operation, and querying across multiple Prometheus servers \cite{ThanosWebsite,ThanosGitHub}.

This thesis does not adopt Thanos, because the primary goal is to provide a reliable view of the current operational state of the MetaCenter PBS-based grid environment, rather than to design a long-term metrics retention and global query layer. Nevertheless, Thanos remains relevant as a future consideration if PBSMon (or surrounding infrastructure) evolves towards stronger historical analytics.

\section{Kubernetes}
Kubernetes is an open-source container orchestration platform for automating deployment, scaling, and management of containerized applications \cite{KubernetesDocs,Burns2022Kubernetes}. It provides a declarative model for running workloads, service discovery, and operational automation that is now standard in cloud-native environments \cite{KubernetesDocs,Burns2022Kubernetes}.

MetaCenter (within e-INFRA CZ) also operates a container platform based on Kubernetes, including a Rancher-based distribution and related user-facing tooling \cite{EinfraHPCKubernetes}. This is relevant for PBSMon because Kubernetes-based services are part of the broader infrastructure portfolio and strengthen the requirement for modularity and extensibility of monitoring solutions.

\chapter{Design of the New Solution}
\label{chap:design}
Building upon the analysis in Chapter~\ref{chap:analysis}, this chapter focuses on the design of a modern replacement for the original PBSMon system. The goal is to propose a solution that not only preserves the existing functionality, but also improves maintainability, usability and alignment with current infrastructure and security requirements.

In addition to the functional rewriting of the existing features, the new design must introduce first-class support for working with OpenStack. In particular, it has to enable users to view OpenStack projects, the virtual machines assigned to them, and other relevant metadata in a clear and structured manner consistent with how resources are organised in the underlying cloud environment.

Furthermore, the design addresses the potential GDPR issue identified in Chapter~\ref{chap:analysis}, where personal computational activity may become visible to other users. To mitigate this risk, the new solution introduces a simple role model with two distinct roles: \emph{user} and \emph{admin}. This separation of privileges restricts access to sensitive information and ensures that operations affecting other users or system-wide configuration are only available to administrators.

The remainder of this chapter is structured as follows. Section~\ref{sec:functional-requirements} defines the functional requirements derived from the analysis and target use cases. Section~\ref{sec:gui-design} outlines the user interface design of the new application. Section~\ref{sec:architecture-design} describes the overall system architecture, including the division into backend and frontend parts. Finally, Section~\ref{sec:selected-technologies} justifies the selection of the technologies used to implement the proposed design.

\section{Functional Requirements}
\label{sec:functional-requirements}

The goal of the new solution is to provide a read-only monitoring application that aggregates information from multiple data sources used in MetaCenter. The system does not modify the state of the underlying infrastructure; instead, it offers a unified user interface for viewing job information, infrastructure topology and cloud resources. Access to this information is controlled by a simple role model with two roles: \emph{user} and \emph{admin}. An administrator can perform all actions available to a regular user, plus additional operations related to support and diagnostics.

\subsection{Domain Entities and Data Sources}
\label{sec:domain-entities}

The new solution reuses most of the domain model of the original PBSMon application and extends it with entities from the OpenStack environment. PBS and Perun entities were already introduced in the description of the existing solution (see Chapter~\ref{chap:analysis}); this section briefly summarises them and then focuses on the new OpenStack concepts that motivate several of the following requirements.

\subsubsection*{PBS and Perun (recap)}

From PBS, the application uses primarily jobs, nodes, queues, reservations and fairshare entries, together with UNIX groups obtained from the \texttt{/etc/group} file to determine relationships between users. From Perun, it reuses the list of users with richer metadata (full name, organisation, \dots) and the description of the MetaCenter infrastructure, including organisations, clusters and their nodes. For a detailed description of these entities, see Chapter~\ref{chap:analysis}.

\subsubsection*{OpenStack entities}

The new solution also obtains more detailed information from the OpenStack cloud environment. Two main entities are relevant for the design:

\begin{itemize}
  \item \textbf{Project} -- a logical container that owns cloud resources. Each project has an owner (either an individual user or a group), a date of creation and assigned resources (quotas). Projects can therefore represent both individual and group activities.
  \item \textbf{Virtual machine (VM)} -- a virtual machine instance assigned to a specific project. For each VM, the system needs to track at least its name, associated project and creation time, so that it can be attributed to the correct user or group in the monitoring interface.
\end{itemize}

These OpenStack entities are used in the functional requirements below, in particular in the personal view for users and in the infrastructure overview.

\subsection{Roles and Access Control}
The system defines two roles:

\begin{itemize}
  \item \textbf{User} -- a regular MetaCenter user. A user has access primarily to information related to their own activity and to entities connected to their UNIX groups. Global information is either anonymised or aggregated where necessary.
  \item \textbf{Admin} -- an administrative user. An admin can see complete, non-anonymised information across all users and resources and can impersonate other users for troubleshooting and support. An admin has all capabilities of a regular user.
\end{itemize}

\subsection{Functional Requirements for Users}
\label{subsec:functional-requirements-for-users}

A regular user must be able to use the application as a personal monitoring dashboard and as a tool for exploring the infrastructure relevant to their work. The following functional requirements apply:

\begin{description}
  \item[FR-01 Personal overview]  
  The system shall provide a personal view (dashboard) where a user can see a summary of their total usage statistics, their recent and active PBS jobs, and their OpenStack projects.

  \item[FR-02 View own jobs]  
  The system shall allow a user to list and inspect the details of their own PBS jobs, including state, queue, submission time, requested resources and basic runtime information.

  \item[FR-03 View global jobs with anonymisation]  
  The system shall allow a user to browse all PBS jobs in the system. Jobs owned by other users shall be anonymised, except when the owner is:
  \begin{itemize}
    \item the current user, or
    \item a user belonging to one of the same UNIX groups as the current user.
  \end{itemize}
  In these cases, the real user identity shall be shown.

  \item[FR-04 View OpenStack projects and VMs]  
  The system shall allow a user to view all OpenStack projects associated with them and to list the virtual machines assigned to each of these projects, including basic metadata (such as name and reserved resources).

  \item[FR-05 View queues]  
  The system shall provide an overview of all PBS queues, including their basic configuration and purpose, so that users can understand where their jobs are being scheduled.

  \item[FR-06 View infrastructure]  
  The system shall provide a view of the MetaCenter infrastructure based on Perun data, showing organisations, clusters and nodes, together with reserved resources on these nodes.

  \item[FR-07 View users within groups]  
  The system shall allow a user to see basic information about other users who belong to the same UNIX groups, using data from \texttt{/etc/group}.

  \item[FR-08 View own group membership]  
  The system shall display the current user's group membership as derived from \texttt{/etc/group}, so that the user understands which groups they are associated with.

  \item[FR-09 View fairshare information]  
  The system shall display fairshare information relevant to the current user, derived from the PBS fairshare entries, and optionally allow comparison with other users in the same groups.

  \item[FR-10 QSUB assembler]  
  The system shall provide a ``QSUB assembler'' -- an interactive interface that helps the user construct a valid \texttt{qsub} command by selecting queues, resources and other parameters. The result shall be presented in a form that can be copied and used in a terminal. Alltogether with the list of nodes that fullfils the requirements.

  \item[FR-11 Detail pages]  
  For all major entities (job, node, queue, project, VM, reservation, user), the system shall offer detail pages accessible from overview tables, with information limited according to the user's role and the anonymisation rules described above.

  \item[FR-12 View storage spaces]  
  The system shall provide a view of the shared storage spaces, showing their total capacity, used capacity and free capacity. 
\end{description}

\subsection{Functional Requirements for Admins}

Administrators require a complete, non-anonymised view of the system in order to support users and diagnose problems. In addition to FR-01~--~FR-12, the following requirements apply:

\begin{description}
  \item[FR-13 Full visibility]  
  The system shall allow an admin to see all available data for all users, jobs, queues, infrastructure elements, projects and VMs without anonymisation.

  \item[FR-14 User impersonation]  
  The system shall allow an admin to impersonate a selected user, i.e., to temporarily view the application exactly as that user would see it (including all access restrictions and anonymisation). This functionality is intended solely for support and debugging.

\end{description}

These functional requirements form the basis for the graphical user interface design described in Section~\ref{sec:gui-design} and for the architecture of the new solution discussed in Section~\ref{sec:architecture-design}.

\newpage
\section{Graphical User Interface Design}
\label{sec:gui-design}

A key part of the new solution is the redesign of the graphical user interface (GUI). The existing application has grown over time and its interface is not intuitive for new users, which makes common tasks unnecessarily difficult and increases the learning curve. To improve usability, it was necessary to propose a clearer and more consistent user interface that better reflects typical workflows and makes important information easier to find.

At the same time, the current users are already familiar with the existing interface and rely on established interaction patterns. A radical change to the layout or navigation could therefore have a negative impact on productivity and would likely be met with resistance. The GUI design of the new solution therefore aims for an evolutionary rather than revolutionary change: it introduces a more intuitive structure and visual hierarchy, but preserves similar navigation concepts and overall information density so that experienced users can adapt quickly.

Another important requirement was to keep the visual identity of the MetaCenter computing grid\footnote{\url{https://www.metacentrum.cz/en/}}. The brand colour is a distinctive shade of orange, which is already used in existing tools and documentation and therefore forms an important part of the user experience. However, large orange areas on the screen can be visually tiring and, in extreme cases, reduce readability. For this reason, the new interface is based on a dark grey theme that serves as a neutral background, while orange is used as an accent colour for interactive elements such as primary buttons, active navigation items and important status indicators. This approach preserves the recognisable MetaCenter brand while improving visual comfort during long-term use.

For the design process, the Figma tool was used to create the visual specification of the interface. In Figma, a set of basic building blocks was first defined as reusable components, ensuring consistency across the entire application and simplifying future modifications. On top of these components, the main layout was designed, including a sidebar menu for navigation between the key parts of the application and a content area for displaying overviews and detail views. The resulting design of the new personal view is illustrated in Figure~\ref{fig:new-pbsMon-gui-design} and serves both as a communication tool with stakeholders and as a reference for the implementation of the frontend.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{figures/figma-personal-view.png}
  \caption{Figma design of the new personal view}
  \label{fig:new-pbsmon-gui-design}
\end{figure}

\newpage
\section{Architecture Design}
\label{sec:architecture-design}
The solution is implemented as a modular web application composed of multiple cooperating components. It provides a user interface, a backend HTTP API, and a dedicated PBS data-collection service. The components are deployed behind a reverse proxy and integrate with external infrastructure systems (PBS, Perun, and OpenStack metrics).


All application components are developed in a single git repository (monorepo). The monorepo contains the frontend, backend API, PBS data-collection service and reverse proxy. This setup enables coordinated development and shared tooling while preserving clear boundaries between components.

\subsection{Main Components}
\label{subsec:main-components}

The architecture consists of the following main components (Figure~\ref{fig:architecture-diagram}):

\begin{itemize}
  \item \textbf{Reverse proxy (Nginx)} --- acts as a single entry point for the application and routes requests to the frontend or the backend API.
  \item \textbf{Frontend (React)} --- a browser application responsible for rendering page views and handling user interaction. It consumes monitoring data exclusively through the backend API.
  \item \textbf{Backend API (NestJS)} --- exposes the system functionality and domain data through a JSON-based HTTP API. It enforces authentication/authorization rules and provides a unified interface for the frontend.
  \item \textbf{PBS Collector microservice} --- encapsulates PBS-specific communication and periodic data collection. This separates low-level PBS integration from the API and keeps API request handling independent of polling logic.
  \item \textbf{External systems} --- PBS server (cluster state), Perun (push-based membership/identity data), Prometheus (OpenStack metrics) and Accounting database (PostgreSQL).
\end{itemize}

\begin{figure}[t]
  \centering
  \includegraphics[width=1\textwidth]{figures/architecture-diagram.png}
  \caption{Architecture diagram}
  \label{fig:architecture-diagram}
\end{figure}

\subsection{Reverse Proxy}
\label{subsec:reverse-proxy}
The application is deployed behind an Nginx reverse proxy, which provides a single public entry point for all HTTP traffic. The proxy routes requests either to the frontend (static assets and client-side routing) or to the backend API based on the requested path. This simplifies deployment by hiding internal service topology from users and enables handling cross-cutting concerns consistently in one place, such as TLS termination, basic request filtering, and response compression.

\subsection{API Layer}

The API layer acts as an integration and abstraction layer over the underlying systems:

\begin{itemize}
  \item It collects data from external systems and periodically fetches the necessary entities.
  \item It normalises and combines the collected data into a consistent model tailored to the needs of the monitoring application.
  \item It implements role-based access control and anonymisation rules as described in the functional requirements, including the distinction between regular users and administrators.
  \item It exposes the data through a set of endpoints that return JSON, covering overviews (e.g. lists of jobs or projects) as well as detail views for individual entities.
\end{itemize}

By concentrating all domain logic and access control in the API, the system ensures that the same rules are applied consistently regardless of the client and that sensitive operations cannot be bypassed by directly accessing the underlying systems.

\subsection{PBS Collector Microservice}
\label{subsec:pbs-collector-microservice}
During the architectural design, an important decision concerned how PBS data should be acquired and integrated into the system. Instead of embedding PBS communication directly into the main API, the design introduces a separate service, \texttt{pbs-collector}, responsible solely for periodic PBS data acquisition.

This decision follows from the operational characteristics of the PBS environment. Access to PBS requires specific dependencies and runtime conditions, such as the \texttt{libopenpbs} IFL library, a compatible operating system environment, and a properly configured Kerberos setup. Including these requirements in the API service would increase deployment complexity and tightly couple the API runtime to PBS-specific constraints.

The separation is also motivated by reliability. Experience with the legacy PBSMon solution showed that the integration with the IFL library may be unstable over long runtimes, including failures attributable to memory leaks in the OpenPBS library. In a monolithic design, such issues could compromise the availability of the whole application, because a fault in the PBS integration would directly impact the API and therefore the user interface.

To address these risks, the design isolates PBS collection into a dedicated component that can be constrained and managed independently. The \texttt{pbs-collector} service periodically retrieves core PBS entities (e.g., jobs, nodes, queues, and related objects) and exports them in an intermediate form for consumption by the API. As a result, the main API remains focused on consolidating data sources, enforcing access control, and serving the unified domain model, while PBS-specific dependencies and failure modes are contained within the dedicated collector service.

\subsection{Frontend Layer}
The frontend layer is implemented as a React application that is responsible for rendering the user interface and handling user interaction. It consumes the data from the API and renders it into the graphical user interface described in Section~\ref{sec:gui-design}.

\subsection{Rationale}
The selected component-based design provides clear separation of responsibilities: presentation logic is contained in the frontend, domain rules and access control are centralized in the API, and PBS data collection logic is encapsulated in a dedicated collector service. This structure improves maintainability and supports future extensions, such as adding new infrastructure modules (e.g., additional services beyond PBS/OpenStack) without redesigning the entire system.

\section{Selected technologies}
\label{sec:selected-technologies}

As mentioned earlier, the original solution was implemented as a web application. The new solution preserves this form, but it is built on a modern technology stack selected with an emphasis on long-term maintainability, extensibility, and availability of community support. The selection also reflects practical operational requirements such as container-friendly deployment, clear dependency management, and the ability to evolve the system as new infrastructure modules are introduced.

In addition to these general criteria, an important constraint was compatibility with the ZEUS project (Section~\ref{sec:project-zeus}). ZEUS targets a similar environment and addresses related operational needs within CESNET; therefore, aligning the technology stack reduces the cost of future cooperation between the two systems. A unified stack simplifies potential integration scenarios, such as sharing common domain models, authentication and authorization mechanisms, API clients, or UI components, and it also lowers the barrier for developers to contribute across both codebases.

Finally, stack unification keeps open the option that the two applications may be combined into a single project in the future. By choosing technologies consistent with ZEUS, the design avoids unnecessary fragmentation of tools and expertise and supports incremental convergence without requiring a major rewrite.


\subsection{NestJS}

NestJS is a progressive Node.js framework designed for building efficient, reliable and scalable server-side applications. It leverages TypeScript as its primary language, while still supporting plain JavaScript. The framework is heavily inspired by architectural patterns known from enterprise environments, such as the Model–View–Controller (MVC) pattern and layered architectures, and it emphasises concepts like modules, controllers and providers. \cite{nestjs2025}

One of the main advantages of NestJS is its modular architecture. The application is structured into self-contained modules, which encapsulate related functionality. This organisation improves the readability of the codebase and simplifies long-term maintenance and extensibility. NestJS also provides built-in support for dependency injection, which encourages loose coupling between components and facilitates testing. \cite{nestjs2025}

According to articles \cite{nestjs2025-devto, nestjsbackend2025}, NestJS is well suited for backend development and has accumulated more than 60,000 stars on GitHub, placing it among the world’s most popular backend frameworks. It is a modern, flexible and powerful framework that is gaining popularity thanks to its strong TypeScript support, modular architecture and mature ecosystem.

For these reasons, NestJS was chosen as the framework for implementing the API layer of the new solution.

\subsection{React}

React is a JavaScript library for building user interfaces using a declarative, component-based model. It allows composing complex views from smaller reusable components and integrates well with modern JavaScript and TypeScript tooling. \cite{react-docs}

In this project, React is used as a purely client-side rendered frontend. Rendering of the user interface is thus offloaded from the server to the client, which reduces the load on the backend API and simplifies the server-side implementation. The main trade-off of client-side rendering is weaker support for search engine optimisation (SEO) compared to server-side rendering or static site generation. However, SEO is not required in this system, as it targets authenticated users and is not intended for public indexing. For this reason, a purely client-side React application is sufficient and avoids additional complexity. \cite{react-docs}

React was chosen mainly due to its wide adoption, mature ecosystem and good TypeScript support, which make it a suitable choice for the frontend part of the solution \cite{react-docs}.


\subsubsection{Tailwind CSS}

Tailwind CSS is a utility-first CSS framework that provides a set of pre-defined classes for styling HTML elements. It is designed to be used in conjunction with React, allowing for a more efficient and consistent styling approach. Tailwind CSS is a popular choice for modern web development due to its ease of use, flexibility and performance. \cite{tailwindcss2025}

I have chosen Tailwind CSS due to its popularity and ease of use. It is a popular choice for modern web development and is a good fit for the new solution.

\subsection{Nginx}

Nginx ("engine x") is a high-performance HTTP web server and reverse proxy that can also act as a load balancer and content cache. It is designed to handle a large number of concurrent connections with low resource usage, which makes it suitable as an entry point for web applications.  \cite{nginx-docs}

In this project, Nginx is used primarily as a reverse proxy in front of the backend API and as a static file server for the React frontend. Requests to the API are forwarded from Nginx to the NestJS application, while requests for frontend assets are served directly from Nginx. This setup enables centralised configuration of TLS termination, request routing and basic security headers, while the application servers focus on business logic. \cite{nginx-docs}

As mentioned in the section \ref{sec:architecture-design}, the new solution consists of separate modules — the API and the frontend — Nginx is used to provide a single unified interface to the outside world. Both parts of the system are exposed under one domain and port, and the internal structure is hidden from clients, which simplifies deployment and future maintenance. 

\subsection{Docker}
\label{subsec:docker}
%% \todo - Some explanation about docker and history of dockerization, why it is used

Docker is a platform for building, distributing and running applications in lightweight containers. A container packages the application together with its runtime dependencies and configuration, which makes the resulting environment reproducible across different machines. \cite{docker-docs}

In this project, Docker is used to containerise the backend API, the frontend and supporting services (such as Nginx). Each service runs in its own container, while a shared configuration (e.g.\ using \texttt{docker-compose}) defines how the containers are connected and started. This approach simplifies local development, testing and deployment, as the entire system can be started or stopped with a single command and behaves consistently across environments. \cite{docker-docs}


\chapter{Implementation}
\label{chap:implementation}

This chapter describes the implementation of the new PBSMon solution. The main objective was not only to migrate the legacy monolithic system to a modern technology stack, but also to deliver a maintainable and extensible codebase that supports the current scope of MetaCenter infrastructure. In particular, the implementation covers monitoring data obtained from the PBS scheduling environment and extends the original functionality by integrating OpenStack as an additional data source.

The resulting system is implemented as a separated backend and frontend that communicate via a JSON API. This separation enables clearer responsibility boundaries, simplifies testing, and allows both parts to evolve independently. At the same time, the implementation keeps the core purpose of PBSMon intact: regularly collecting infrastructure metadata, normalizing it into a consistent representation, and presenting it to users in a readable form.


% \section{Authorization and Authentication}
% \label{sec:authorization-and-authentication}
% TBD - using Perun's e-infra AAI, using OIDC token

\section{Repository directory structure}
\label{sec:system-architecture-new}
As mentioned in section \ref{sec:architecture-design}, the new solution is implemented as a monorepo. Therefore all the modules are stored in the same repository. The repository is structured into the following main components:

\begin{itemize}
  \item \texttt{api/} -- backend service exposing a JSON API consumed by the frontend. It aggregates and normalizes monitoring data from the supported infrastructure sources (PBS, Perun and OpenStack).
  \item \texttt{web/} -- frontend web application responsible for rendering page views and visualizing the collected monitoring data.
  \item \texttt{pbs-collector/} -- a dedicated micro-service for periodic data acquisition from PBS environment. Described in more detail in section \ref{sec:pbs-data-collection}.
  \item \texttt{nginx/} -- reverse proxy configuration used as the entry point to route requests to the web application and the API.
  \item \texttt{docker-compose.prod.yml} -- production deployment definition describing services, networking, and environment configuration.
  \item \texttt{deploy.sh} -- deployment helper script that automates the deployment process.
\end{itemize}

At runtime, user requests are handled through the reverse proxy, which serves the frontend and forwards API calls to the backend. The frontend then communicates with the backend exclusively through the JSON API, while the collector runs independently to keep the monitoring data up to date.


\section{API}
\label{sec:impl-api}
The backend API forms the integration layer between the monitored infrastructure and the web user interface. Its primary purpose is to provide a JSON API for the frontend while hiding the complexity of individual data sources. In PBSMon 2.0, the API aggregates monitoring data from the PBS environment, Perun and OpenStack, normalizes them into a consistent representation, and exposes them through JSON endpoints.


\newpage
\subsubsection{API code structure}
The API source code is organized into the following top-level directories:
\begin{itemize}
  \item \texttt{src/} -- source code of the API.
  \begin{itemize}
    \item \texttt{common/} -- shared utilities and cross-cutting concerns.
    \item \texttt{config/} -- application and environment configuration. Eq. QSUB assembler configuration, pbs server list, etc.
    \item \texttt{modules/} -- feature-oriented modules. Each module represent a domain entity and its related data. Each module consists of controllers and services. And therefore each module have its own routes and endpoints. There are list of modules:
    \begin{itemize}
      \item \texttt{accounting/} -- module for collecting accounting data from the accounting database.
      \item \texttt{app/} 
      \item \texttt{data-collection/} -- module responsible for collecting data from the infrastructure sources.
      \item \texttt{groups/} -- module for retrieving user groups from the PBS server.
      \item \texttt{infrastructure/} -- module for the infrastructure overview.
      \item \texttt{jobs/} -- module for the jobs overview.
      \item \texttt{projects/} -- module for the OpenStack projects overview.
      \item \texttt{qsub/} -- module for the QSUB assembler.
      \item \texttt{queues/} -- module for the queues overview.
      \item \texttt{status/} -- module for the status overview.
      \item \texttt{storage-spaces/} -- module for the storage spaces overview.
      \item \texttt{users/} -- module for the users overview.
    \end{itemize}
  \end{itemize}
  \item \texttt{data/} -- runtime collected data from the infrastructure sources.
\end{itemize}


\subsection{PBS data collection}
\label{sec:pbs-data-collection}
As mentioned in section \ref{subsec:pbs-collector-microservice}, the PBS data collection logic is separated into an independent microservice.

The \texttt{pbs-collector} microservice is therefore responsible for periodic retrieval of PBS data and exporting it into filesystem for later processing by the API. It collects core PBS entities (e.g., jobs, nodes, queues, and related objects) using the IFL interface of \texttt{libopenpbs} (as described in Section~\ref{subsubsec:batch-interface-library}), and it also gathers fairshare values for the users via the \texttt{list\_cache} command (as described in Section~\ref{subsubsec:fairshare-metrics}). This separation keeps the API focused on data delivery and presentation concerns, while the collector encapsulates the low-level PBS-specific integration and its operational constraints.


\subsection{Perun data collection}
\label{sec:perun-data-collection}
Perun is not collected by PBSMon in the same way as PBS or OpenStack, because the required data are provided to PBSMon via a push mechanism external to the application. For this reason, PBSMon does not implement an active Perun client; instead, it only consumes prepared Perun exports.

Within the API repository, Perun-related inputs are stored under \texttt{/api/data/perun/}. This directory contains static JSON files:

\begin{itemize}
  \item \texttt{pbsmon\_users.json} -- users data from Perun.
  \item \texttt{pbsmon\_machines.json} -- machines data from Perun.
\end{itemize}

These files were described in more detail in section~\ref{subsec:entities-in-perun}. During development, these files were mocked to enable frontend and API development without relying on the external Perun export pipeline. In production, the application follows the same approach as the legacy PBSMon and loads these JSON files directly from the filesystem at runtime.


\subsection{OpenStack data collection}
\label{subsec:openstack-data-collection}
In addition to PBS- and Perun-derived datasets, the system collects selected operational and inventory information related to the OpenStack environment. These data are obtained from a Prometheus server, which provides metrics and metadata exported by infrastructure components and custom exporters \cite{PrometheusOverview}. Unlike PBS and Perun data (which are loaded from files), the Prometheus-derived data are treated as a \emph{live} source and are therefore not persisted to the filesystem. Instead, the latest successful snapshot is stored only in the application memory and served through the API on demand.

The collection logic is implemented in the \texttt{Prometheus\-Collection\-Service}. The service defines a fixed set of prometheus queries that cover both hardware-related metrics (CPU, memory, GPU, disks, networking) and OpenStack-specific inventory data (projects, VM's, and users). The collecting proccess iterates over the configured queries and executes them sequentially. Each query result is stored under a human-readable key (the query \texttt{name}) together with a timestamp representing the time of the snapshot creation. If a query fails, the error is logged and collection continues, so that partial data can still be exposed to the frontend.


\subsection{Documentation}
\label{sec:api-documentation}
The API contract is described using an OpenAPI specification, which defines the available endpoints, their parameters, and response schemas. In addition to the static specification, the implementation exposes a Swagger UI that renders the OpenAPI document and provides an interactive interface for exploring and testing the API. This allows developers and administrators to call individual endpoints directly from the browser, inspect responses, and verify the expected behaviour without requiring external tools. The OpenAPI specification is included within the thesis archive.


\newpage
\section{Frontend - page views}
\label{sec:page-views}
This section describes the implemented frontend page views and how they present monitoring information to end users. Each view corresponds to a specific domain area of the system (e.g., jobs, nodes, queues, outages, news, or OpenStack-related data) and is implemented as a composition of reusable UI components such as tables, filters, and detail panels. The pages obtain all data exclusively through the backend JSON API, which keeps the frontend independent of the underlying infrastructure protocols and allows the UI to evolve without changing data-collection logic.

\subsection{Layout and navigation}
\label{subsec:layout-and-navigation}

The frontend uses a dashboard-style layout consisting of a persistent navigation sidebar on the left and a main content area on the right (Figure~\ref{fig:ui-layout}). The sidebar provides the primary navigation between page views, while the main content area displays the currently selected view.

The navigation contains two top-level entries (\emph{Personal View} and \emph{Qsub Assembler}) and an expandable group \emph{Resource Status}. The \emph{Resource Status} group aggregates the main monitoring domains and links to the corresponding views: \emph{Machines}, \emph{Storage Spaces}, \emph{Cloud Projects}, \emph{Jobs Queues}, \emph{Jobs}, \emph{Waiting Jobs}, \emph{Users}, and \emph{Groups}. A small header area above the content provides global controls such as language switching. The lower part of the sidebar contains auxiliary links (support, documentation, FAQ).

The following subsections describe the individual page views listed in the navigation. Each subsection explains what data the view presents and which backend API endpoints it consumes.

\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{figures/new-pbsmon/layout.png}
  \caption{Frontend layout with a navigation sidebar}
  \label{fig:ui-layout}
\end{figure}

\newpage
\subsection{Personal View}
\label{subsec:personal-view}

The \emph{Personal View} page provides a personalized overview of monitoring data related to the currently authenticated user (Figure~\ref{fig:ui-personal-view}). Its purpose is to present a compact summary of the user’s current activity across both the PBS environment and cloud projects. This page implements \textbf{FR-01 Personal overview} by combining high-level totals with a direct listing of jobs and cloud projects.

The upper part of the page contains a \emph{My Statistics} panel with two summary cards. The first card (\emph{Total Tasks}) displays aggregated counts of the user’s tasks by state (queued, running, done) together with the total. The second card (\emph{Resource Utilization}) provides a similar breakdown for reserved resources, separated into CPU and GPU sections.

Below the summary, the page presents two main lists:
(i) \emph{My PBS Jobs}, which shows the currently relevant PBS jobs (including state, job identifier, job name, target machine, and reserved resources such as CPU/GPU/RAM), and
(ii) \emph{My Cloud Projects}, which lists cloud projects accessible to the user together with basic project metadata and reserved resource totals. Both lists are displayed in a table form and provide an ordering and a simple search field.

\paragraph{Consumed API endpoints.}
\begin{itemize}
  \item \texttt{GET /pbs/jobs} -- source of PBS job data rendered in \emph{My PBS Jobs}, and total statistics of PBS jobs.
  \item \texttt{GET /projects} -- source of cloud project data rendered in \emph{My Cloud Projects}.
\end{itemize}

\begin{figure}[t]
  \centering
  % Replace with the actual relative path in your repository.
  \includegraphics[width=\textwidth]{figures/new-pbsmon/personal-view-full.png}
  \caption{Personal View page showing a user-oriented overview: summary statistics, active PBS jobs, and accessible cloud projects.}
  \label{fig:ui-personal-view}
\end{figure}


\subsection{QSUB assembler}
\label{subsec:new-qsub-assembler}

The \emph{QSUB Assembler} page helps users compose a valid PBS job submission based on selected resource requirements and provides a preview of nodes that satisfy these requirements. Its main purpose is to returns a \texttt{qsub} command and or A PBS job script template that the user can use to submit the job. As well as to show the user the nodes that satisfy the requirements.
This page satisfies \textbf{FR-10 QSUB assembler}. 

Figure~\ref{fig:ui-qsub-assembler-form} (see Appendix~\ref{app:qsub-assembler-figures}) shows the initial state of the page, which consists mainly of a configuration form. The form is split into \emph{Basic Settings} (e.g., walltime, queue, CPU count, RAM, GPU requirements, scratch) and \emph{Advanced Settings} (e.g., architecture/vendor constraints and additional selectors such as PBS server or OS-related constraints).

After the user requests a preview (button \emph{Show Results}), the page displays the computed output (Figure~\ref{fig:ui-qsub-assembler-results}, see Appendix~\ref{app:qsub-assembler-figures}). The results section contains:
(i) a one-line \texttt{qsub} command that reflects the selected requirements,
(ii) a PBS job script template containing \texttt{\#PBS} directive lines (PBS job script directives),
and (iii) a summary of matching resources, including statistics such as the number of qualified nodes and the number of nodes immediately available, followed by a list/grid of qualified nodes with their current utilization. The preview is intended to provide quick feedback on feasibility (whether the requested resources exist and how constrained the request is) and to offer a copy-ready submission template.

\paragraph{Consumed API endpoints.}
The view obtains data from two backend endpoints:
\begin{itemize}
  \item \texttt{GET /qsub/config} -- returns the form configuration (available queues, selectable constraint values, and other UI options). This allows to backend to dynamically update the form based on api configuration.
  \item \texttt{POST /qsub/preview} -- evaluates the submitted requirements and returns the preview output (qualified nodes, generated \texttt{qsub} command, and PBS script template).
\end{itemize}


\subsection{Machines}
\label{subsec:machines}

The \emph{Machines} page shows the MetaCenter infrastructure in a single hierarchical view: \emph{organization} $\rightarrow$ \emph{cluster} $\rightarrow$ \emph{node} (Figure~\ref{fig:ui-machines}). It provides overall totals (organizations, clusters, nodes, CPU, GPU) and displays node cards with current state (e.g., occupied, partially free, maintenance) and per-node resource indicators. A right-side \emph{Quick Links} panel enables fast navigation between organizations. The content is identical for users and administrators; no anonymization is applied.
This page satisfies \textbf{FR-06 View infrastructure}. 

\paragraph{Consumed API endpoint.}
\begin{itemize}
  \item \texttt{GET /infrastructure} - returns all infrastructure data in a hierarchical structure. This includes organizations, clusters, and nodes with their current state and resource indicators.
\end{itemize}

\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{figures/new-pbsmon/machines.png}
  \caption{Machines page overview.}
  \label{fig:ui-machines}
\end{figure}


\subsubsection{Node detail view}
\label{subsec:machine-detail}

The \emph{Machine detail} view provides a single-node overview: identification (node, cluster, owner), current state, reserved resources (CPU, RAM), and available scratch space. It also aggregates node-related data into tabs, including currently active jobs, available queues for the node, basic system information, and operational restrictions/outage history. The goal is to inspect one node in context without navigating across multiple list pages.

\paragraph{Consumed API endpoints.}
\begin{itemize}
  \item \texttt{GET /infrastructure/machines/:nodeName} -- returns node metadata (name, cluster, owner), current state and reserved resources 
  \item \texttt{GET /pbs/jobs} -- provides PBS jobs filtered for the selected node and displayed in the \emph{Tasks} tab.
  \item \texttt{GET /pbs/queues} -- provides queue definitions relevant to the selected node and displayed in the \emph{Queues} tab.
  \item \texttt{GET /accounting/nodes/:nodeName/outages} -- returns the outage/availability history for the node, rendered in the \emph{Operational Restrictions} tab.
\end{itemize}


\subsection{Storage spaces}
\label{subsec:storage-spaces}

The \emph{Storage Spaces} page provides an overview of shared storage and its current free capacity. This information is useful for both users and administrators, because these disk arrays are typically used for persistent data (eq. for storing job outputs). User quota information is not shown in the current implementation; showing personal quotas was not part of the old PBSMon and is left as a potential future improvement.
This page satisfies \textbf{FR-12 View storage spaces}.

\paragraph{Consumed API endpoint.}
\begin{itemize}
  \item \texttt{GET /storage-spaces} -- returns a list of storage spaces with total/used/free capacity (also aggregated totals).
\end{itemize}

\subsection{Cloud Projects}
\label{subsec:cloud-projects}

The \emph{Cloud Projects} page lists OpenStack projects in the e-INFRA infrastructure. It supports searching, ordering, and pagination, and each row links to a project detail view. The table shows basic project state and aggregated reserved resources (e.g., VM count, vCPU, memory). In the administrator view, the list contains all projects.

This page partially satisfies \textbf{FR-04 View OpenStack projects and VMs}: it displays project-level information and VM counts, while VM-level details are provided by the project detail page.

\paragraph{Consumed API endpoint.}
\begin{itemize}
  \item \texttt{GET /projects} -- returns the list of OpenStack projects (including aggregated resource information used in the table).
\end{itemize}

\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{figures/new-pbsmon/cloud-projects.png}
  \caption{Cloud Projects page (administrator view) listing OpenStack projects with aggregated reserved resources.}
  \label{fig:ui-cloud-projects}
\end{figure}


\subsubsection{Project detail view}

\label{subsec:project-detail}

The \emph{Project detail} page shows detailed information for a single OpenStack project: basic metadata (name, ID, status, description), aggregated reserved resources, the list of project VMs, and the list of associated users. It completes \textbf{FR-04 View OpenStack projects and VMs} by providing VM-level information.

\paragraph{Consumed API endpoint.}
\begin{itemize}
  \item \texttt{GET /projects/:id} -- returns project metadata together with VM and members lists.
\end{itemize}

\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{figures/new-pbsmon/cloud-project-detail.png}
  \caption{Project detail page showing project metadata, VMs, and users.}
  \label{fig:ui-project-detail}
\end{figure}


\subsection{Jobs Queues}
\label{subsec:jobs-queues}

The \emph{Jobs Queues} page lists PBS queues and their current state. It satisfies \textbf{FR-05 View queues}. Queues are displayed hierarchically (e.g., route queues and their execution queues) and include key properties such as priority, time limits, and job statistics (queued, running, done, total). Queues the current user cannot submit to are marked with a lock icon (red background). As mentioned in the \ref{subsubsec:reservations} section, the reservation is an entity within the PBS, which always creates, the associated queue. This queue are distinguished by a dedicated icon of calendar before the queue name. The page, also implement ordering functionality.

\paragraph{Consumed API endpoint.}
\begin{itemize}
  \item \texttt{GET /pbs/queues} - returns the list of queues in a hierarchical structure.
\end{itemize}

\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{figures/new-pbsmon/jobs-queue-list.png}
  \caption{Jobs Queues page (user view) showing hierarchical queue list, access restrictions, and reservation queues.}
  \label{fig:ui-jobs-queues}
\end{figure}

\subsubsection{Queue detail view}
The \emph{Queue detail} page provides details for a selected PBS queue. It shows basic properties (name, type, priority, maximum walltime) and the parent queue. A statistics panel summarizes queued/running/done counts. The lower part contains tabs with queue-related lists, primarily jobs associated with the queue (status, owner, machine, reserved resources), and additional tabs for related machines and system information. If a queue were created by reservation, it shows detail information about reservation. This page satisfies \textbf{FR-11 Detail pages}.


\paragraph{Consumed API endpoint.}
\begin{itemize}
  \item \texttt{GET /pbs/queues/:id} - details information about queue
  \item \texttt{GET /pbs/jobs} - list of jobs associated with the queue
  \item \texttt{GET /infrastructure} - list of infrastructure associated with the queue
\end{itemize}

\newpage
\subsection{Jobs}
\label{subsec:jobs}

The \emph{Jobs} page lists PBS jobs in a table view with search, ordering, and pagination. It provides three tabs: \emph{My Jobs} (jobs of the current user), \emph{All Jobs} (global view), and \emph{Waiting Jobs}. Each row shows job state, identifier, name, owner, assigned machine, and reserved resources (CPU/GPU/RAM) including basic utilization indicators.The waiting jobs in addition show information about a reason of waiting of the job.

The view satisfies \textbf{FR-02 View own jobs} and \textbf{FR-03 View global jobs with anonymisation} by showing the user’s jobs in full detail while anonymizing user-identifying fields in the global listing. 

\paragraph{Consumed API endpoint.}
\begin{itemize}
  \item \texttt{GET /pbs/jobs} - endpoints used for all three tabs (My Jobs, All Jobs, Waiting Jobs).
\end{itemize}

\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{figures/new-pbsmon/jobs-list.png}
  \caption{Jobs page (All Jobs tab) showing global job listing with anonymized user information.}
  \label{fig:ui-jobs}
\end{figure}

\subsubsection{Job detail view}
\label{subsec:job-detail}

The \emph{Job detail} page provides a complete overview of a selected PBS job. It includes an optional \emph{Messages} section with warnings derived from resource usage (e.g., low CPU/GPU/memory utilization compared to the requested resources). The \emph{Basic Information} section shows identifiers and placement (job ID, name, state, owner, queue, server, assigned node) and the job comment. \emph{Timestamps} summarize key times (created, started, eligible, last state change). The \emph{Resources} section shows the requested resources string, reserved resources (CPU, GPU, memory, walltime), and current usage indicators (CPU time used, GPU usage, memory used, runtime). Finally, \emph{Allocated Resources} lists the concrete allocation (machine and per-resource amounts), followed by a \emph{System Information} table with additional PBS attributes.

\paragraph{Consumed API endpoint.}
\begin{itemize}
  \item \texttt{GET /pbs/jobs/:id} -- returns detailed job metadata, resource requests/reservations, usage, and additional PBS attributes rendered on the page.
\end{itemize}

\subsection{Users}
\label{subsec:users}
The \emph{Users} page provides a paginated table of users with search and ordering. It shows username/nickname, fairshare value, job counters (queued/running/done/total), and aggregated resource usage; in the administrator view it also includes an \emph{Impersonate} action.

This view satisfies \textbf{FR-09 View fairshare information}, \textbf{FR-13 Full visibility}, \textbf{FR-14 User impersonation}, and supports \textbf{FR-07 View users within groups} as an entry point for user--group navigation.

\paragraph{Consumed API endpoint.}
\begin{itemize}
  \item \texttt{GET /users} -- returns a paginated user list and supports server-side search and ordering via query parameters.
\end{itemize}

\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{figures/new-pbsmon/users-list-admin.png}
  \caption{Users page (administrator view) with pagination, search, ordering, and impersonation action. Sensitive identifiers are anonymized manually.}
  \label{fig:ui-users}
\end{figure}


\newpage
\subsubsection{User detail view}
\label{subsec:user-detail}
The \emph{User detail} page aggregates identity and usage information for a selected user. It shows basic identity data (username, full name, organization, membership expiration), fairshare per server, and accounting summaries (e.g., total jobs, total CPU time, yearly breakdown). The lower part contains tabs with the user’s current jobs and jobs queues available for the user.

\paragraph{Consumed API endpoints.}
\begin{itemize}
  \item \texttt{GET /users/:id} -- user identity and profile metadata.
  \item \texttt{GET /accounting/users/:username} -- aggregated historical jobs statistics and yearly breakdowns.
  \item \texttt{GET /pbs/jobs} -- jobs filtered for the selected user.
  \item \texttt{GET /pbs/queues} -- queues available for the selected user.
\end{itemize}

\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{figures/new-pbsmon/user-detail.png}
  \caption{User detail page with identity data, fairshare, accounting summary, and user job list.}
  \label{fig:ui-user-detail}
\end{figure}


\subsection{Users groups}
\label{subsec:groups}
The \emph{Groups} page lists UNIX groups with their name, GID, and member count. Regular users see only groups they belong to, while administrators see all groups. This view satisfies \textbf{FR-08 View own group membership}.

\paragraph{Consumed API endpoint.}
\begin{itemize}
  \item \texttt{GET /users/groups} -- returns the list of groups (name, GID, member count).
\end{itemize}

\subsubsection{Group detail view}
\label{subsubsec:group-detail}

The \emph{Group detail} page shows details of a selected UNIX group (name, GID) and its member list (nickname and optional full name from Perun). Visibility follows the same rule as the list view: users can access only groups they are members of, while administrators can access all groups.

\paragraph{Consumed API endpoint.}
\begin{itemize}
  \item \texttt{GET /users/groups/:name} -- returns group metadata and the list of members.
\end{itemize}

\chapter{Future improvements}
\label{chap:future-improvements}
The implemented solution focuses on reliable collection and presentation of PBS, OpenStack, and related infrastructure data. Since the system is designed in a modular way, it can be extended with additional collectors and UI views with minimal impact on existing parts. The following improvements would further increase the usefulness of the service for both end users and administrators:

\begin{itemize}
  \item \textbf{Kubernetes module.} Add a new collector and page views for monitoring the MetaCenter Kubernetes service (cluster health, nodes, workloads, and basic resource usage).
  \item \textbf{Administrative exports.} Provide CSV exports for selected views (e.g., jobs and nodes) to support analysis for admin users.
  \item \textbf{User storage quotas.} Display personal disk quotas to complement compute usage information in the personal view and user detail view.
  \item \textbf{PBS job submission API.} Add an option for submitting PBS jobs using the API.
\end{itemize}

\chapter*{Conclusion}
\markboth{Conclusion}{Conclusion}
\addcontentsline{toc}{chapter}{Conclusion}

The objectives of this thesis were fulfilled by designing and implementing a new web application intended to replace the existing PBSMon monitoring tool. The resulting solution provides a modern, maintainable system for presenting the state of the computing infrastructure. However in contrast to the old solution, the new solution also shows a more detailed view of OpenStack entities. In addition to that, it shows GPU usage of PBS jobs, which old solution did not support and introduces two user roles - administrator, which can see all unanonymised data and user, which can see only anonymised data.

The new solution is built as a modular web application with a clear separation between a backend API and a frontend, which improves extensibility and supports future development of additional features and data sources.

The development was preceded by an analysis phase focused on understanding the old PBSMon implementation, identifying its limitations, and deriving requirements for the new solution. The analysis also considered the environment in which the application operates and the available external systems. Based on the identified needs, the work defined the main functional requirements  and designed an architecture.

The final architecture integrates data from multiple sources, specifically PBS, Perun, and OpenStack, and exposes the processed information through a JSON-based API. To support maintainability and easier evolution of the system, the implementation uses a monorepository structure and a technology stack aligned with the ZEUS project \cite{Valalsky2025thesis}, which was an important decision due to potential future integration or merging of both solutions into a single project. The API provides role-based access control for user and administrator operations, and the API is documented through an OpenAPI specification.

Finally, the app was deployed using docker compose and is available at \url{https://mu-pub-245-82.flt.openstack.cloud.e-infra.cz}.

Future work should primarily target extending the system beyond PBS and OpenStack by adding a dedicated Kubernetes module for the MetaCentrum Kubernetes service, including both a new collector and corresponding UI views. Additional improvements include providing administrative CSV exports for selected views (e.g., jobs and nodes), displaying user storage quotas to complement compute usage in personal and user detail pages, and extending the backend API with support for PBS job submission.

\appendix
\chapter{OpenStack Response Example}
\label{app:openstack-response-example}

This appendix contains an example of the JSON response structure for the OpenStack cluster named "glados" that is collected by the PBSMon system.

\begin{lstlisting}[caption={Example response for OpenStack cluster "glados"},
  label={lst:openstack-glados-response}]
[
    {
        "CPUs": "40",
        "Hypervisor": "ics-gladosag-007-ostack.priv.cloud.muni.cz",
        "VMs": [
            {
                "CPUs": "38",
                "created": "2020-10-09T09:32:42Z",
                "instance_state": "ACTIVE",
                "name": "RationAI-node-2",
                "user_id": "1633180b677608f61e96784ee5cbc608c0f4b62d@einfra.cesnet.cz"
            }
        ]
    },
    {
        "CPUs": "40",
        "Hypervisor": "ics-gladosag-006-ostack.priv.cloud.muni.cz",
        "VMs": []
    },
    {
        "CPUs": "40",
        "Hypervisor": "ics-gladosag-003-ostack.priv.cloud.muni.cz",
        "VMs": [
            {
                "CPUs": "38",
                "created": "2020-01-17T10:29:22Z",
                "instance_state": "ACTIVE",
                "name": "RationAI-node-1",
                "user_id": "1633180b677608f61e96784ee5cbc608c0f4b62d@einfra.cesnet.cz"
            }
        ]
    },
    {
        "CPUs": "40",
        "Hypervisor": "ics-gladosag-004-ostack.priv.cloud.muni.cz",
        "VMs": []
    },
    {
        "CPUs": "40",
        "Hypervisor": "ics-gladosag-001-ostack.priv.cloud.muni.cz",
        "VMs": [
            {
                "CPUs": "16",
                "created": "2022-08-17T13:07:15Z",
                "instance_state": "ACTIVE",
                "name": "front-82e95edc-1e2d-11ed-9687-0ee20d64cb6e",
                "user_id": "9cd61d48508661633e261f711634b749fdc5d9fcc20769e55baac46ef01c5a63@egi.eu"
            }
        ]
    }
]
\end{lstlisting}

\chapter{QSUB Assembler Figures}
\label{app:qsub-assembler-figures}

This appendix contains the figures for the QSUB Assembler page, which are too large to include in the main text.

\begin{figure}[p]
  \centering
  % Replace with your actual path
  \includegraphics[width=\textwidth]{figures/new-pbsmon/qsub-assembler.png}
  \caption{QSUB Assembler page: configuration form for job requirements and constraints.}
  \label{fig:ui-qsub-assembler-form}
\end{figure}

\begin{figure}[p]
  \centering
  % Replace with your actual path
  \includegraphics[width=\textwidth]{figures/new-pbsmon/qsub-assembler-results.png}
  \caption{QSUB Assembler page after preview: generated \texttt{qsub} command, PBS job script template (\texttt{\#PBS} directives), and a list of qualified nodes with availability summary.}
  \label{fig:ui-qsub-assembler-results}
\end{figure}

\clearpage

\end{document}
